{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "#ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "ls2.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n"
     ]
    }
   ],
   "source": [
    "np_clinic_1, np_clinic_2,np_non_var_1, np_non_var_2, np_class_ID_1, np_class_ID_2 = DF.get_all_dataframe_patient_specific(200,patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_class_ID_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-bd66d076770b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp_class_ID_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp_class_ID_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp_class_ID_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp_class_ID_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_clinic_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_clinic_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np_class_ID_1' is not defined"
     ]
    }
   ],
   "source": [
    "np_class_ID_1 = [int(i) for i in np_class_ID_1]\n",
    "np_class_ID_2 = [int(i) for i in np_class_ID_2]\n",
    "\n",
    "X_train = np_clinic_1\n",
    "X_test = np_clinic_2\n",
    "y_train = np.asarray(np_class_ID_1)\n",
    "y_test = np.asarray(np_class_ID_2)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np_class_ID_1_old\n",
    "y_test = np_class_ID_2_old\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1 = [int(i) for i in np_class_ID_1]\n",
    "np_class_ID_2 = [int(i) for i in np_class_ID_2]\n",
    "\n",
    "\n",
    "X_train = np_non_var_1\n",
    "X_test = np_non_var_2\n",
    "y_train = np.asarray(np_class_ID_1)\n",
    "y_test = np.asarray(np_class_ID_2)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np_non_var_1_old\n",
    "X_test = np_non_var_2_old\n",
    "y_train = np_class_ID_1_old\n",
    "y_test = np_class_ID_2_old\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_clinic_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7658833237ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnorm_var_1_clinic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpost_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalised_values_multiples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_clinic_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnorm_var_2_clinic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpost_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalised_values_multiples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_clinic_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnp_class_ID_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp_class_ID_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp_class_ID_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp_class_ID_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp_clinic_new_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_clinic_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_var_1_clinic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np_clinic_1' is not defined"
     ]
    }
   ],
   "source": [
    "norm_var_1_clinic = post_features.normalised_values_multiples(np_clinic_1)\n",
    "norm_var_2_clinic = post_features.normalised_values_multiples(np_clinic_2)\n",
    "np_class_ID_1 = [int(i) for i in np_class_ID_1]\n",
    "np_class_ID_2 = [int(i) for i in np_class_ID_2]\n",
    "np_clinic_new_1 = np.hstack((np_clinic_1, norm_var_1_clinic))\n",
    "np_clinic_new_2 = np.hstack((np_clinic_2, norm_var_2_clinic))\n",
    "\n",
    "X_train = np_clinic_new_1\n",
    "X_test =np_clinic_new_2\n",
    "y_train = np.asarray(np_class_ID_1)\n",
    "y_test = np.asarray(np_class_ID_2)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45809 976 3788 414\n",
      "45809 976 3788 414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45809 976 3788 414\n",
      "41228 878 3409 372\n",
      "41228 878 3409 372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "old_beats_train = y_train\n",
    "n_beat = X_train[y_train==0]\n",
    "s_beat = X_train[y_train==1]\n",
    "v_beat = X_train[y_train==2]\n",
    "f_beat = X_train[y_train==3]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = y_train[y_train==0]\n",
    "s_beat_y = y_train[y_train==1]\n",
    "v_beat_y = y_train[y_train==2]\n",
    "f_beat_y = y_train[y_train==3]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "outdetect = IsolationForest(max_features = input_size)\n",
    "\n",
    "outliers_n = outdetect.fit_predict(n_beat)\n",
    "outliers_s = outdetect.fit_predict(s_beat)\n",
    "outliers_v = outdetect.fit_predict(v_beat)\n",
    "outliers_f = outdetect.fit_predict(f_beat)\n",
    "\n",
    "print(len(outliers_n), len(outliers_s), len(outliers_v),len(outliers_f) )\n",
    "\n",
    "n_beat = n_beat[outliers_n==1]\n",
    "s_beat = s_beat[outliers_s==1]\n",
    "v_beat = v_beat[outliers_v==1]\n",
    "f_beat = f_beat[outliers_f==1]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = n_beat_y[outliers_n==1]\n",
    "s_beat_y = s_beat_y[outliers_s==1]\n",
    "v_beat_y = v_beat_y[outliers_v==1]\n",
    "f_beat_y = f_beat_y[outliers_f==1]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "n_beat_y = n_beat_y.reshape(n_beat_y.shape[0],1)\n",
    "s_beat_y = s_beat_y.reshape(s_beat_y.shape[0],1)\n",
    "v_beat_y = v_beat_y.reshape(v_beat_y.shape[0],1)\n",
    "f_beat_y = f_beat_y.reshape(f_beat_y.shape[0],1)\n",
    "\n",
    "\n",
    "X_train = np.vstack((n_beat,s_beat, v_beat, f_beat))\n",
    "y_train = np.vstack((n_beat_y,s_beat_y, v_beat_y, f_beat_y))\n",
    "\n",
    "\n",
    "#outlers_test = outdetect.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44021 2050 3220 388\n",
      "44021 2050 3220 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44021 2050 3220 388\n",
      "39619 1845 2898 349\n",
      "39619 1845 2898 349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "old_beats_train = y_test\n",
    "n_beat = X_test[y_test==0]\n",
    "s_beat = X_test[y_test==1]\n",
    "v_beat = X_test[y_test==2]\n",
    "f_beat = X_test[y_test==3]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = y_test[y_test==0]\n",
    "s_beat_y = y_test[y_test==1]\n",
    "v_beat_y = y_test[y_test==2]\n",
    "f_beat_y = y_test[y_test==3]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "outdetect = IsolationForest(max_features = input_size)\n",
    "\n",
    "outliers_n = outdetect.fit_predict(n_beat)\n",
    "outliers_s = outdetect.fit_predict(s_beat)\n",
    "outliers_v = outdetect.fit_predict(v_beat)\n",
    "outliers_f = outdetect.fit_predict(f_beat)\n",
    "\n",
    "print(len(outliers_n), len(outliers_s), len(outliers_v),len(outliers_f) )\n",
    "\n",
    "n_beat = n_beat[outliers_n==1]\n",
    "s_beat = s_beat[outliers_s==1]\n",
    "v_beat = v_beat[outliers_v==1]\n",
    "f_beat = f_beat[outliers_f==1]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = n_beat_y[outliers_n==1]\n",
    "s_beat_y = s_beat_y[outliers_s==1]\n",
    "v_beat_y = v_beat_y[outliers_v==1]\n",
    "f_beat_y = f_beat_y[outliers_f==1]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "n_beat_y = n_beat_y.reshape(n_beat_y.shape[0],1)\n",
    "s_beat_y = s_beat_y.reshape(s_beat_y.shape[0],1)\n",
    "v_beat_y = v_beat_y.reshape(v_beat_y.shape[0],1)\n",
    "f_beat_y = f_beat_y.reshape(f_beat_y.shape[0],1)\n",
    "\n",
    "\n",
    "X_test = np.vstack((n_beat,s_beat, v_beat, f_beat))\n",
    "y_test = np.vstack((n_beat_y,s_beat_y, v_beat_y, f_beat_y))\n",
    "\n",
    "\n",
    "#outlers_test = outdetect.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\nkf.get_n_splits(X_test) # returns the number of splitting iterations in the cross-validator\\n\\n\\nX_test_list = []\\ny_test_list = []\\nfor one, two in kf.split(X_test):\\n    \\n    X_test_list.append(X_test[one])\\n    X_test_list.append(X_test[two])\\n    y_test_list.append(y_test[one])\\n    y_test_list.append(y_test[two])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold # import KFold\n",
    "kf = KFold(n_splits=10) # Define the split - into 2 folds \n",
    "kf.get_n_splits(X_train, y_train)#, group) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "for one, two in kf.split(X_train, y_train):\n",
    "    \n",
    "    X_train_list.append(X_train[one])\n",
    "    #X_train_list.append(X_train[two])\n",
    "    y_train_list.append(y_train[one])\n",
    "    #y_train_list.append(y_train[two])\n",
    "\n",
    "\"\"\"\"\"\n",
    "kf.get_n_splits(X_test) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "for one, two in kf.split(X_test):\n",
    "    \n",
    "    X_test_list.append(X_test[one])\n",
    "    X_test_list.append(X_test[two])\n",
    "    y_test_list.append(y_test[one])\n",
    "    y_test_list.append(y_test[two])\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train=gooddata_X_train\n",
    "y_train=gooddata_y_train \n",
    "X_test=gooddata_X_test \n",
    "y_test=gooddata_y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def linear_svc(X_train, y_train, X_test, y_test,penalty='l2', loss='squared_hinge', dual=True, tol=0.001, C=10, multi_class='crammer_singer', fit_intercept=True, intercept_scaling=10, class_weight='balanced'):\n",
    "    linearclass = LinearSVC(penalty=penalty, loss=loss, dual=dual, tol=tol, C=C, multi_class=multi_class, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight)\n",
    "    linearclass.fit(X_train, y_train)\n",
    "    y_pred = linearclass.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test))\n",
    "    \n",
    "    return metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "## numner of estimators leaves at 1000 \n",
    "\n",
    "def randomForest(X_train, y_train, X_test, y_test,n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, max_features=50, max_leaf_nodes=None, class_weight='balanced'):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,min_weight_fraction_leaf=min_weight_fraction_leaf, class_weight=class_weight, min_impurity_decrease=min_impurity_decrease,max_features=max_features)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test))\n",
    "    \n",
    "    return rf, metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "\n",
    "\n",
    "def svm_model_linear(X_train, y_train, X_test, y_test, C=10, kernel='inear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test))\n",
    "    \n",
    "    return svm_model_linear, metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## small C from 1 to 3 seems to be good for f class recalled\n",
    "## degree 3 is better all around\n",
    "## Tol at 0.01 seems best\n",
    "\n",
    "def svm_model_poly(X_train, y_train, X_test, y_test, C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                        coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test))\n",
    "    \n",
    "    return svm_model_linear, metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## quite greedy on the v class\n",
    "## might remove this one\n",
    "\n",
    "def svm_model_rbf(X_train, y_train, X_test, y_test, C=10, kernel='rbf', degree=3, gamma='auto', \n",
    "                        coef0=0.001, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test))\n",
    "    \n",
    "    return svm_model_linear, metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def Linear_D(X_train, y_train, X_test, y_test):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test))\n",
    "    \n",
    "    return clf, metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def ExtraTree(X_train, y_train, X_test, y_test,n_estimators=1000, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, max_features=50, max_leaf_nodes=None, class_weight='balanced'):\n",
    "    clf = ExtraTreesClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,min_weight_fraction_leaf=min_weight_fraction_leaf, class_weight=class_weight, min_impurity_decrease=min_impurity_decrease,max_features=max_features)\n",
    "    clf.fit(X_train, y_train)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test))\n",
    "    \n",
    "    return clf, metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "## L1 one is good , l2 would be used\n",
    "## very good at f class recall\n",
    "## solver = ['newton-cg', 'lbfgs', 'liblinear'are all good, ensemble with all would be good\n",
    "\n",
    "## false dual is better\n",
    "def logisticRegress(X_train, y_train, X_test, y_test, penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None):\n",
    "    lr = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        # dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "    lr.fit(X_train, y_train.ravel())  \n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel()))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel()))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel()))\n",
    "    \n",
    "    return lr, metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27212  3605  4501  4301]\n",
      " [ 1186   510    53    96]\n",
      " [  194   188  2408   108]\n",
      " [    0     0     0   349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.69      0.80     39619\n",
      "           1       0.12      0.28      0.17      1845\n",
      "           2       0.35      0.83      0.49      2898\n",
      "           3       0.07      1.00      0.13       349\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     44711\n",
      "   macro avg       0.37      0.70      0.40     44711\n",
      "weighted avg       0.87      0.68      0.75     44711\n",
      "\n",
      "[0.23927842243753328, 1.5717402214192098, 0.31610673889616786]\n"
     ]
    }
   ],
   "source": [
    "ld, score = logisticRegress(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-0beccbff6565>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mada\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mada\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mada\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \"\"\"\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 333\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    802\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=2500, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, max_features=50, max_leaf_nodes=None, class_weight='balanced')\n",
    "\n",
    "\n",
    "ada = AdaBoostClassifier(base_estimator=rf, n_estimators=50 )\n",
    "ada.fit(X_train, y_train)  \n",
    "y_pred = ada.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metric.get_metrics(y_pred,y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgbo is good for normal classes too\n",
    "## do this one next \n",
    "## XGB is very goos for V class\n",
    "## XGboost tree classifcation is good with 5 or more tree depth\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def xgboost(X_train,y_train, X_test, y_test, max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, process_type='default', predictor= 'cpu_predictor'): \n",
    "    model = XGBClassifier(max_depth = max_depth, eta = eta, gamma = gamma,min_child_weight=min_child_weight,subsample=subsample, max_delta_step= max_delta_step,colsample_bytree=colsample_bytree,colsample_bylevel=colsample_bylevel,colsample_bynode=colsample_bynode,alpha=alpha,reg_lambda=reg_lambda,grow_policy=grow_policy, tree_method=tree_method,refresh_leaf=refresh_leaf,process_type=process_type, predictor= predictor,objective = 'reg:logistic')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel()))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel()))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel()))\n",
    "    \n",
    "    return model, metric.get_metrics(y_pred,y_test)[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgbo is good for normal classes too\n",
    "## do this one next \n",
    "## XGB is very goos for V class\n",
    "## XGboost tree classifcation is good with 5 or more tree depth\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def xgboost(X_train,y_train, X_test, y_test,  predictor= 'cpu_predictor'): \n",
    "    model = XGBClassifier( booster = 'dart',predictor= predictor,objective = 'reg:logistic')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel()))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel()))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel()))\n",
    "    \n",
    "    return model, metric.get_metrics(y_pred,y_test)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33610  1910  1959  2140]\n",
      " [ 1467   289    69    20]\n",
      " [   87   116  2669    26]\n",
      " [   22     0   109   218]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90     39619\n",
      "           1       0.12      0.16      0.14      1845\n",
      "           2       0.56      0.92      0.69      2898\n",
      "           3       0.09      0.62      0.16       349\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     44711\n",
      "   macro avg       0.43      0.64      0.47     44711\n",
      "weighted avg       0.89      0.82      0.85     44711\n",
      "\n",
      "[0.39533479259712656, 1.7578050478657792, 0.4173930272817857]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4173930272817857"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost(X_train, y_train, X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33617    50  5949     3]\n",
      " [ 1740     0   105     0]\n",
      " [  626     4  2268     0]\n",
      " [    0     0   349     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89     39619\n",
      "           1       0.00      0.00      0.00      1845\n",
      "           2       0.26      0.78      0.39      2898\n",
      "           3       0.00      0.00      0.00       349\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     44711\n",
      "   macro avg       0.30      0.41      0.32     44711\n",
      "weighted avg       0.84      0.80      0.81     44711\n",
      "\n",
      "[0.28019892330281276, 1.0441702225810172, 0.27062073947403353]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "nbrs = KNeighborsClassifier(n_neighbors=100, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "nbrs.fit(X_train, y_train)\n",
    "y_pred = nbrs.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metric.get_metrics(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['features', 'rfscore']\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(features,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>rfscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>wave_MLII43</td>\n",
       "      <td>0.064886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wave_MLII42</td>\n",
       "      <td>0.054265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>wave_V16</td>\n",
       "      <td>0.043476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wave_MLII22</td>\n",
       "      <td>0.043289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>wave_MLII32</td>\n",
       "      <td>0.037404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>wave_MLII31</td>\n",
       "      <td>0.034340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>wave_V15</td>\n",
       "      <td>0.026865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>wave_MLII29</td>\n",
       "      <td>0.026337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>wave_V143</td>\n",
       "      <td>0.024959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>hbf_MLII6</td>\n",
       "      <td>0.024383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>wave_V125</td>\n",
       "      <td>0.022057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wave_MLII23</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wave_MLII11</td>\n",
       "      <td>0.021087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>wave_MLII30</td>\n",
       "      <td>0.019891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>wave_MLII28</td>\n",
       "      <td>0.019269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wave_MLII10</td>\n",
       "      <td>0.015870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>wave_V142</td>\n",
       "      <td>0.015707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>wave_MLII24</td>\n",
       "      <td>0.013854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>wave_MLII33</td>\n",
       "      <td>0.013631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wave_MLII5</td>\n",
       "      <td>0.013398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>hbf_MLII2</td>\n",
       "      <td>0.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wave_MLII6</td>\n",
       "      <td>0.013172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>wave_V17</td>\n",
       "      <td>0.012964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>wave_MLII44</td>\n",
       "      <td>0.012335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wave_MLII7</td>\n",
       "      <td>0.011036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wave_MLII16</td>\n",
       "      <td>0.010482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>wave_MLII25</td>\n",
       "      <td>0.010347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>wave_MLII34</td>\n",
       "      <td>0.010029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>wave_V121</td>\n",
       "      <td>0.009461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>wave_MLII26</td>\n",
       "      <td>0.009421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>wave_MLII41</td>\n",
       "      <td>0.008518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>wave_V123</td>\n",
       "      <td>0.007694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wave_MLII4</td>\n",
       "      <td>0.007422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wave_MLII9</td>\n",
       "      <td>0.007342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>wave_MLII35</td>\n",
       "      <td>0.007097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wave_MLII14</td>\n",
       "      <td>0.006979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>wave_V135</td>\n",
       "      <td>0.006750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>wave_V131</td>\n",
       "      <td>0.006649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>wave_MLII21</td>\n",
       "      <td>0.006171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>hbf_V16</td>\n",
       "      <td>0.006135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>hbf_MLII1</td>\n",
       "      <td>0.006089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>wave_V144</td>\n",
       "      <td>0.005969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>wave_V129</td>\n",
       "      <td>0.005891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>wave_V132</td>\n",
       "      <td>0.005619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>hbf_MLII11</td>\n",
       "      <td>0.005422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>wave_V130</td>\n",
       "      <td>0.005249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>wave_V18</td>\n",
       "      <td>0.005034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>wave_V134</td>\n",
       "      <td>0.004578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>wave_MLII18</td>\n",
       "      <td>0.004571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>wave_V122</td>\n",
       "      <td>0.004546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wave_MLII8</td>\n",
       "      <td>0.004442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>wave_V14</td>\n",
       "      <td>0.004403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>wave_V136</td>\n",
       "      <td>0.004378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wave_MLII19</td>\n",
       "      <td>0.004369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>wave_MLII36</td>\n",
       "      <td>0.004277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>wave_MLII15</td>\n",
       "      <td>0.004226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wave_MLII12</td>\n",
       "      <td>0.004177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>hbf_V10</td>\n",
       "      <td>0.004069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>hos_MLII0</td>\n",
       "      <td>0.004041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>wave_V118</td>\n",
       "      <td>0.004004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        features   rfscore\n",
       "43   wave_MLII43  0.064886\n",
       "42   wave_MLII42  0.054265\n",
       "51      wave_V16  0.043476\n",
       "22   wave_MLII22  0.043289\n",
       "32   wave_MLII32  0.037404\n",
       "31   wave_MLII31  0.034340\n",
       "50      wave_V15  0.026865\n",
       "29   wave_MLII29  0.026337\n",
       "88     wave_V143  0.024959\n",
       "96     hbf_MLII6  0.024383\n",
       "70     wave_V125  0.022057\n",
       "23   wave_MLII23  0.021505\n",
       "11   wave_MLII11  0.021087\n",
       "30   wave_MLII30  0.019891\n",
       "28   wave_MLII28  0.019269\n",
       "10   wave_MLII10  0.015870\n",
       "87     wave_V142  0.015707\n",
       "24   wave_MLII24  0.013854\n",
       "33   wave_MLII33  0.013631\n",
       "5     wave_MLII5  0.013398\n",
       "92     hbf_MLII2  0.013316\n",
       "6     wave_MLII6  0.013172\n",
       "52      wave_V17  0.012964\n",
       "44   wave_MLII44  0.012335\n",
       "7     wave_MLII7  0.011036\n",
       "16   wave_MLII16  0.010482\n",
       "25   wave_MLII25  0.010347\n",
       "34   wave_MLII34  0.010029\n",
       "66     wave_V121  0.009461\n",
       "26   wave_MLII26  0.009421\n",
       "..           ...       ...\n",
       "41   wave_MLII41  0.008518\n",
       "68     wave_V123  0.007694\n",
       "4     wave_MLII4  0.007422\n",
       "9     wave_MLII9  0.007342\n",
       "35   wave_MLII35  0.007097\n",
       "14   wave_MLII14  0.006979\n",
       "80     wave_V135  0.006750\n",
       "76     wave_V131  0.006649\n",
       "21   wave_MLII21  0.006171\n",
       "111      hbf_V16  0.006135\n",
       "91     hbf_MLII1  0.006089\n",
       "89     wave_V144  0.005969\n",
       "74     wave_V129  0.005891\n",
       "77     wave_V132  0.005619\n",
       "101   hbf_MLII11  0.005422\n",
       "75     wave_V130  0.005249\n",
       "53      wave_V18  0.005034\n",
       "79     wave_V134  0.004578\n",
       "18   wave_MLII18  0.004571\n",
       "67     wave_V122  0.004546\n",
       "8     wave_MLII8  0.004442\n",
       "49      wave_V14  0.004403\n",
       "81     wave_V136  0.004378\n",
       "19   wave_MLII19  0.004369\n",
       "36   wave_MLII36  0.004277\n",
       "15   wave_MLII15  0.004226\n",
       "12   wave_MLII12  0.004177\n",
       "105      hbf_V10  0.004069\n",
       "120    hos_MLII0  0.004041\n",
       "63     wave_V118  0.004004\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[df['rfscore'] >= 0.01]\n",
    "\n",
    "df = df.sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "df[df['rfscore'] >= 0.004]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "norm_dtw = [\"dtw1_v1\",\"dtw2_v1\"]\n",
    "classID = [\"classID\"]\n",
    "non_clinic = list(DB1_non_cli.iloc[:,0:140].columns.values)\n",
    "\n",
    "feature_norm_mill = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_mill.append(str(feat_labels[i]+\"_n_MLII\"))\n",
    "\n",
    "feature_norm_V1 = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_V1.append(str(feat_labels[i]+\"_n_V1\"))\n",
    "    \n",
    "\n",
    "c_ID = np.asarray(classID)\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "non_cli = np.asarray(non_clinic)\n",
    "norm_mlii = np.asarray(feature_norm_mill)\n",
    "norm_v1 = np.asarray(feature_norm_V1)\n",
    "norm_dtw = np.asarray(norm_dtw)\n",
    "\n",
    "\n",
    "#features_clinic = np.hstack((c_ID,non_cli))#f_M,f_V, f_d,non_cli,norm_mlii, norm_v1,norm_dtw))\n",
    "features_clinic = np.hstack((f_M,f_V, f_d,))\n",
    "#features_clinic = np.hstack((non_cli))\n",
    "\n",
    "features = []\n",
    "for feature in zip(features_clinic):\n",
    "    \n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf, score = randomForest(X_train, y_train, X_test, y_test)\n",
    "\n",
    "#rf.feature_importances_\n",
    "\n",
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "features_clinic = np.hstack((f_M,f_V, f_d ))\n",
    "features = []\n",
    " \n",
    "#for feature in zip(features_clinic, rf.feature_importances_):\n",
    "    \n",
    "    #features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33119  1879  1352  3269]\n",
      " [ 1021   249   572     3]\n",
      " [   72   130  2657    39]\n",
      " [    0     0    34   315]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90     39619\n",
      "           1       0.11      0.13      0.12      1845\n",
      "           2       0.58      0.92      0.71      2898\n",
      "           3       0.09      0.90      0.16       349\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     44711\n",
      "   macro avg       0.44      0.70      0.47     44711\n",
      "weighted avg       0.90      0.81      0.85     44711\n",
      "\n",
      "[0.40098866856017795, 1.737804439257663, 0.41771988918729686]\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32850  1862  1504  3403]\n",
      " [  995   249   600     1]\n",
      " [   75   131  2654    38]\n",
      " [    0     0    23   326]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89     39619\n",
      "           1       0.11      0.13      0.12      1845\n",
      "           2       0.56      0.92      0.69      2898\n",
      "           3       0.09      0.93      0.16       349\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     44711\n",
      "   macro avg       0.43      0.70      0.47     44711\n",
      "weighted avg       0.90      0.81      0.84     44711\n",
      "\n",
      "[0.39306288486313334, 1.7169388974280861, 0.4111488046100774]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32885  1859  1532  3343]\n",
      " [  979   245   615     6]\n",
      " [   76   130  2652    40]\n",
      " [    0     0    28   321]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89     39619\n",
      "           1       0.11      0.13      0.12      1845\n",
      "           2       0.55      0.92      0.69      2898\n",
      "           3       0.09      0.92      0.16       349\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     44711\n",
      "   macro avg       0.43      0.70      0.46     44711\n",
      "weighted avg       0.90      0.81      0.84     44711\n",
      "\n",
      "[0.39390142664601635, 1.706983526306447, 0.41032365411131405]\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33020  1807  1505  3287]\n",
      " [ 1007   249   582     7]\n",
      " [   75   135  2655    33]\n",
      " [    0     0    34   315]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.90     39619\n",
      "           1       0.11      0.13      0.12      1845\n",
      "           2       0.56      0.92      0.69      2898\n",
      "           3       0.09      0.90      0.16       349\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     44711\n",
      "   macro avg       0.43      0.70      0.47     44711\n",
      "weighted avg       0.90      0.81      0.84     44711\n",
      "\n",
      "[0.3976268023806075, 1.7206596771794742, 0.413895860837738]\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32982  1860  1556  3221]\n",
      " [  994   250   597     4]\n",
      " [   61   136  2670    31]\n",
      " [    0     0    40   309]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.90     39619\n",
      "           1       0.11      0.14      0.12      1845\n",
      "           2       0.55      0.92      0.69      2898\n",
      "           3       0.09      0.89      0.16       349\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     44711\n",
      "   macro avg       0.43      0.69      0.47     44711\n",
      "weighted avg       0.90      0.81      0.84     44711\n",
      "\n",
      "[0.39778927347106235, 1.7171792006634614, 0.41354203681846385]\n"
     ]
    }
   ],
   "source": [
    "penalty = ['l1','l2']\n",
    "loss = ['hinge', 'squared_hinge'] \n",
    "fit_intercept = [True, False]\n",
    "tol = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "g = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "cache_size = [100, 500, 1000, 1500, 2000,2500,3000,3500,4000]\n",
    "criterion = ['gini', 'entropy']\n",
    "min_samples_split = [2,3,5,10,20,50,100,200,500,1000]\n",
    "#fit_in\n",
    "clf = []\n",
    "class_weight = [None,'balanced']\n",
    "min_samples_leaf = [1,2,3,5,10,20,40,50,100]\n",
    "min_weight_fraction_leaf = [0,0.0001,0.001,0.01,0.1]\n",
    "min_impurity_decrease = [0,0.0001,0.001,0.01,0.1, 1]\n",
    "gamma = ['scale','auto']\n",
    "coef0 = [0.00001,0.0001,0.001,0.01,0.1,1,10,100]\n",
    "max_features = ['auto', 'sqrt','log2', None, 10, 50, 100, 200]\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "C=[0.01,0.1,1,2,5,10,20,50,100]\n",
    "degree = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "shrinking = [True, False]\n",
    "multi_class=['ovr', 'multinomial', 'auto']\n",
    "max_depth = [1,2,4,8,16,32,64,128,236,500,1000]\n",
    "\n",
    "max_depth = [10,16,20,30,50]\n",
    "\n",
    "min_child_weight = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8]\n",
    "gamma = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10,100]\n",
    "subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8]\n",
    "max_delta_step = [0,0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10,100]\n",
    "\n",
    "colsample_bytree =  [0.01,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,1]\n",
    "colsample_bylevel =  [0.01,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,1]\n",
    "colsample_bynode =  [0.01,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,1]\n",
    "reg_lambda = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10,100]\n",
    "alpha = [0,0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10,100]\n",
    "\n",
    "tree_method = ['auto', 'exact', 'approx', 'hist', 'gpu_hist']\n",
    "#updater = [('distcol','grow_histmaker', 'grow_local_histmaker','grow_skmaker','sync', 'refresh', 'prune'), ('grow_colmaker','prune')]\n",
    "\n",
    "grow_policy=['depthwise', 'lossguide']\n",
    "refresh_leaf = [True,False]\n",
    "\n",
    "\n",
    "predictor= ['cpu_predictor','gpu_predictor']\n",
    "\n",
    "process_type = ['default', 'update']\n",
    "\n",
    "ls = max_depth\n",
    "\n",
    "for i in range(len(ls)):\n",
    "    print(ls[i])\n",
    "    clf.append(randomForest(X_train, y_train, X_test, y_test, max_depth=ls[i])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34882  1612  1245  1880]\n",
      " [ 1498   246    84    17]\n",
      " [   64    51  2762    21]\n",
      " [   25     0   108   216]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92     39619\n",
      "           1       0.13      0.13      0.13      1845\n",
      "           2       0.66      0.95      0.78      2898\n",
      "           3       0.10      0.62      0.17       349\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     44711\n",
      "   macro avg       0.46      0.65      0.50     44711\n",
      "weighted avg       0.90      0.85      0.87     44711\n",
      "\n",
      "[0.45085314000459276, 1.8730433569146072, 0.4595569896166223]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4595569896166223"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ls = [str(i) for i in ls]\n",
    "plt.plot(ls, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30001  9618]\n",
      " [ 1363  3729]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85     39619\n",
      "           1       0.28      0.73      0.40      5092\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     44711\n",
      "   macro avg       0.62      0.74      0.62     44711\n",
      "weighted avg       0.88      0.75      0.80     44711\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-0677170b7572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mbin_y_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbin_labels_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mvoting_ensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbin_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-42ce579f0947>\u001b[0m in \u001b[0;36mvoting_ensemble\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0meclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_metrics\u001b[1;34m(yhat, labels)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_kappa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mjkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkappa\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.125\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_kappa\u001b[1;34m(confusion_matrix, n_samples)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mPe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mPo\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mPe\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mPo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPo\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "bin_labels = y_train.copy()\n",
    "bin_labels[y_train != 0]=1\n",
    "bin_labels[y_train == 0]=0\n",
    "bin_y_train=np.array(bin_labels, dtype=int)\n",
    "\n",
    "bin_labels_test = y_test.copy()\n",
    "bin_labels_test[y_test != 0]=1\n",
    "bin_labels_test[y_test == 0]=0\n",
    "bin_y_test=np.array(bin_labels_test, dtype=int)\n",
    "\n",
    "voting_ensemble(X_train, bin_y_train, X_test,bin_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "def voting_ensemble(X_train, y_train, X_test,y_test):\n",
    "    svm_model_linear = svm.SVC(C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                            coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                            coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, \n",
    "                                min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, \n",
    "                                max_features=50, max_leaf_nodes=None, class_weight='balanced')\n",
    "\n",
    "\n",
    "    lr = LogisticRegression( penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, \n",
    "                            class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn',\n",
    "                            verbose=0, warm_start=False, n_jobs=None)\n",
    "\n",
    "\n",
    "    xgb = XGBClassifier(max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, \n",
    "                        subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, \n",
    "                        reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, \n",
    "                        process_type='default', predictor= 'cpu_predictor',objective = 'reg:logistic')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[('svc_2', svm_model_poly), ('xgb',xgb),('rf',rf), ('lr',lr), ('svm_ln   ',svm_model_linear)],\n",
    "                           voting='hard', weights=[2,2,2,2,2])\n",
    "\n",
    "\n",
    "\n",
    "    #clf1 = clf1.fit(X, y)\n",
    "    #clf2 = clf2.fit(X, y)\n",
    "    #clf3 = clf3.fit(X, y)\n",
    "    eclf = eclf.fit(X_train, bin_y_train)\n",
    "\n",
    "    predict = eclf.predict(X_test)\n",
    "    #scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "    #scores.mean()                             \n",
    "\n",
    "    print(confusion_matrix(y_test,predict))  \n",
    "    print(classification_report(y_test,predict))\n",
    "    print(metric.get_metrics(predict.ravel(),y_test.ravel()))\n",
    "    \n",
    "    return eclf, metric.get_metrics(predict.ravel(),y_test.ravel())[2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_model_poly = svm.SVC(C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                        coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "svm_model_linear = svm.SVC(C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, \n",
    "                            min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, \n",
    "                            max_features=50, max_leaf_nodes=None, class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34391  1094  2826  1308]\n",
      " [  462  1180   198     5]\n",
      " [  276   235  2338    49]\n",
      " [   63     0     0   286]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92     39619\n",
      "           1       0.47      0.64      0.54      1845\n",
      "           2       0.44      0.81      0.57      2898\n",
      "           3       0.17      0.82      0.29       349\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     44711\n",
      "   macro avg       0.51      0.78      0.58     44711\n",
      "weighted avg       0.91      0.85      0.88     44711\n",
      "\n",
      "[0.5011853165216339, 2.3526679074581622, 0.5446761466930872]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33583  2956   787  2293]\n",
      " [ 1162   435   169    79]\n",
      " [  287    98  2421    92]\n",
      " [  128    13     3   205]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90     39619\n",
      "           1       0.12      0.24      0.16      1845\n",
      "           2       0.72      0.84      0.77      2898\n",
      "           3       0.08      0.59      0.14       349\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     44711\n",
      "   macro avg       0.47      0.63      0.49     44711\n",
      "weighted avg       0.90      0.82      0.85     44711\n",
      "\n",
      "[0.38751074006272596, 1.9116630082182284, 0.43271324605864153]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31058  2769  2053  3739]\n",
      " [  399   298  1148     0]\n",
      " [   45    83  2689    81]\n",
      " [    0     0     8   341]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87     39619\n",
      "           1       0.09      0.16      0.12      1845\n",
      "           2       0.46      0.93      0.61      2898\n",
      "           3       0.08      0.98      0.15       349\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     44711\n",
      "   macro avg       0.40      0.71      0.44     44711\n",
      "weighted avg       0.91      0.77      0.82     44711\n",
      "\n",
      "[0.36469031325907675, 1.6399193473140068, 0.3873350750437892]\n"
     ]
    }
   ],
   "source": [
    "svm_model_poly.fit(X_train, y_train)\n",
    "\n",
    "predict = svm_model_poly.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predict))  \n",
    "print(classification_report(y_test,predict))\n",
    "print(metric.get_metrics(predict.ravel(),y_test.ravel()))\n",
    "\n",
    "svm_model_linear.fit(X_train, y_train)\n",
    "\n",
    "predict = svm_model_linear.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predict))  \n",
    "print(classification_report(y_test,predict))\n",
    "print(metric.get_metrics(predict.ravel(),y_test.ravel()))\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predict = rf.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predict))  \n",
    "print(classification_report(y_test,predict))\n",
    "print(metric.get_metrics(predict.ravel(),y_test.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dae067232d8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mada\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_model_poly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mada\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mada\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \"\"\"\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[1;31m# The weights are all 1. for SAMME.R\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             pred = sum(_samme_proba(estimator, n_classes, X)\n\u001b[1;32m--> 669\u001b[1;33m                        for estimator in self.estimators_)\n\u001b[0m\u001b[0;32m    670\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# self.algorithm == \"SAMME\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m             pred = sum((estimator.predict(X) == classes).T * w\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[1;31m# The weights are all 1. for SAMME.R\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             pred = sum(_samme_proba(estimator, n_classes, X)\n\u001b[1;32m--> 669\u001b[1;33m                        for estimator in self.estimators_)\n\u001b[0m\u001b[0;32m    670\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# self.algorithm == \"SAMME\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m             pred = sum((estimator.predict(X) == classes).T * w\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_samme_proba\u001b[1;34m(estimator, n_classes, X)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \"\"\"\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;31m# Displace zero probabilities so the log is defined.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    626\u001b[0m         pred_proba = (self._sparse_predict_proba\n\u001b[0;32m    627\u001b[0m                       if self._sparse else self._dense_predict_proba)\n\u001b[1;32m--> 628\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpred_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m             cache_size=self.cache_size, coef0=self.coef0, gamma=self._gamma)\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "ada = AdaBoostClassifier(base_estimator=svm_model_poly, n_estimators=50 )\n",
    "ada.fit(X_train, y_train)  \n",
    "y_pred = ada.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metric.get_metrics(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, \n",
    "                            min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, \n",
    "                            max_features=50, max_leaf_nodes=None, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df = pd.DataFrame(features,columns=columns)(features,columns=columns)\n",
    "\n",
    "dataframe_X = pd.DataFrame(X_train,columns=features)\n",
    "dataframe_y = pd.DataFrame(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_X.to_csv('database/good_X_train.csv')\n",
    "dataframe_y.to_csv('database/good_y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
