{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n"
     ]
    }
   ],
   "source": [
    "np_clinic_1, np_clinic_2,np_non_var_1, np_non_var_2, np_class_ID_1, np_class_ID_2 = DF.get_all_dataframe_patient_specific(500,patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1 = [int(i) for i in np_class_ID_1]\n",
    "np_class_ID_2 = [int(i) for i in np_class_ID_2]\n",
    "X_train_new = np_clinic_1\n",
    "X_test_new = np_clinic_2\n",
    "y_train_new = np.asarray(np_class_ID_1)\n",
    "y_test_new = np.asarray(np_class_ID_2)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "train, train_labels=ada.fit_sample(X_train_new, y_train_new)\n",
    "test, test_labels=ada.fit_sample(X_test_new, y_test_new)\n",
    "\n",
    "X_train_new = train\n",
    "#X_test = test\n",
    "y_train_new = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features_X = np.asarray(pd.read_csv(\"database/good_X_train.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/good_y_train.csv\").iloc[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45809 976 3788 414\n",
      "45809 976 3788 414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45809 976 3788 414\n",
      "41228 878 3409 372\n",
      "41228 878 3409 372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "old_beats_train = y_train\n",
    "n_beat = X_train[y_train==0]\n",
    "s_beat = X_train[y_train==1]\n",
    "v_beat = X_train[y_train==2]\n",
    "f_beat = X_train[y_train==3]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = y_train[y_train==0]\n",
    "s_beat_y = y_train[y_train==1]\n",
    "v_beat_y = y_train[y_train==2]\n",
    "f_beat_y = y_train[y_train==3]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "outdetect = IsolationForest(max_features = input_size)\n",
    "\n",
    "outliers_n = outdetect.fit_predict(n_beat)\n",
    "outliers_s = outdetect.fit_predict(s_beat)\n",
    "outliers_v = outdetect.fit_predict(v_beat)\n",
    "outliers_f = outdetect.fit_predict(f_beat)\n",
    "\n",
    "print(len(outliers_n), len(outliers_s), len(outliers_v),len(outliers_f) )\n",
    "\n",
    "n_beat = n_beat[outliers_n==1]\n",
    "s_beat = s_beat[outliers_s==1]\n",
    "v_beat = v_beat[outliers_v==1]\n",
    "f_beat = f_beat[outliers_f==1]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = n_beat_y[outliers_n==1]\n",
    "s_beat_y = s_beat_y[outliers_s==1]\n",
    "v_beat_y = v_beat_y[outliers_v==1]\n",
    "f_beat_y = f_beat_y[outliers_f==1]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "n_beat_y = n_beat_y.reshape(n_beat_y.shape[0],1)\n",
    "s_beat_y = s_beat_y.reshape(s_beat_y.shape[0],1)\n",
    "v_beat_y = v_beat_y.reshape(v_beat_y.shape[0],1)\n",
    "f_beat_y = f_beat_y.reshape(f_beat_y.shape[0],1)\n",
    "\n",
    "\n",
    "X_train = np.vstack((n_beat,s_beat, v_beat, f_beat))\n",
    "y_train = np.vstack((n_beat_y,s_beat_y, v_beat_y, f_beat_y))\n",
    "\n",
    "\n",
    "#outlers_test = outdetect.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "old_beats_train = y_test\n",
    "n_beat = X_test[y_test==0]\n",
    "s_beat = X_test[y_test==1]\n",
    "v_beat = X_test[y_test==2]\n",
    "f_beat = X_test[y_test==3]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = y_test[y_test==0]\n",
    "s_beat_y = y_test[y_test==1]\n",
    "v_beat_y = y_test[y_test==2]\n",
    "f_beat_y = y_test[y_test==3]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "outdetect = IsolationForest(max_features = input_size)\n",
    "\n",
    "outliers_n = outdetect.fit_predict(n_beat)\n",
    "outliers_s = outdetect.fit_predict(s_beat)\n",
    "outliers_v = outdetect.fit_predict(v_beat)\n",
    "outliers_f = outdetect.fit_predict(f_beat)\n",
    "\n",
    "print(len(outliers_n), len(outliers_s), len(outliers_v),len(outliers_f) )\n",
    "\n",
    "n_beat = n_beat[outliers_n==1]\n",
    "s_beat = s_beat[outliers_s==1]\n",
    "v_beat = v_beat[outliers_v==1]\n",
    "f_beat = f_beat[outliers_f==1]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = n_beat_y[outliers_n==1]\n",
    "s_beat_y = s_beat_y[outliers_s==1]\n",
    "v_beat_y = v_beat_y[outliers_v==1]\n",
    "f_beat_y = f_beat_y[outliers_f==1]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "n_beat_y = n_beat_y.reshape(n_beat_y.shape[0],1)\n",
    "s_beat_y = s_beat_y.reshape(s_beat_y.shape[0],1)\n",
    "v_beat_y = v_beat_y.reshape(v_beat_y.shape[0],1)\n",
    "f_beat_y = f_beat_y.reshape(f_beat_y.shape[0],1)\n",
    "\n",
    "\n",
    "X_test = np.vstack((n_beat,s_beat, v_beat, f_beat))\n",
    "y_test = np.vstack((n_beat_y,s_beat_y, v_beat_y, f_beat_y))\n",
    "\n",
    "\n",
    "#outlers_test = outdetect.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gooddata_X_train= X_train\n",
    "gooddata_y_train = y_train \n",
    "#gooddata_X_test = X_test\n",
    "#gooddata_y_test = y_test\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(gooddata_X_train,columns=features_clinic)\n",
    "df2 = pd.DataFrame(gooddata_y_train)\n",
    "df.to_csv('database/gooddata_X_train_no_outliers.csv')\n",
    "df2.to_csv('database/gooddata_y_train_no_outliers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=gooddata_X_train\n",
    "y_train=gooddata_y_train \n",
    "X_test=gooddata_X_test \n",
    "y_test=gooddata_y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37945   984  3305  1787]\n",
      " [ 1399   471   105    75]\n",
      " [  233   221  1831   935]\n",
      " [   47     2     5   334]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91     44021\n",
      "           1       0.28      0.23      0.25      2050\n",
      "           2       0.35      0.57      0.43      3220\n",
      "           3       0.11      0.86      0.19       388\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     49679\n",
      "   macro avg       0.42      0.63      0.45     49679\n",
      "weighted avg       0.88      0.82      0.84     49679\n",
      "\n",
      "[0.35630862598200463, 1.4281087678274935, 0.356667908969439]\n"
     ]
    }
   ],
   "source": [
    "predictions_svm_poly = svm_model_poly(X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## small C from 1 to 3 seems to be good for f class recalled\n",
    "## degree 3 is better all around\n",
    "## Tol at 0.01 seems best\n",
    "\n",
    "def svm_model_poly(X_train, y_train, X_test, y_test, C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                        coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3]):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test,lb=labels))\n",
    "    \n",
    "    return svm_model_linear,y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def Linear_D(X_train, y_train, X_test, y_test, labels = [0,1,2,3]):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred, labels = labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return clf, metric.get_metrics(y_pred,y_test,lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "## numner of estimators leaves at 1000 \n",
    "\n",
    "def randomForest(X_train, y_train, X_test, y_test,n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, max_features=50, max_leaf_nodes=None, class_weight='balanced', labels = [0,1,2,3]):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,min_weight_fraction_leaf=min_weight_fraction_leaf, class_weight=class_weight, min_impurity_decrease=min_impurity_decrease,max_features=max_features)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return rf, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgbo is good for normal classes too\n",
    "## do this one next \n",
    "## XGB is very goos for V class\n",
    "## XGboost tree classifcation is good with 5 or more tree depth\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def xgboost(X_train,y_train, X_test, y_test, max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, process_type='default', predictor= 'cpu_predictor', labels = [0,1,2,3]): \n",
    "    model = XGBClassifier(max_depth = max_depth, eta = eta, gamma = gamma,min_child_weight=min_child_weight,subsample=subsample, max_delta_step= max_delta_step,colsample_bytree=colsample_bytree,colsample_bylevel=colsample_bylevel,colsample_bynode=colsample_bynode,alpha=alpha,reg_lambda=reg_lambda,grow_policy=grow_policy, tree_method=tree_method,refresh_leaf=refresh_leaf,process_type=process_type, predictor= predictor,objective = 'reg:logistic')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return model, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "## L1 one is good , l2 would be used\n",
    "## very good at f class recall\n",
    "## solver = ['newton-cg', 'lbfgs', 'liblinear'are all good, ensemble with all would be good\n",
    "\n",
    "## false dual is better\n",
    "def logisticRegress(X_train, y_train, X_test, y_test, penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, labels = [0,1,2,3]):\n",
    "    lr = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        # dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "    lr.fit(X_train, y_train.ravel())  \n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return lr, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def ada(X_train, y_train, X_test, y_test,labels = [0,1,2,3]):\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                            coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    ada = AdaBoostClassifier(base_estimator=svm_model_poly, n_estimators=50 )\n",
    "    ada.fit(X_train, y_train)  \n",
    "    y_pred = ada.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return ada, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "\n",
    "\n",
    "def svm_model_linear(X_train, y_train, X_test, y_test, C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3] ):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return svm_model_linear, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr, HOS, myMorph training features\n",
    "train_rr=np.loadtxt('rr-train.txt', dtype=float)[:,1:] # extract class labels\n",
    "train_hos=np.loadtxt('hos_train.txt', dtype=float)\n",
    "train_mmorph=np.loadtxt('morph_train.txt', dtype=float)\n",
    "\n",
    "# rr, HOS, myMorph testing features\n",
    "test_rr=np.loadtxt('rr-test.txt', dtype=float)[:,1:] # extract class labels\n",
    "test_rr=np.loadtxt('rr-test.txt', dtype=float)[:,1:] # extract class labels\n",
    "test_hos=np.loadtxt('hos_test.txt', dtype=float)\n",
    "test_mmorph=np.loadtxt('morph_test.txt', dtype=float)\n",
    "\n",
    "# training and testing class labels.\n",
    "train_labels=np.loadtxt('labels_train.txt', dtype=str)\n",
    "test_labels=np.loadtxt('labels_test.txt', dtype=str)\n",
    "\n",
    "# aggregate all features.\n",
    "train=np.hstack((train_rr, train_hos, train_mmorph))\n",
    "test=np.hstack((test_rr, test_hos, test_mmorph))\n",
    "\n",
    "#train=np.hstack((train_rr, train_mmorph))\n",
    "#test=np.hstack((test_rr, test_mmorph))\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "train, train_labels=ada.fit_sample(train, train_labels)\n",
    "#test, test_labels=ada.fit_sample(test_rr, test_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "#new_train, new_train_labels=ada.fit_sample(df[rank], y_train)\n",
    "#new_x_test, new_y_test=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "\n",
    "#input_size=X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n",
      "[[33692  2528  3085  4716]\n",
      " [ 1283   356   389    22]\n",
      " [   72    48  3056    44]\n",
      " [   31     1    69   287]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.77      0.85     44021\n",
      "           1       0.12      0.17      0.14      2050\n",
      "           2       0.46      0.95      0.62      3220\n",
      "           3       0.06      0.74      0.11       388\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     49679\n",
      "   macro avg       0.40      0.66      0.43     49679\n",
      "weighted avg       0.89      0.75      0.80     49679\n",
      "\n",
      "[0.3176265139704748, 1.7072047585882033, 0.3722138518087628]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32181  3319   379  8142]\n",
      " [  263  1407   348    32]\n",
      " [   50   285  2546   339]\n",
      " [   13     0    12   363]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.73      0.84     44021\n",
      "           1       0.28      0.69      0.40      2050\n",
      "           2       0.78      0.79      0.78      3220\n",
      "           3       0.04      0.94      0.08       388\n",
      "\n",
      "   micro avg       0.73      0.73      0.73     49679\n",
      "   macro avg       0.52      0.79      0.53     49679\n",
      "weighted avg       0.94      0.73      0.81     49679\n",
      "\n",
      "[0.35335561088176926, 2.5328450239649096, 0.49328343343649833]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30913  4469   126  8513]\n",
      " [  528  1165   319    38]\n",
      " [   40   251  2362   567]\n",
      " [   13     0     7   368]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.70      0.82     44021\n",
      "           1       0.20      0.57      0.29      2050\n",
      "           2       0.84      0.73      0.78      3220\n",
      "           3       0.04      0.95      0.07       388\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     49679\n",
      "   macro avg       0.51      0.74      0.49     49679\n",
      "weighted avg       0.93      0.70      0.79     49679\n",
      "\n",
      "[0.30093057297025216, 2.339168528977189, 0.4428613526072747]\n",
      "[[34264  2626   435  6696]\n",
      " [  270  1455   307    18]\n",
      " [  123   622  2238   237]\n",
      " [   15     0    11   362]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87     44021\n",
      "           1       0.31      0.71      0.43      2050\n",
      "           2       0.75      0.70      0.72      3220\n",
      "           3       0.05      0.93      0.09       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.52      0.78      0.53     49679\n",
      "weighted avg       0.94      0.77      0.84     49679\n",
      "\n",
      "[0.3863037323026547, 2.462408881072667, 0.5009529762854108]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
       "   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "   tol=0.1, verbose=False), 0.5009529762854108)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0.02\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "xgboost(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y, X_test_df[df[df['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "logisticRegress(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y, X_test_df[df[df['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "Linear_D(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y, X_test_df[df[df['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "svm_model_linear(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y, X_test_df[df[df['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "\n",
    "#randomForest(train, train_labels, test, test_labels, max_depth=5, labels=['N','S','V','F'])\n",
    "#rf = randomForest(X_train_new, y_train_new ,X_test_new , y_test_new, labels=[0,1,2,3])[0]\n",
    "#Linear_D(np_non_var_1_old, y_train, np_non_var_2_old, y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261'] ['R_duration', 'R_height', 'R_amp0', 'R_amp1', 'R_amp2', 'R_amp3', 'R_amp4', 'R_amp5', 'R_amp6', 'R_amp7', 'R_amp8', 'R_amp9', 'R_prominence', 'R_areas', 'Q_duration', 'Q_height', 'Q_amp0', 'Q_amp1', 'Q_amp2', 'Q_amp3', 'Q_amp4', 'Q_amp5', 'Q_amp6', 'Q_amp7', 'Q_amp8', 'Q_amp9', 'Q_prominence', 'Q_areas', 'S_duration', 'S_height', 'S_amp0', 'S_amp1', 'S_amp2', 'S_amp3', 'S_amp4', 'S_amp5', 'S_amp6', 'S_amp7', 'S_amp8', 'S_amp9', 'S_prominence', 'S_areas', 'P_duration', 'P_height', 'P_amp0', 'P_amp1', 'P_amp2', 'P_amp3', 'P_amp4', 'P_amp5', 'P_amp6', 'P_amp7', 'P_amp8', 'P_amp9', 'P_prominence', 'P_areas', 'P_neg_duration', 'P_neg_height', 'P_neg_amp0', 'P_neg_amp1', 'P_neg_amp2', 'P_neg_amp3', 'P_neg_amp4', 'P_neg_amp5', 'P_neg_amp6', 'P_neg_amp7', 'P_neg_amp8', 'P_neg_amp9', 'P_neg_prominence', 'P_neg_areas', 'T_duration', 'T_height', 'T_amp0', 'T_amp1', 'T_amp2', 'T_amp3', 'T_amp4', 'T_amp5', 'T_amp6', 'T_amp7', 'T_amp8', 'T_amp9', 'T_prominence', 'T_areas', 'T_neg_durations', 'T_neg_height', 'T_neg_amp0', 'T_neg_amp1', 'T_neg_amp2', 'T_neg_amp3', 'T_neg_amp4', 'T_neg_amp5', 'T_neg_amp6', 'T_neg_amp7', 'T_neg_amp8', 'T_neg_amp9', 'T_neg_prominence', 'T_neg_areas', 'rr_int_pre', 'rr_int_post', 'rr_int_10', 'rr_int_50', 'rr_int_all', 'QRS_int', 'QRS_int_10', 'QRS_int_50', 'PQ_int', 'PQ_int_10', 'PQ_int_50', 'PR_int', 'PR_int_10', 'PR_int_50', 'ST_int', 'ST_int_10', 'ST_int_50', 'RT_int', 'RT_int_10', 'RT_int_50', 'PT_int', 'PT_int_10', 'PT_int_50', 'RP', 'TR', 'neg_RQ', 'neg_PR', 'neg_ST', 'neg_RT', 'neg_PT', 'P_neg_T', 'neg_P_neg_T', 'R_duration_V', 'R_height_V', 'R_amp0_V', 'R_amp1_V', 'R_amp2_V', 'R_amp3_V', 'R_amp4_V', 'R_amp5_V', 'R_amp6_V', 'R_amp7_V', 'R_amp8_V', 'R_amp9_V', 'R_prominence_V', 'R_areas_V', 'Q_duration_V', 'Q_height_V', 'Q_amp0_V', 'Q_amp1_V', 'Q_amp2_V', 'Q_amp3_V', 'Q_amp4_V', 'Q_amp5_V', 'Q_amp6_V', 'Q_amp7_V', 'Q_amp8_V', 'Q_amp9_V', 'Q_prominence_V', 'Q_areas_V', 'S_duration_V', 'S_height_V', 'S_amp0_V', 'S_amp1_V', 'S_amp2_V', 'S_amp3_V', 'S_amp4_V', 'S_amp5_V', 'S_amp6_V', 'S_amp7_V', 'S_amp8_V', 'S_amp9_V', 'S_prominence_V', 'S_areas_V', 'P_duration_V', 'P_height_V', 'P_amp0_V', 'P_amp1_V', 'P_amp2_V', 'P_amp3_V', 'P_amp4_V', 'P_amp5_V', 'P_amp6_V', 'P_amp7_V', 'P_amp8_V', 'P_amp9_V', 'P_prominence_V', 'P_areas_V', 'P_neg_duration_V', 'P_neg_height_V', 'P_neg_amp0_V', 'P_neg_amp1_V', 'P_neg_amp2_V', 'P_neg_amp3_V', 'P_neg_amp4_V', 'P_neg_amp5_V', 'P_neg_amp6_V', 'P_neg_amp7_V', 'P_neg_amp8_V', 'P_neg_amp9_V', 'P_neg_prominence_V', 'P_neg_areas_V', 'T_duration_V', 'T_height_V', 'T_amp0_V', 'T_amp1_V', 'T_amp2_V', 'T_amp3_V', 'T_amp4_V', 'T_amp5_V', 'T_amp6_V', 'T_amp7_V', 'T_amp8_V', 'T_amp9_V', 'T_prominence_V', 'T_areas_V', 'T_neg_durations_V', 'T_neg_height_V', 'T_neg_amp0_V', 'T_neg_amp1_V', 'T_neg_amp2_V', 'T_neg_amp3_V', 'T_neg_amp4_V', 'T_neg_amp5_V', 'T_neg_amp6_V', 'T_neg_amp7_V', 'T_neg_amp8_V', 'T_neg_amp9_V', 'T_neg_prominence_V', 'T_neg_areas_V', 'rr_int_pre_V', 'rr_int_post_V', 'rr_int_10_V', 'rr_int_50_V', 'rr_int_all_V', 'QRS_int_V', 'QRS_int_10_V', 'QRS_int_50_V', 'PQ_int_V', 'PQ_int_10_V', 'PQ_int_50_V', 'PR_int_V', 'PR_int_10_V', 'PR_int_50_V', 'ST_int_V', 'ST_int_10_V', 'ST_int_50_V', 'RT_int_V', 'RT_int_10_V', 'RT_int_50_V', 'PT_int_V', 'PT_int_10_V', 'PT_int_50_V', 'RP_V', 'TR_V', 'neg_RQ_V', 'neg_PR_V', 'neg_ST_V', 'neg_RT_V', 'neg_PT_V', 'P_neg_T_V', 'neg_P_neg_T_V', 'dtw1', 'dtw2']\nexpected f214, f93, f102, f46, f105, f125, f182, f196, f229, f258, f65, f151, f218, f139, f224, f195, f33, f56, f31, f18, f118, f103, f9, f48, f39, f4, f245, f115, f34, f167, f148, f26, f17, f44, f86, f109, f185, f226, f260, f63, f217, f91, f10, f72, f113, f249, f236, f187, f252, f40, f152, f121, f162, f190, f163, f88, f180, f2, f110, f183, f154, f209, f14, f142, f19, f165, f75, f23, f20, f94, f235, f248, f250, f52, f155, f108, f212, f160, f131, f129, f207, f179, f261, f92, f120, f66, f173, f64, f0, f80, f145, f169, f3, f51, f135, f166, f146, f222, f53, f227, f193, f8, f77, f54, f144, f156, f253, f194, f96, f211, f233, f254, f177, f204, f117, f221, f161, f27, f15, f42, f184, f59, f12, f97, f90, f138, f6, f5, f76, f176, f191, f239, f41, f237, f68, f198, f82, f157, f208, f87, f78, f192, f189, f197, f219, f16, f127, f71, f247, f47, f257, f172, f111, f220, f58, f60, f171, f28, f243, f25, f100, f175, f61, f123, f79, f232, f69, f158, f74, f85, f95, f244, f231, f210, f149, f188, f106, f49, f246, f36, f35, f114, f1, f234, f242, f255, f251, f37, f83, f29, f101, f147, f13, f225, f141, f107, f200, f24, f168, f112, f186, f153, f201, f104, f181, f81, f30, f43, f159, f136, f215, f122, f89, f134, f143, f132, f205, f98, f213, f99, f128, f203, f124, f170, f50, f22, f11, f45, f57, f84, f116, f199, f73, f126, f238, f21, f67, f70, f7, f240, f230, f137, f150, f55, f174, f133, f62, f32, f140, f178, f241, f259, f256, f38, f130, f206, f164, f228, f216, f223, f202, f119 in input data\ntraining data did not have the following fields: Q_amp2, R_amp8, PT_int, T_amp2, Q_duration, P_neg_amp0, T_areas_V, PT_int_10, PR_int_10_V, S_amp8_V, P_duration_V, P_neg_amp5, Q_amp5_V, P_neg_amp9, T_neg_amp9_V, P_neg_amp9_V, P_amp4, neg_ST, P_amp2, R_height_V, R_amp7_V, P_amp3, P_prominence_V, Q_amp4, R_amp8_V, T_areas, T_neg_durations_V, R_amp0_V, P_amp9, rr_int_all_V, RP_V, Q_areas_V, R_amp5_V, neg_PR_V, Q_amp5, T_amp6, S_height_V, P_neg_T_V, T_amp4, R_prominence, T_amp3, T_neg_amp5, T_neg_prominence, T_neg_amp4, R_duration_V, S_amp0_V, P_neg_amp2, P_amp5_V, P_areas_V, R_amp1, R_amp4_V, Q_amp8_V, T_amp5, P_neg_amp8, neg_RT_V, Q_amp9, S_amp7, P_neg_areas_V, ST_int_50_V, P_neg_amp7_V, T_height_V, T_amp9_V, P_duration, Q_height, rr_int_post_V, P_neg_prominence_V, T_amp1_V, S_amp3_V, T_neg_amp0, S_amp1_V, S_amp9_V, S_height, S_amp7_V, P_amp0_V, T_neg_amp1_V, T_neg_amp0_V, PQ_int_10_V, neg_RQ_V, P_neg_amp7, T_amp7, P_neg_amp4_V, P_neg_amp4, T_height, RT_int_10_V, P_amp1, QRS_int_50, PQ_int, Q_amp0, P_neg_amp1, TR, P_neg_amp8_V, RT_int_V, S_amp5_V, S_amp9, TR_V, R_amp7, P_neg_T, S_amp4, neg_RQ, P_neg_amp0_V, P_amp5, T_amp1, Q_areas, P_neg_areas, T_amp9, neg_RT, T_neg_amp2, S_amp5, Q_amp7_V, T_neg_height, P_neg_duration_V, RT_int, R_amp3_V, T_amp5_V, T_neg_prominence_V, ST_int_10_V, T_neg_amp4_V, P_amp9_V, T_duration_V, T_neg_areas_V, P_height_V, T_neg_amp1, P_amp6_V, S_amp2, Q_amp6, neg_P_neg_T, R_amp4, neg_PR, PQ_int_10, P_amp2_V, T_neg_amp7_V, rr_int_pre, PR_int_50_V, T_neg_amp3, P_amp3_V, R_amp5, T_amp7_V, T_neg_height_V, P_amp7, P_height, P_prominence, S_amp6_V, P_amp4_V, PR_int_V, R_amp1_V, R_areas_V, S_prominence_V, rr_int_all, T_amp6_V, rr_int_50_V, S_prominence, P_amp1_V, rr_int_10_V, QRS_int_50_V, P_neg_amp3, R_amp2, P_neg_amp6_V, S_amp1, QRS_int_V, T_amp3_V, neg_ST_V, R_amp6_V, P_areas, Q_duration_V, PT_int_50_V, R_amp6, Q_amp3_V, T_amp8_V, R_height, Q_amp1_V, rr_int_50, PT_int_V, Q_amp7, RT_int_50, T_neg_durations, S_areas, R_prominence_V, T_amp8, Q_amp1, PQ_int_V, Q_prominence, P_neg_height, PR_int, PT_int_10_V, neg_PT, P_amp8, ST_int, S_amp2_V, PQ_int_50, P_amp7_V, S_amp6, P_amp0, Q_prominence_V, T_amp0_V, PR_int_50, neg_PT_V, S_amp8, P_neg_amp1_V, T_amp0, dtw2, ST_int_50, ST_int_V, R_amp9_V, R_amp0, P_amp8_V, R_amp3, P_amp6, T_neg_amp6, QRS_int, P_neg_amp5_V, RT_int_50_V, T_amp4_V, Q_amp6_V, S_amp4_V, RT_int_10, Q_amp3, T_duration, T_neg_amp9, ST_int_10, R_amp2_V, Q_amp0_V, Q_height_V, QRS_int_10_V, T_neg_areas, Q_amp8, T_amp2_V, S_amp0, QRS_int_10, T_neg_amp7, P_neg_amp2_V, neg_P_neg_T_V, P_neg_height_V, Q_amp4_V, T_prominence_V, S_duration, P_neg_amp6, T_neg_amp6_V, PQ_int_50_V, P_neg_amp3_V, T_neg_amp3_V, S_duration_V, T_neg_amp2_V, T_neg_amp8_V, PR_int_10, Q_amp2_V, rr_int_pre_V, T_neg_amp8, rr_int_post, T_prominence, PT_int_50, rr_int_10, S_amp3, R_amp9, RP, T_neg_amp5_V, R_duration, P_neg_duration, P_neg_prominence, dtw1, R_areas, S_areas_V, Q_amp9_V",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-05ce45ec98d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msvm_model_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_features_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgood_features_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mxgboost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_features_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgood_features_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlogisticRegress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_features_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgood_features_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-89157572ee64>\u001b[0m in \u001b[0;36mxgboost\u001b[1;34m(X_train, y_train, X_test, y_test, max_depth, eta, gamma, min_child_weight, max_delta_step, subsample, colsample_bytree, colsample_bylevel, colsample_bynode, alpha, reg_lambda, tree_method, grow_policy, refresh_leaf, process_type, predictor, labels)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubsample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_delta_step\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmax_delta_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolsample_bylevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolsample_bylevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolsample_bynode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolsample_bynode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreg_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg_lambda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrow_policy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrow_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrefresh_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrefresh_leaf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocess_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprocess_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'reg:logistic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[0;32m    789\u001b[0m                                                  \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m                                                  validate_features=validate_features)\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;31m# If output_margin is active, simply return the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[1;32m-> 1690\u001b[1;33m                                             data.feature_names))\n\u001b[0m\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261'] ['R_duration', 'R_height', 'R_amp0', 'R_amp1', 'R_amp2', 'R_amp3', 'R_amp4', 'R_amp5', 'R_amp6', 'R_amp7', 'R_amp8', 'R_amp9', 'R_prominence', 'R_areas', 'Q_duration', 'Q_height', 'Q_amp0', 'Q_amp1', 'Q_amp2', 'Q_amp3', 'Q_amp4', 'Q_amp5', 'Q_amp6', 'Q_amp7', 'Q_amp8', 'Q_amp9', 'Q_prominence', 'Q_areas', 'S_duration', 'S_height', 'S_amp0', 'S_amp1', 'S_amp2', 'S_amp3', 'S_amp4', 'S_amp5', 'S_amp6', 'S_amp7', 'S_amp8', 'S_amp9', 'S_prominence', 'S_areas', 'P_duration', 'P_height', 'P_amp0', 'P_amp1', 'P_amp2', 'P_amp3', 'P_amp4', 'P_amp5', 'P_amp6', 'P_amp7', 'P_amp8', 'P_amp9', 'P_prominence', 'P_areas', 'P_neg_duration', 'P_neg_height', 'P_neg_amp0', 'P_neg_amp1', 'P_neg_amp2', 'P_neg_amp3', 'P_neg_amp4', 'P_neg_amp5', 'P_neg_amp6', 'P_neg_amp7', 'P_neg_amp8', 'P_neg_amp9', 'P_neg_prominence', 'P_neg_areas', 'T_duration', 'T_height', 'T_amp0', 'T_amp1', 'T_amp2', 'T_amp3', 'T_amp4', 'T_amp5', 'T_amp6', 'T_amp7', 'T_amp8', 'T_amp9', 'T_prominence', 'T_areas', 'T_neg_durations', 'T_neg_height', 'T_neg_amp0', 'T_neg_amp1', 'T_neg_amp2', 'T_neg_amp3', 'T_neg_amp4', 'T_neg_amp5', 'T_neg_amp6', 'T_neg_amp7', 'T_neg_amp8', 'T_neg_amp9', 'T_neg_prominence', 'T_neg_areas', 'rr_int_pre', 'rr_int_post', 'rr_int_10', 'rr_int_50', 'rr_int_all', 'QRS_int', 'QRS_int_10', 'QRS_int_50', 'PQ_int', 'PQ_int_10', 'PQ_int_50', 'PR_int', 'PR_int_10', 'PR_int_50', 'ST_int', 'ST_int_10', 'ST_int_50', 'RT_int', 'RT_int_10', 'RT_int_50', 'PT_int', 'PT_int_10', 'PT_int_50', 'RP', 'TR', 'neg_RQ', 'neg_PR', 'neg_ST', 'neg_RT', 'neg_PT', 'P_neg_T', 'neg_P_neg_T', 'R_duration_V', 'R_height_V', 'R_amp0_V', 'R_amp1_V', 'R_amp2_V', 'R_amp3_V', 'R_amp4_V', 'R_amp5_V', 'R_amp6_V', 'R_amp7_V', 'R_amp8_V', 'R_amp9_V', 'R_prominence_V', 'R_areas_V', 'Q_duration_V', 'Q_height_V', 'Q_amp0_V', 'Q_amp1_V', 'Q_amp2_V', 'Q_amp3_V', 'Q_amp4_V', 'Q_amp5_V', 'Q_amp6_V', 'Q_amp7_V', 'Q_amp8_V', 'Q_amp9_V', 'Q_prominence_V', 'Q_areas_V', 'S_duration_V', 'S_height_V', 'S_amp0_V', 'S_amp1_V', 'S_amp2_V', 'S_amp3_V', 'S_amp4_V', 'S_amp5_V', 'S_amp6_V', 'S_amp7_V', 'S_amp8_V', 'S_amp9_V', 'S_prominence_V', 'S_areas_V', 'P_duration_V', 'P_height_V', 'P_amp0_V', 'P_amp1_V', 'P_amp2_V', 'P_amp3_V', 'P_amp4_V', 'P_amp5_V', 'P_amp6_V', 'P_amp7_V', 'P_amp8_V', 'P_amp9_V', 'P_prominence_V', 'P_areas_V', 'P_neg_duration_V', 'P_neg_height_V', 'P_neg_amp0_V', 'P_neg_amp1_V', 'P_neg_amp2_V', 'P_neg_amp3_V', 'P_neg_amp4_V', 'P_neg_amp5_V', 'P_neg_amp6_V', 'P_neg_amp7_V', 'P_neg_amp8_V', 'P_neg_amp9_V', 'P_neg_prominence_V', 'P_neg_areas_V', 'T_duration_V', 'T_height_V', 'T_amp0_V', 'T_amp1_V', 'T_amp2_V', 'T_amp3_V', 'T_amp4_V', 'T_amp5_V', 'T_amp6_V', 'T_amp7_V', 'T_amp8_V', 'T_amp9_V', 'T_prominence_V', 'T_areas_V', 'T_neg_durations_V', 'T_neg_height_V', 'T_neg_amp0_V', 'T_neg_amp1_V', 'T_neg_amp2_V', 'T_neg_amp3_V', 'T_neg_amp4_V', 'T_neg_amp5_V', 'T_neg_amp6_V', 'T_neg_amp7_V', 'T_neg_amp8_V', 'T_neg_amp9_V', 'T_neg_prominence_V', 'T_neg_areas_V', 'rr_int_pre_V', 'rr_int_post_V', 'rr_int_10_V', 'rr_int_50_V', 'rr_int_all_V', 'QRS_int_V', 'QRS_int_10_V', 'QRS_int_50_V', 'PQ_int_V', 'PQ_int_10_V', 'PQ_int_50_V', 'PR_int_V', 'PR_int_10_V', 'PR_int_50_V', 'ST_int_V', 'ST_int_10_V', 'ST_int_50_V', 'RT_int_V', 'RT_int_10_V', 'RT_int_50_V', 'PT_int_V', 'PT_int_10_V', 'PT_int_50_V', 'RP_V', 'TR_V', 'neg_RQ_V', 'neg_PR_V', 'neg_ST_V', 'neg_RT_V', 'neg_PT_V', 'P_neg_T_V', 'neg_P_neg_T_V', 'dtw1', 'dtw2']\nexpected f214, f93, f102, f46, f105, f125, f182, f196, f229, f258, f65, f151, f218, f139, f224, f195, f33, f56, f31, f18, f118, f103, f9, f48, f39, f4, f245, f115, f34, f167, f148, f26, f17, f44, f86, f109, f185, f226, f260, f63, f217, f91, f10, f72, f113, f249, f236, f187, f252, f40, f152, f121, f162, f190, f163, f88, f180, f2, f110, f183, f154, f209, f14, f142, f19, f165, f75, f23, f20, f94, f235, f248, f250, f52, f155, f108, f212, f160, f131, f129, f207, f179, f261, f92, f120, f66, f173, f64, f0, f80, f145, f169, f3, f51, f135, f166, f146, f222, f53, f227, f193, f8, f77, f54, f144, f156, f253, f194, f96, f211, f233, f254, f177, f204, f117, f221, f161, f27, f15, f42, f184, f59, f12, f97, f90, f138, f6, f5, f76, f176, f191, f239, f41, f237, f68, f198, f82, f157, f208, f87, f78, f192, f189, f197, f219, f16, f127, f71, f247, f47, f257, f172, f111, f220, f58, f60, f171, f28, f243, f25, f100, f175, f61, f123, f79, f232, f69, f158, f74, f85, f95, f244, f231, f210, f149, f188, f106, f49, f246, f36, f35, f114, f1, f234, f242, f255, f251, f37, f83, f29, f101, f147, f13, f225, f141, f107, f200, f24, f168, f112, f186, f153, f201, f104, f181, f81, f30, f43, f159, f136, f215, f122, f89, f134, f143, f132, f205, f98, f213, f99, f128, f203, f124, f170, f50, f22, f11, f45, f57, f84, f116, f199, f73, f126, f238, f21, f67, f70, f7, f240, f230, f137, f150, f55, f174, f133, f62, f32, f140, f178, f241, f259, f256, f38, f130, f206, f164, f228, f216, f223, f202, f119 in input data\ntraining data did not have the following fields: Q_amp2, R_amp8, PT_int, T_amp2, Q_duration, P_neg_amp0, T_areas_V, PT_int_10, PR_int_10_V, S_amp8_V, P_duration_V, P_neg_amp5, Q_amp5_V, P_neg_amp9, T_neg_amp9_V, P_neg_amp9_V, P_amp4, neg_ST, P_amp2, R_height_V, R_amp7_V, P_amp3, P_prominence_V, Q_amp4, R_amp8_V, T_areas, T_neg_durations_V, R_amp0_V, P_amp9, rr_int_all_V, RP_V, Q_areas_V, R_amp5_V, neg_PR_V, Q_amp5, T_amp6, S_height_V, P_neg_T_V, T_amp4, R_prominence, T_amp3, T_neg_amp5, T_neg_prominence, T_neg_amp4, R_duration_V, S_amp0_V, P_neg_amp2, P_amp5_V, P_areas_V, R_amp1, R_amp4_V, Q_amp8_V, T_amp5, P_neg_amp8, neg_RT_V, Q_amp9, S_amp7, P_neg_areas_V, ST_int_50_V, P_neg_amp7_V, T_height_V, T_amp9_V, P_duration, Q_height, rr_int_post_V, P_neg_prominence_V, T_amp1_V, S_amp3_V, T_neg_amp0, S_amp1_V, S_amp9_V, S_height, S_amp7_V, P_amp0_V, T_neg_amp1_V, T_neg_amp0_V, PQ_int_10_V, neg_RQ_V, P_neg_amp7, T_amp7, P_neg_amp4_V, P_neg_amp4, T_height, RT_int_10_V, P_amp1, QRS_int_50, PQ_int, Q_amp0, P_neg_amp1, TR, P_neg_amp8_V, RT_int_V, S_amp5_V, S_amp9, TR_V, R_amp7, P_neg_T, S_amp4, neg_RQ, P_neg_amp0_V, P_amp5, T_amp1, Q_areas, P_neg_areas, T_amp9, neg_RT, T_neg_amp2, S_amp5, Q_amp7_V, T_neg_height, P_neg_duration_V, RT_int, R_amp3_V, T_amp5_V, T_neg_prominence_V, ST_int_10_V, T_neg_amp4_V, P_amp9_V, T_duration_V, T_neg_areas_V, P_height_V, T_neg_amp1, P_amp6_V, S_amp2, Q_amp6, neg_P_neg_T, R_amp4, neg_PR, PQ_int_10, P_amp2_V, T_neg_amp7_V, rr_int_pre, PR_int_50_V, T_neg_amp3, P_amp3_V, R_amp5, T_amp7_V, T_neg_height_V, P_amp7, P_height, P_prominence, S_amp6_V, P_amp4_V, PR_int_V, R_amp1_V, R_areas_V, S_prominence_V, rr_int_all, T_amp6_V, rr_int_50_V, S_prominence, P_amp1_V, rr_int_10_V, QRS_int_50_V, P_neg_amp3, R_amp2, P_neg_amp6_V, S_amp1, QRS_int_V, T_amp3_V, neg_ST_V, R_amp6_V, P_areas, Q_duration_V, PT_int_50_V, R_amp6, Q_amp3_V, T_amp8_V, R_height, Q_amp1_V, rr_int_50, PT_int_V, Q_amp7, RT_int_50, T_neg_durations, S_areas, R_prominence_V, T_amp8, Q_amp1, PQ_int_V, Q_prominence, P_neg_height, PR_int, PT_int_10_V, neg_PT, P_amp8, ST_int, S_amp2_V, PQ_int_50, P_amp7_V, S_amp6, P_amp0, Q_prominence_V, T_amp0_V, PR_int_50, neg_PT_V, S_amp8, P_neg_amp1_V, T_amp0, dtw2, ST_int_50, ST_int_V, R_amp9_V, R_amp0, P_amp8_V, R_amp3, P_amp6, T_neg_amp6, QRS_int, P_neg_amp5_V, RT_int_50_V, T_amp4_V, Q_amp6_V, S_amp4_V, RT_int_10, Q_amp3, T_duration, T_neg_amp9, ST_int_10, R_amp2_V, Q_amp0_V, Q_height_V, QRS_int_10_V, T_neg_areas, Q_amp8, T_amp2_V, S_amp0, QRS_int_10, T_neg_amp7, P_neg_amp2_V, neg_P_neg_T_V, P_neg_height_V, Q_amp4_V, T_prominence_V, S_duration, P_neg_amp6, T_neg_amp6_V, PQ_int_50_V, P_neg_amp3_V, T_neg_amp3_V, S_duration_V, T_neg_amp2_V, T_neg_amp8_V, PR_int_10, Q_amp2_V, rr_int_pre_V, T_neg_amp8, rr_int_post, T_prominence, PT_int_50, rr_int_10, S_amp3, R_amp9, RP, T_neg_amp5_V, R_duration, P_neg_duration, P_neg_prominence, dtw1, R_areas, S_areas_V, Q_amp9_V"
     ]
    }
   ],
   "source": [
    "score = 0.1\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "xgboost(good_features_X, good_features_y, X_test, y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "logisticRegress(good_features_X, good_features_y, X_test, y_test, labels=[0,1,2,3])\n",
    "Linear_D(good_features_X, good_features_y, X_test, y_test, labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "svm_model_linear(good_features_X, good_features_y, X_test, y_test, labels=[0,1,2,3])\n",
    "\n",
    "#randomForest(train, train_labels, test, test_labels, max_depth=5, labels=['N','S','V','F'])\n",
    "#rf = randomForest(X_train_new, y_train_new ,X_test_new , y_test_new, labels=[0,1,2,3])[0]\n",
    "#Linear_D(np_non_var_1_old, y_train, np_non_var_2_old, y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1656,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_features_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40286   194    90  3451]\n",
      " [ 1725    28   260    37]\n",
      " [  341    55  2421   403]\n",
      " [   26     0     6   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93     44021\n",
      "           1       0.10      0.01      0.02      2050\n",
      "           2       0.87      0.75      0.81      3220\n",
      "           3       0.08      0.92      0.15       388\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     49679\n",
      "   macro avg       0.50      0.65      0.48     49679\n",
      "weighted avg       0.90      0.87      0.88     49679\n",
      "\n",
      "[0.44652078814722895, 1.7384090282630495, 0.44056152260649567]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " 0.44056152260649567)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear_D(X_train[df[df['rfscore'] >= score]['features'].values], y_train, X_test[df[df['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n",
      "done norm\n",
      "row 140 done\n",
      "done norm\n",
      "row 141 done\n",
      "done norm\n",
      "row 142 done\n",
      "done norm\n",
      "row 143 done\n",
      "done norm\n",
      "row 144 done\n",
      "done norm\n",
      "row 145 done\n",
      "done norm\n",
      "row 146 done\n",
      "done norm\n",
      "row 147 done\n",
      "done norm\n",
      "row 148 done\n",
      "done norm\n",
      "row 149 done\n",
      "done norm\n",
      "row 150 done\n",
      "done norm\n",
      "row 151 done\n",
      "done norm\n",
      "row 152 done\n",
      "done norm\n",
      "row 153 done\n",
      "done norm\n",
      "row 154 done\n",
      "done norm\n",
      "row 155 done\n",
      "done norm\n",
      "row 156 done\n",
      "done norm\n",
      "row 157 done\n",
      "done norm\n",
      "row 158 done\n",
      "done norm\n",
      "row 159 done\n",
      "done norm\n",
      "row 160 done\n",
      "done norm\n",
      "row 161 done\n",
      "done norm\n",
      "row 162 done\n",
      "done norm\n",
      "row 163 done\n",
      "done norm\n",
      "row 164 done\n",
      "done norm\n",
      "row 165 done\n",
      "done norm\n",
      "row 166 done\n",
      "done norm\n",
      "row 167 done\n",
      "done norm\n",
      "row 168 done\n",
      "done norm\n",
      "row 169 done\n",
      "done norm\n",
      "row 170 done\n",
      "done norm\n",
      "row 171 done\n",
      "done norm\n",
      "row 172 done\n",
      "done norm\n",
      "row 173 done\n",
      "done norm\n",
      "row 174 done\n",
      "done norm\n",
      "row 175 done\n",
      "done norm\n",
      "row 176 done\n",
      "done norm\n",
      "row 177 done\n",
      "done norm\n",
      "row 178 done\n",
      "done norm\n",
      "row 179 done\n",
      "done norm\n",
      "row 180 done\n",
      "done norm\n",
      "row 181 done\n",
      "done norm\n",
      "row 182 done\n",
      "done norm\n",
      "row 183 done\n",
      "done norm\n",
      "row 184 done\n",
      "done norm\n",
      "row 185 done\n",
      "done norm\n",
      "row 186 done\n",
      "done norm\n",
      "row 187 done\n",
      "done norm\n",
      "row 188 done\n",
      "done norm\n",
      "row 189 done\n",
      "done norm\n",
      "row 190 done\n",
      "done norm\n",
      "row 191 done\n",
      "done norm\n",
      "row 192 done\n",
      "done norm\n",
      "row 193 done\n",
      "done norm\n",
      "row 194 done\n",
      "done norm\n",
      "row 195 done\n",
      "done norm\n",
      "row 196 done\n",
      "done norm\n",
      "row 197 done\n",
      "done norm\n",
      "row 198 done\n",
      "done norm\n",
      "row 199 done\n",
      "done norm\n",
      "row 200 done\n",
      "done norm\n",
      "row 201 done\n",
      "done norm\n",
      "row 202 done\n",
      "done norm\n",
      "row 203 done\n",
      "done norm\n",
      "row 204 done\n",
      "done norm\n",
      "row 205 done\n",
      "done norm\n",
      "row 206 done\n",
      "done norm\n",
      "row 207 done\n",
      "done norm\n",
      "row 208 done\n",
      "done norm\n",
      "row 209 done\n",
      "done norm\n",
      "row 210 done\n",
      "done norm\n",
      "row 211 done\n",
      "done norm\n",
      "row 212 done\n",
      "done norm\n",
      "row 213 done\n",
      "done norm\n",
      "row 214 done\n",
      "done norm\n",
      "row 215 done\n",
      "done norm\n",
      "row 216 done\n",
      "done norm\n",
      "row 217 done\n",
      "done norm\n",
      "row 218 done\n",
      "done norm\n",
      "row 219 done\n",
      "done norm\n",
      "row 220 done\n",
      "done norm\n",
      "row 221 done\n",
      "done norm\n",
      "row 222 done\n",
      "done norm\n",
      "row 223 done\n",
      "done norm\n",
      "row 224 done\n",
      "done norm\n",
      "row 225 done\n",
      "done norm\n",
      "row 226 done\n",
      "done norm\n",
      "row 227 done\n",
      "done norm\n",
      "row 228 done\n",
      "done norm\n",
      "row 229 done\n",
      "done norm\n",
      "row 230 done\n",
      "done norm\n",
      "row 231 done\n",
      "done norm\n",
      "row 232 done\n",
      "done norm\n",
      "row 233 done\n",
      "done norm\n",
      "row 234 done\n",
      "done norm\n",
      "row 235 done\n",
      "done norm\n",
      "row 236 done\n",
      "done norm\n",
      "row 237 done\n",
      "done norm\n",
      "row 238 done\n",
      "done norm\n",
      "row 239 done\n",
      "done norm\n",
      "row 240 done\n",
      "done norm\n",
      "row 241 done\n",
      "done norm\n",
      "row 242 done\n",
      "done norm\n",
      "row 243 done\n",
      "done norm\n",
      "row 244 done\n",
      "done norm\n",
      "row 245 done\n",
      "done norm\n",
      "row 246 done\n",
      "done norm\n",
      "row 247 done\n",
      "done norm\n",
      "row 248 done\n",
      "done norm\n",
      "row 249 done\n",
      "done norm\n",
      "row 250 done\n",
      "done norm\n",
      "row 251 done\n",
      "done norm\n",
      "row 252 done\n",
      "done norm\n",
      "row 253 done\n",
      "done norm\n",
      "row 254 done\n",
      "done norm\n",
      "row 255 done\n",
      "done norm\n",
      "row 256 done\n",
      "done norm\n",
      "row 257 done\n",
      "done norm\n",
      "row 258 done\n",
      "done norm\n",
      "row 259 done\n",
      "done norm\n",
      "row 260 done\n",
      "done norm\n",
      "row 261 done\n",
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n",
      "done norm\n",
      "row 140 done\n",
      "done norm\n",
      "row 141 done\n",
      "done norm\n",
      "row 142 done\n",
      "done norm\n",
      "row 143 done\n",
      "done norm\n",
      "row 144 done\n",
      "done norm\n",
      "row 145 done\n",
      "done norm\n",
      "row 146 done\n",
      "done norm\n",
      "row 147 done\n",
      "done norm\n",
      "row 148 done\n",
      "done norm\n",
      "row 149 done\n",
      "done norm\n",
      "row 150 done\n",
      "done norm\n",
      "row 151 done\n",
      "done norm\n",
      "row 152 done\n",
      "done norm\n",
      "row 153 done\n",
      "done norm\n",
      "row 154 done\n",
      "done norm\n",
      "row 155 done\n",
      "done norm\n",
      "row 156 done\n",
      "done norm\n",
      "row 157 done\n",
      "done norm\n",
      "row 158 done\n",
      "done norm\n",
      "row 159 done\n",
      "done norm\n",
      "row 160 done\n",
      "done norm\n",
      "row 161 done\n",
      "done norm\n",
      "row 162 done\n",
      "done norm\n",
      "row 163 done\n",
      "done norm\n",
      "row 164 done\n",
      "done norm\n",
      "row 165 done\n",
      "done norm\n",
      "row 166 done\n",
      "done norm\n",
      "row 167 done\n",
      "done norm\n",
      "row 168 done\n",
      "done norm\n",
      "row 169 done\n",
      "done norm\n",
      "row 170 done\n",
      "done norm\n",
      "row 171 done\n",
      "done norm\n",
      "row 172 done\n",
      "done norm\n",
      "row 173 done\n",
      "done norm\n",
      "row 174 done\n",
      "done norm\n",
      "row 175 done\n",
      "done norm\n",
      "row 176 done\n",
      "done norm\n",
      "row 177 done\n",
      "done norm\n",
      "row 178 done\n",
      "done norm\n",
      "row 179 done\n",
      "done norm\n",
      "row 180 done\n",
      "done norm\n",
      "row 181 done\n",
      "done norm\n",
      "row 182 done\n",
      "done norm\n",
      "row 183 done\n",
      "done norm\n",
      "row 184 done\n",
      "done norm\n",
      "row 185 done\n",
      "done norm\n",
      "row 186 done\n",
      "done norm\n",
      "row 187 done\n",
      "done norm\n",
      "row 188 done\n",
      "done norm\n",
      "row 189 done\n",
      "done norm\n",
      "row 190 done\n",
      "done norm\n",
      "row 191 done\n",
      "done norm\n",
      "row 192 done\n",
      "done norm\n",
      "row 193 done\n",
      "done norm\n",
      "row 194 done\n",
      "done norm\n",
      "row 195 done\n",
      "done norm\n",
      "row 196 done\n",
      "done norm\n",
      "row 197 done\n",
      "done norm\n",
      "row 198 done\n",
      "done norm\n",
      "row 199 done\n",
      "done norm\n",
      "row 200 done\n",
      "done norm\n",
      "row 201 done\n",
      "done norm\n",
      "row 202 done\n",
      "done norm\n",
      "row 203 done\n",
      "done norm\n",
      "row 204 done\n",
      "done norm\n",
      "row 205 done\n",
      "done norm\n",
      "row 206 done\n",
      "done norm\n",
      "row 207 done\n",
      "done norm\n",
      "row 208 done\n",
      "done norm\n",
      "row 209 done\n",
      "done norm\n",
      "row 210 done\n",
      "done norm\n",
      "row 211 done\n",
      "done norm\n",
      "row 212 done\n",
      "done norm\n",
      "row 213 done\n",
      "done norm\n",
      "row 214 done\n",
      "done norm\n",
      "row 215 done\n",
      "done norm\n",
      "row 216 done\n",
      "done norm\n",
      "row 217 done\n",
      "done norm\n",
      "row 218 done\n",
      "done norm\n",
      "row 219 done\n",
      "done norm\n",
      "row 220 done\n",
      "done norm\n",
      "row 221 done\n",
      "done norm\n",
      "row 222 done\n",
      "done norm\n",
      "row 223 done\n",
      "done norm\n",
      "row 224 done\n",
      "done norm\n",
      "row 225 done\n",
      "done norm\n",
      "row 226 done\n",
      "done norm\n",
      "row 227 done\n",
      "done norm\n",
      "row 228 done\n",
      "done norm\n",
      "row 229 done\n",
      "done norm\n",
      "row 230 done\n",
      "done norm\n",
      "row 231 done\n",
      "done norm\n",
      "row 232 done\n",
      "done norm\n",
      "row 233 done\n",
      "done norm\n",
      "row 234 done\n",
      "done norm\n",
      "row 235 done\n",
      "done norm\n",
      "row 236 done\n",
      "done norm\n",
      "row 237 done\n",
      "done norm\n",
      "row 238 done\n",
      "done norm\n",
      "row 239 done\n",
      "done norm\n",
      "row 240 done\n",
      "done norm\n",
      "row 241 done\n",
      "done norm\n",
      "row 242 done\n",
      "done norm\n",
      "row 243 done\n",
      "done norm\n",
      "row 244 done\n",
      "done norm\n",
      "row 245 done\n",
      "done norm\n",
      "row 246 done\n",
      "done norm\n",
      "row 247 done\n",
      "done norm\n",
      "row 248 done\n",
      "done norm\n",
      "row 249 done\n",
      "done norm\n",
      "row 250 done\n",
      "done norm\n",
      "row 251 done\n",
      "done norm\n",
      "row 252 done\n",
      "done norm\n",
      "row 253 done\n",
      "done norm\n",
      "row 254 done\n",
      "done norm\n",
      "row 255 done\n",
      "done norm\n",
      "row 256 done\n",
      "done norm\n",
      "row 257 done\n",
      "done norm\n",
      "row 258 done\n",
      "done norm\n",
      "row 259 done\n",
      "done norm\n",
      "row 260 done\n",
      "done norm\n",
      "row 261 done\n",
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n",
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n"
     ]
    }
   ],
   "source": [
    "norm_var_1_clinic = post_features.normalised_values_multiples(X_train_new)\n",
    "norm_var_2_clinic = post_features.normalised_values_multiples(np_clinic_2)\n",
    "\n",
    "norm_var_1_non = post_features.normalised_values_multiples(np_non_var_1)\n",
    "norm_var_2_non = post_features.normalised_values_multiples(np_non_var_2)\n",
    "\n",
    "X_train_new = np.hstack((X_train_new,norm_var_1_clinic))\n",
    "X_test_new = np.hstack((np_clinic_2,norm_var_2_clinic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26230  2439  1108  4357]\n",
      " [  210  1381    50    13]\n",
      " [   15    53  2436    73]\n",
      " [    7     4    11   270]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87     34134\n",
      "           1       0.36      0.83      0.50      1654\n",
      "           2       0.68      0.95      0.79      2577\n",
      "           3       0.06      0.92      0.11       292\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     38657\n",
      "   macro avg       0.52      0.87      0.57     38657\n",
      "weighted avg       0.94      0.78      0.84     38657\n",
      "\n",
      "[0.4383588429181813, 2.812162207099102, 0.5706996973464784]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01224338, 0.00262943, 0.00156042, 0.0026396 , 0.00252463,\n",
       "       0.0038379 , 0.00431359, 0.00502345, 0.00747251, 0.00748938,\n",
       "       0.00539596, 0.00935091, 0.00251766, 0.00542588, 0.00263724,\n",
       "       0.00438373, 0.00066302, 0.00087843, 0.00056888, 0.00062152,\n",
       "       0.0008627 , 0.00065381, 0.00055323, 0.00032957, 0.00039459,\n",
       "       0.0005349 , 0.00551031, 0.00042176, 0.0007129 , 0.00334684,\n",
       "       0.0019257 , 0.00081275, 0.00112662, 0.00084212, 0.00069075,\n",
       "       0.00075486, 0.00061316, 0.00056535, 0.00068565, 0.00081098,\n",
       "       0.00314891, 0.00050349, 0.00255472, 0.00514555, 0.00160234,\n",
       "       0.00131449, 0.00105005, 0.00051623, 0.00083934, 0.00114121,\n",
       "       0.00076764, 0.00085289, 0.00112406, 0.00226696, 0.00426258,\n",
       "       0.00138857, 0.00082762, 0.00055925, 0.00051742, 0.00041184,\n",
       "       0.00041901, 0.00036265, 0.00071274, 0.00067027, 0.00044498,\n",
       "       0.00044325, 0.00046753, 0.0004081 , 0.000805  , 0.00020019,\n",
       "       0.00043018, 0.00106789, 0.00248378, 0.00265278, 0.00189291,\n",
       "       0.00184204, 0.00210266, 0.00274258, 0.00207487, 0.00316406,\n",
       "       0.00264686, 0.00187436, 0.0008146 , 0.00123359, 0.00046198,\n",
       "       0.00069086, 0.00112473, 0.00079531, 0.00093784, 0.00076791,\n",
       "       0.00103497, 0.00075627, 0.00080812, 0.00064699, 0.00081963,\n",
       "       0.00110986, 0.00057293, 0.0003378 , 0.02292064, 0.01386103,\n",
       "       0.00217047, 0.00303582, 0.01267603, 0.00158328, 0.00148704,\n",
       "       0.00462167, 0.00086601, 0.00086489, 0.00526642, 0.00054975,\n",
       "       0.00057712, 0.00084984, 0.00107717, 0.00053642, 0.00127513,\n",
       "       0.00216982, 0.00038212, 0.00055669, 0.00156363, 0.00058668,\n",
       "       0.00084794, 0.00041537, 0.00281505, 0.01140116, 0.0013349 ,\n",
       "       0.00028165, 0.00053057, 0.0006802 , 0.00023976, 0.00047246,\n",
       "       0.00131102, 0.00057662, 0.00312039, 0.00513619, 0.00605399,\n",
       "       0.00739781, 0.00506389, 0.00305184, 0.00281698, 0.00125132,\n",
       "       0.00126305, 0.00150527, 0.00059736, 0.00059578, 0.00018546,\n",
       "       0.00061864, 0.0005229 , 0.0002718 , 0.00042817, 0.00043164,\n",
       "       0.00045064, 0.00056319, 0.00047552, 0.00045973, 0.00026252,\n",
       "       0.0002809 , 0.00074514, 0.00018571, 0.00039692, 0.00051256,\n",
       "       0.00070385, 0.00099831, 0.00092603, 0.00053369, 0.00039832,\n",
       "       0.00031919, 0.00044101, 0.00082784, 0.00192196, 0.00218532,\n",
       "       0.00037694, 0.00017444, 0.00168145, 0.00176751, 0.00024414,\n",
       "       0.00036   , 0.0004181 , 0.00051378, 0.00036105, 0.00038338,\n",
       "       0.00034804, 0.0002589 , 0.000304  , 0.00031206, 0.00162913,\n",
       "       0.00026984, 0.00061363, 0.00083413, 0.0002142 , 0.00025712,\n",
       "       0.00041433, 0.00027843, 0.0003876 , 0.00044382, 0.000417  ,\n",
       "       0.00031638, 0.0003138 , 0.00030621, 0.00097602, 0.00031922,\n",
       "       0.00026256, 0.0003563 , 0.00191202, 0.0024234 , 0.0034476 ,\n",
       "       0.00231445, 0.00208795, 0.00155114, 0.00131969, 0.00103416,\n",
       "       0.00132998, 0.00183364, 0.0004808 , 0.00063661, 0.0006387 ,\n",
       "       0.00044654, 0.00027104, 0.00044421, 0.00031142, 0.00055324,\n",
       "       0.00047794, 0.00026166, 0.00033448, 0.00042463, 0.00034126,\n",
       "       0.00032601, 0.0004096 , 0.00029585, 0.02421221, 0.01219526,\n",
       "       0.00178905, 0.00431016, 0.01135979, 0.00043272, 0.00114482,\n",
       "       0.00292462, 0.00546397, 0.00083128, 0.00266151, 0.00857642,\n",
       "       0.00099081, 0.00163665, 0.00075247, 0.00058088, 0.00124379,\n",
       "       0.00054376, 0.00069691, 0.00134267, 0.00044224, 0.00051535,\n",
       "       0.00083085, 0.00898154, 0.00081647, 0.00038902, 0.00020937,\n",
       "       0.00032899, 0.00132292, 0.00040385, 0.00176107, 0.00056854,\n",
       "       0.01584604, 0.00840837, 0.01353489, 0.00322475, 0.00185876,\n",
       "       0.00208506, 0.0027416 , 0.0047512 , 0.00365407, 0.00636598,\n",
       "       0.0090822 , 0.00659157, 0.0047827 , 0.01041103, 0.00270481,\n",
       "       0.00435897, 0.00381401, 0.00420243, 0.00081938, 0.00069404,\n",
       "       0.00076054, 0.00075962, 0.00084619, 0.00067943, 0.00076039,\n",
       "       0.0004165 , 0.00034506, 0.0003928 , 0.00479922, 0.00052658,\n",
       "       0.00078093, 0.00298539, 0.0025072 , 0.00070559, 0.00109745,\n",
       "       0.00116492, 0.00084926, 0.00071701, 0.00059682, 0.00055865,\n",
       "       0.00082568, 0.00083779, 0.00375561, 0.0005799 , 0.00290671,\n",
       "       0.00421676, 0.00233797, 0.00143583, 0.00115372, 0.00064498,\n",
       "       0.00096598, 0.00049644, 0.00064712, 0.00076124, 0.00121107,\n",
       "       0.00156938, 0.00461031, 0.00098159, 0.0007013 , 0.0005912 ,\n",
       "       0.00047806, 0.00040559, 0.00048789, 0.00062507, 0.0005597 ,\n",
       "       0.00062759, 0.00054117, 0.00044479, 0.00043085, 0.00042517,\n",
       "       0.00070828, 0.00015497, 0.00052598, 0.0011383 , 0.0027986 ,\n",
       "       0.00248424, 0.00203969, 0.00225802, 0.00216371, 0.0026566 ,\n",
       "       0.00222208, 0.00208794, 0.00245705, 0.00275849, 0.0010019 ,\n",
       "       0.00096343, 0.00047   , 0.00034524, 0.00107135, 0.00088878,\n",
       "       0.00103572, 0.00076646, 0.00091518, 0.00083595, 0.00095214,\n",
       "       0.00120007, 0.00096972, 0.00107694, 0.00060761, 0.00046845,\n",
       "       0.02172026, 0.0127064 , 0.00229334, 0.00392652, 0.01162305,\n",
       "       0.00158115, 0.0010978 , 0.00459283, 0.00085878, 0.00078063,\n",
       "       0.00334128, 0.00050087, 0.00076037, 0.0009177 , 0.00137417,\n",
       "       0.0006361 , 0.00129492, 0.00292933, 0.00036389, 0.00070431,\n",
       "       0.00205115, 0.00054915, 0.0007732 , 0.00038185, 0.00285012,\n",
       "       0.01432559, 0.00113425, 0.00029379, 0.00050196, 0.00071621,\n",
       "       0.00042657, 0.00046487, 0.00144108, 0.00057816, 0.00263534,\n",
       "       0.00451773, 0.00515249, 0.00595034, 0.00770519, 0.00278853,\n",
       "       0.00288903, 0.00157031, 0.00159302, 0.00160437, 0.0004848 ,\n",
       "       0.00059353, 0.00022367, 0.00079436, 0.0004313 , 0.00037593,\n",
       "       0.00053931, 0.00046924, 0.00064282, 0.00052112, 0.00053566,\n",
       "       0.00051022, 0.00030075, 0.00023865, 0.00091795, 0.00020371,\n",
       "       0.00042908, 0.00041228, 0.00067495, 0.000829  , 0.00111251,\n",
       "       0.00062539, 0.00038415, 0.00042299, 0.00052929, 0.00085986,\n",
       "       0.0009196 , 0.00213216, 0.00052283, 0.00028391, 0.00139346,\n",
       "       0.00209391, 0.00026023, 0.00039427, 0.0003839 , 0.00042156,\n",
       "       0.00045542, 0.00050257, 0.00030485, 0.00035008, 0.00021643,\n",
       "       0.00021045, 0.00204452, 0.00024385, 0.00062017, 0.00086654,\n",
       "       0.00015999, 0.00035363, 0.00044677, 0.00041129, 0.00041819,\n",
       "       0.00043424, 0.00037929, 0.00031634, 0.00025916, 0.00028512,\n",
       "       0.00065608, 0.00017253, 0.00029196, 0.00039314, 0.00234896,\n",
       "       0.00166533, 0.00262316, 0.00257591, 0.00157802, 0.00129938,\n",
       "       0.00133492, 0.00080335, 0.00143972, 0.00118185, 0.00044963,\n",
       "       0.00086332, 0.000512  , 0.00029798, 0.00027914, 0.00028575,\n",
       "       0.00039437, 0.00040671, 0.0003805 , 0.00050031, 0.00035312,\n",
       "       0.00041047, 0.00029651, 0.00044771, 0.00039702, 0.00029182,\n",
       "       0.0263042 , 0.0104813 , 0.00191002, 0.00448204, 0.01128756,\n",
       "       0.00038907, 0.00095005, 0.00271477, 0.00646456, 0.00077125,\n",
       "       0.00202205, 0.00749368, 0.00067894, 0.00170304, 0.00069283,\n",
       "       0.00066209, 0.00113865, 0.00054949, 0.00074162, 0.0017695 ,\n",
       "       0.00046398, 0.00051996, 0.00085934, 0.00671388, 0.00062581,\n",
       "       0.00038586, 0.00026709, 0.00039551, 0.00249677, 0.0004107 ,\n",
       "       0.00125977, 0.00059414, 0.01792283, 0.0069309 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = randomForest(X_train_new, y_train_new ,X_test_new , y_test_new, labels=[0,1,2,3])[0]\n",
    "\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB1.columns.get_loc('rr_int_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38657,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "norm_dtw = [\"dtw1_v1\",\"dtw2_v1\"]\n",
    "classID = [\"classID\"]\n",
    "non_clinic = list(DB1_non_cli.iloc[:,0:140].columns.values)\n",
    "\n",
    "feature_norm_mill = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_mill.append(str(feat_labels[i]+\"_n_MLII\"))\n",
    "\n",
    "feature_norm_V1 = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_V1.append(str(feat_labels[i]+\"_n_V1\"))\n",
    "    \n",
    "\n",
    "c_ID = np.asarray(classID)\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "non_cli = np.asarray(non_clinic)\n",
    "norm_mlii = np.asarray(feature_norm_mill)\n",
    "norm_v1 = np.asarray(feature_norm_V1)\n",
    "norm_dtw = np.asarray(norm_dtw)\n",
    "\n",
    "\n",
    "features_clinic = np.hstack((f_M,f_V, f_d,norm_mlii, norm_v1,norm_dtw ))\n",
    "\n",
    "#row=[]\n",
    "#for i in range(0,len(np_clinic_new_1)):\n",
    "    #row.append(i)\n",
    "    \n",
    "features = []\n",
    "for feature in zip(features_clinic, rf.feature_importances_):\n",
    "    \n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['features', 'rfscore']\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(features,columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "df[df['rfscore'] >= 0.01]['features'].values\n",
    "df.to_csv('database/features_ranking_with_norm.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_test_df = pd.DataFrame(X_test,columns=features_clinic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1558750839562633"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_test_df[rank['features'].values]['QRS_int_50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-68e755ceba25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m rank = array(['rr_int_pre_V', 'dtw1', 'rr_int_pre', 'dtw2', 'rr_int_post',\n\u001b[0m\u001b[0;32m      2\u001b[0m        \u001b[1;34m'R_duration'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'neg_RQ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rr_int_post_V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_amp7'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rr_int_all_V'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m        \u001b[1;34m'RP_V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rr_int_all'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_amp3_V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PR_int_V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_amp4_V'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m        \u001b[1;34m'PQ_int_V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_amp6'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_amp5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_amp9'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_amp2_V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_amp4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[1;34m'R_amp1_V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Q_prominence'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Q_height'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'R_areas'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rr_int_50_V'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "rank = array(['rr_int_pre_V', 'dtw1', 'rr_int_pre', 'dtw2', 'rr_int_post',\n",
    "       'R_duration', 'neg_RQ', 'rr_int_post_V', 'R_amp7', 'rr_int_all_V',\n",
    "       'RP_V', 'rr_int_all', 'R_amp3_V', 'PR_int_V', 'R_amp4_V',\n",
    "       'PQ_int_V', 'R_amp6', 'R_amp5', 'R_amp9', 'R_amp2_V', 'R_amp4',\n",
    "       'R_amp1_V', 'Q_prominence', 'Q_height', 'R_areas', 'rr_int_50_V',\n",
    "       'T_amp9', 'P_height', 'R_amp8', 'R_prominence', 'S_amp0',\n",
    "       'rr_int_50', 'T_amp2', 'R_amp3', 'QRS_int_50', 'P_prominence',\n",
    "       'T_amp1', 'R_height', 'PQ_int_50', 'TR', 'R_amp0_V', 'T_amp8',\n",
    "       'S_prominence', 'R_amp5_V', 'RT_int', 'S_amp9_V', 'S_height',\n",
    "       'T_amp3', 'T_neg_amp0', 'T_amp0', 'neg_RT_V', 'Q_duration',\n",
    "       'R_duration_V', 'S_amp2', 'T_neg_amp2', 'rr_int_10_V',\n",
    "       'PQ_int_50_V', 'P_duration_V'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0040356294719912"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_rr[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
