{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[298  48  29  13]\n",
      " [349  31   4   4]\n",
      " [ 21   5 362   0]\n",
      " [ 64   5 312   7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.77      0.53       388\n",
      "           1       0.35      0.08      0.13       388\n",
      "           2       0.51      0.93      0.66       388\n",
      "           3       0.29      0.02      0.03       388\n",
      "\n",
      "    accuracy                           0.45      1552\n",
      "   macro avg       0.39      0.45      0.34      1552\n",
      "weighted avg       0.39      0.45      0.34      1552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### voting classification\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "C_value = 10\n",
    "use_probability = True\n",
    "multi_mode = 'ovo'\n",
    "svm_model_linear = svm.SVC(C=C_value, kernel='linear', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "svm_model_rbf = svm.SVC(C=C_value, kernel='rbf', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "svm_model_poly = svm.SVC(C=C_value, kernel='poly', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=20)\n",
    "linear_dis = LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "#eclf = VotingClassifier(estimators=[('svc_1', svm_model_linear),('svc_2',svm_model_rbf), ('svc_3', svm_model_poly), ('knn', clf2), ('dt', clf1), ('ld',linear_dis)],\n",
    "                       #voting='hard', weights=[2,1,1, 1, 1,2])\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('svc_1', svm_model_linear),('svc_2',svm_model_rbf), ('svc_3', svm_model_poly)],\n",
    "                       voting='hard')#, #weights=[2,1,1, 1, 1,2])\n",
    "\n",
    "\n",
    "#clf1 = clf1.fit(X, y)\n",
    "#clf2 = clf2.fit(X, y)\n",
    "#clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X_train, y_train)\n",
    "\n",
    "predic = eclf.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predic))  \n",
    "print(classification_report(y_test,predic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clinically important for voting ensemble soft\n",
    "\n",
    "[[288  24  27  49]\n",
    " [262 107   5  14]\n",
    " [ 15  12 323  38]\n",
    " [ 14   1  14 359]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.74      0.60       388\n",
    "           1       0.74      0.28      0.40       388\n",
    "           2       0.88      0.83      0.85       388\n",
    "           3       0.78      0.93      0.85       388\n",
    "\n",
    "    accuracy                           0.69      1552\n",
    "   macro avg       0.72      0.69      0.67      1552\n",
    "weighted avg       0.72      0.69      0.67      1552\n",
    "\n",
    "## clinically important for voting ensemble hard\n",
    "\n",
    "[[315  18  18  37]\n",
    " [271 100  11   6]\n",
    " [ 19  17 317  35]\n",
    " [ 19   1  16 352]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.81      0.62       388\n",
    "           1       0.74      0.26      0.38       388\n",
    "           2       0.88      0.82      0.85       388\n",
    "           3       0.82      0.91      0.86       388\n",
    "\n",
    "    accuracy                           0.70      1552\n",
    "   macro avg       0.73      0.70      0.68      1552\n",
    "weighted avg       0.73      0.70      0.68      1552\n",
    "\n",
    "## clinically important for voting ensemble hard _ svm only\n",
    "\n",
    "\n",
    "[[311  25  22  30]\n",
    " [187 172  20   9]\n",
    " [ 26   7 308  47]\n",
    " [ 41   0  12 335]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.55      0.80      0.65       388\n",
    "           1       0.84      0.44      0.58       388\n",
    "           2       0.85      0.79      0.82       388\n",
    "           3       0.80      0.86      0.83       388\n",
    "\n",
    "    accuracy                           0.73      1552\n",
    "   macro avg       0.76      0.73      0.72      1552\n",
    "weighted avg       0.76      0.73      0.72      1552\n",
    "\n",
    "## clinically not important for voting ensemble hard\n",
    "\n",
    "\n",
    "[[313  49  14  12]\n",
    " [355  26   6   1]\n",
    " [ 45  14 318  11]\n",
    " [ 54   4  34 296]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.41      0.81      0.54       388\n",
    "           1       0.28      0.07      0.11       388\n",
    "           2       0.85      0.82      0.84       388\n",
    "           3       0.93      0.76      0.84       388\n",
    "\n",
    "    accuracy                           0.61      1552\n",
    "   macro avg       0.62      0.61      0.58      1552\n",
    "weighted avg       0.62      0.61      0.58      1552\n",
    "\n",
    "## clinically not important for voting ensemble hard _svm only\n",
    "\n",
    "\n",
    "[[298  48  29  13]\n",
    " [349  31   4   4]\n",
    " [ 21   5 362   0]\n",
    " [ 64   5 312   7]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.41      0.77      0.53       388\n",
    "           1       0.35      0.08      0.13       388\n",
    "           2       0.51      0.93      0.66       388\n",
    "           3       0.29      0.02      0.03       388\n",
    "\n",
    "    accuracy                           0.45      1552\n",
    "   macro avg       0.39      0.45      0.34      1552\n",
    "weighted avg       0.39      0.45      0.34      1552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "Epoch 1/150\n",
      "1656/1656 [==============================] - 0s 223us/step - loss: 1.1645 - acc: 0.5236\n",
      "Epoch 2/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.8256 - acc: 0.6667\n",
      "Epoch 3/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.7097 - acc: 0.7180\n",
      "Epoch 4/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.6219 - acc: 0.7585\n",
      "Epoch 5/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.5209 - acc: 0.8213\n",
      "Epoch 6/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.4631 - acc: 0.8388\n",
      "Epoch 7/150\n",
      "1656/1656 [==============================] - 0s 55us/step - loss: 0.4244 - acc: 0.8454\n",
      "Epoch 8/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.3763 - acc: 0.8744\n",
      "Epoch 9/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.3531 - acc: 0.8714\n",
      "Epoch 10/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.3364 - acc: 0.8762\n",
      "Epoch 11/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.2977 - acc: 0.8925\n",
      "Epoch 12/150\n",
      "1656/1656 [==============================] - 0s 49us/step - loss: 0.2860 - acc: 0.9034\n",
      "Epoch 13/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.2651 - acc: 0.9155\n",
      "Epoch 14/150\n",
      "1656/1656 [==============================] - 0s 57us/step - loss: 0.2501 - acc: 0.9112\n",
      "Epoch 15/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 0.2259 - acc: 0.9167\n",
      "Epoch 16/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.2176 - acc: 0.9239\n",
      "Epoch 17/150\n",
      "1656/1656 [==============================] - 0s 57us/step - loss: 0.2070 - acc: 0.9300\n",
      "Epoch 18/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.1926 - acc: 0.9336\n",
      "Epoch 19/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.1754 - acc: 0.9390\n",
      "Epoch 20/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.1681 - acc: 0.9469\n",
      "Epoch 21/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.1527 - acc: 0.9463\n",
      "Epoch 22/150\n",
      "1656/1656 [==============================] - 0s 49us/step - loss: 0.1522 - acc: 0.9444\n",
      "Epoch 23/150\n",
      "1656/1656 [==============================] - 0s 49us/step - loss: 0.1412 - acc: 0.9475\n",
      "Epoch 24/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.1325 - acc: 0.9517\n",
      "Epoch 25/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.1227 - acc: 0.9601\n",
      "Epoch 26/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.1136 - acc: 0.9607\n",
      "Epoch 27/150\n",
      "1656/1656 [==============================] - 0s 59us/step - loss: 0.1191 - acc: 0.9632\n",
      "Epoch 28/150\n",
      "1656/1656 [==============================] - 0s 52us/step - loss: 0.1023 - acc: 0.9644\n",
      "Epoch 29/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.1062 - acc: 0.9674\n",
      "Epoch 30/150\n",
      "1656/1656 [==============================] - 0s 57us/step - loss: 0.1106 - acc: 0.9656\n",
      "Epoch 31/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.0834 - acc: 0.9722\n",
      "Epoch 32/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0978 - acc: 0.9662\n",
      "Epoch 33/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0810 - acc: 0.9740\n",
      "Epoch 34/150\n",
      "1656/1656 [==============================] - 0s 49us/step - loss: 0.0822 - acc: 0.9674\n",
      "Epoch 35/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0778 - acc: 0.9771\n",
      "Epoch 36/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0707 - acc: 0.9771\n",
      "Epoch 37/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0716 - acc: 0.9783\n",
      "Epoch 38/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0702 - acc: 0.9758\n",
      "Epoch 39/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0621 - acc: 0.9795\n",
      "Epoch 40/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0661 - acc: 0.9764\n",
      "Epoch 41/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0593 - acc: 0.9783\n",
      "Epoch 42/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0513 - acc: 0.9837\n",
      "Epoch 43/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0515 - acc: 0.9819\n",
      "Epoch 44/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0569 - acc: 0.9801\n",
      "Epoch 45/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0504 - acc: 0.9807\n",
      "Epoch 46/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0577 - acc: 0.9807\n",
      "Epoch 47/150\n",
      "1656/1656 [==============================] - 0s 59us/step - loss: 0.0457 - acc: 0.9825\n",
      "Epoch 48/150\n",
      "1656/1656 [==============================] - 0s 59us/step - loss: 0.0462 - acc: 0.9843\n",
      "Epoch 49/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0476 - acc: 0.9825\n",
      "Epoch 50/150\n",
      "1656/1656 [==============================] - 0s 58us/step - loss: 0.0456 - acc: 0.9849\n",
      "Epoch 51/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.0378 - acc: 0.9873\n",
      "Epoch 52/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0518 - acc: 0.9831\n",
      "Epoch 53/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 0.0347 - acc: 0.9879\n",
      "Epoch 54/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0483 - acc: 0.9837\n",
      "Epoch 55/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0274 - acc: 0.9915\n",
      "Epoch 56/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 0.0326 - acc: 0.9885\n",
      "Epoch 57/150\n",
      "1656/1656 [==============================] - 0s 62us/step - loss: 0.0209 - acc: 0.9921\n",
      "Epoch 58/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0477 - acc: 0.9831\n",
      "Epoch 59/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 0.0384 - acc: 0.9873\n",
      "Epoch 60/150\n",
      "1656/1656 [==============================] - 0s 66us/step - loss: 0.0201 - acc: 0.9934\n",
      "Epoch 61/150\n",
      "1656/1656 [==============================] - 0s 63us/step - loss: 0.0328 - acc: 0.9891\n",
      "Epoch 62/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0292 - acc: 0.9915\n",
      "Epoch 63/150\n",
      "1656/1656 [==============================] - 0s 63us/step - loss: 0.0215 - acc: 0.9934\n",
      "Epoch 64/150\n",
      "1656/1656 [==============================] - 0s 61us/step - loss: 0.0333 - acc: 0.9909\n",
      "Epoch 65/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 0.0306 - acc: 0.9885\n",
      "Epoch 66/150\n",
      "1656/1656 [==============================] - 0s 60us/step - loss: 0.0234 - acc: 0.9909\n",
      "Epoch 67/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 0.0373 - acc: 0.9861\n",
      "Epoch 68/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0238 - acc: 0.9928\n",
      "Epoch 69/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0182 - acc: 0.9946\n",
      "Epoch 70/150\n",
      "1656/1656 [==============================] - 0s 52us/step - loss: 0.0360 - acc: 0.9903\n",
      "Epoch 71/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0140 - acc: 0.9964\n",
      "Epoch 72/150\n",
      "1656/1656 [==============================] - 0s 63us/step - loss: 0.0362 - acc: 0.9885\n",
      "Epoch 73/150\n",
      "1656/1656 [==============================] - 0s 61us/step - loss: 0.0123 - acc: 0.9970\n",
      "Epoch 74/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0261 - acc: 0.9909\n",
      "Epoch 75/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0255 - acc: 0.9921\n",
      "Epoch 76/150\n",
      "1656/1656 [==============================] - 0s 46us/step - loss: 0.0305 - acc: 0.9891\n",
      "Epoch 77/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0142 - acc: 0.9958\n",
      "Epoch 78/150\n",
      "1656/1656 [==============================] - 0s 49us/step - loss: 0.0249 - acc: 0.9928\n",
      "Epoch 79/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0163 - acc: 0.9958\n",
      "Epoch 80/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0142 - acc: 0.9946\n",
      "Epoch 81/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0134 - acc: 0.9958\n",
      "Epoch 82/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0148 - acc: 0.9964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0173 - acc: 0.9940\n",
      "Epoch 84/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0138 - acc: 0.9952\n",
      "Epoch 85/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0186 - acc: 0.9964\n",
      "Epoch 86/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0243 - acc: 0.9909\n",
      "Epoch 87/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0275 - acc: 0.9915\n",
      "Epoch 88/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0295 - acc: 0.9928\n",
      "Epoch 89/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 90/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0180 - acc: 0.9952\n",
      "Epoch 91/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0106 - acc: 0.9964\n",
      "Epoch 92/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0217 - acc: 0.9903\n",
      "Epoch 93/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.0136 - acc: 0.9952\n",
      "Epoch 94/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 0.0337 - acc: 0.9964\n",
      "Epoch 95/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 96/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0132 - acc: 0.9958\n",
      "Epoch 97/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0156 - acc: 0.9952\n",
      "Epoch 98/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0165 - acc: 0.9946\n",
      "Epoch 99/150\n",
      "1656/1656 [==============================] - 0s 46us/step - loss: 0.0152 - acc: 0.9958\n",
      "Epoch 100/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0085 - acc: 0.9982\n",
      "Epoch 101/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0211 - acc: 0.9958\n",
      "Epoch 102/150\n",
      "1656/1656 [==============================] - 0s 46us/step - loss: 0.0226 - acc: 0.9964\n",
      "Epoch 103/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0152 - acc: 0.9946\n",
      "Epoch 104/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0124 - acc: 0.9976\n",
      "Epoch 105/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0223 - acc: 0.9946\n",
      "Epoch 106/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0130 - acc: 0.9970\n",
      "Epoch 107/150\n",
      "1656/1656 [==============================] - 0s 46us/step - loss: 0.0035 - acc: 0.9988\n",
      "Epoch 108/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0228 - acc: 0.9921\n",
      "Epoch 109/150\n",
      "1656/1656 [==============================] - 0s 46us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 110/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0212 - acc: 0.9928\n",
      "Epoch 111/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0061 - acc: 0.9970\n",
      "Epoch 112/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0029 - acc: 0.9982\n",
      "Epoch 113/150\n",
      "1656/1656 [==============================] - 0s 52us/step - loss: 0.0091 - acc: 0.9988\n",
      "Epoch 114/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0126 - acc: 0.9940\n",
      "Epoch 115/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0053 - acc: 0.9976\n",
      "Epoch 116/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0127 - acc: 0.9952\n",
      "Epoch 117/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0331 - acc: 0.9921\n",
      "Epoch 118/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.0056 - acc: 0.9982\n",
      "Epoch 119/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0182 - acc: 0.9964\n",
      "Epoch 120/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0109 - acc: 0.9976\n",
      "Epoch 121/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 9.4379e-04 - acc: 0.9994\n",
      "Epoch 122/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.0172 - acc: 0.9940\n",
      "Epoch 123/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 3.2203e-04 - acc: 1.0000: 0s - loss: 2.0826e-04 - acc: 1.000\n",
      "Epoch 124/150\n",
      "1656/1656 [==============================] - 0s 49us/step - loss: 0.0187 - acc: 0.9964\n",
      "Epoch 125/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0063 - acc: 0.9982\n",
      "Epoch 126/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0081 - acc: 0.9964\n",
      "Epoch 127/150\n",
      "1656/1656 [==============================] - 0s 49us/step - loss: 0.0208 - acc: 0.9940\n",
      "Epoch 128/150\n",
      "1656/1656 [==============================] - 0s 52us/step - loss: 3.3043e-04 - acc: 1.0000\n",
      "Epoch 129/150\n",
      "1656/1656 [==============================] - 0s 44us/step - loss: 0.0081 - acc: 0.9952\n",
      "Epoch 130/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0106 - acc: 0.9976\n",
      "Epoch 131/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 5.0322e-04 - acc: 1.0000\n",
      "Epoch 132/150\n",
      "1656/1656 [==============================] - 0s 49us/step - loss: 0.0118 - acc: 0.9976\n",
      "Epoch 133/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0221 - acc: 0.9934\n",
      "Epoch 134/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0239 - acc: 0.9921\n",
      "Epoch 135/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 3.9968e-04 - acc: 1.0000\n",
      "Epoch 136/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0369 - acc: 0.9940\n",
      "Epoch 137/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0092 - acc: 0.9982\n",
      "Epoch 138/150\n",
      "1656/1656 [==============================] - 0s 56us/step - loss: 1.7076e-04 - acc: 1.0000\n",
      "Epoch 139/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0171 - acc: 0.9970\n",
      "Epoch 140/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0038 - acc: 0.9988\n",
      "Epoch 141/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0076 - acc: 0.9982\n",
      "Epoch 142/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.0267 - acc: 0.9928\n",
      "Epoch 143/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.0098 - acc: 0.9982\n",
      "Epoch 144/150\n",
      "1656/1656 [==============================] - 0s 47us/step - loss: 0.0136 - acc: 0.9964\n",
      "Epoch 145/150\n",
      "1656/1656 [==============================] - 0s 48us/step - loss: 0.0020 - acc: 0.9994\n",
      "Epoch 146/150\n",
      "1656/1656 [==============================] - 0s 45us/step - loss: 0.0241 - acc: 0.9946\n",
      "Epoch 147/150\n",
      "1656/1656 [==============================] - 0s 51us/step - loss: 0.0066 - acc: 0.9982\n",
      "Epoch 148/150\n",
      "1656/1656 [==============================] - 0s 54us/step - loss: 0.0372 - acc: 0.9952\n",
      "Epoch 149/150\n",
      "1656/1656 [==============================] - 0s 53us/step - loss: 0.0030 - acc: 0.9988\n",
      "Epoch 150/150\n",
      "1656/1656 [==============================] - 0s 50us/step - loss: 0.0077 - acc: 0.9988\n"
     ]
    }
   ],
   "source": [
    "#class_weights = {}\n",
    "#for c in range(0,2):\n",
    "   # class_weights.update({c:len(bin_labels) / float(np.count_nonzero(bin_labels == c))})\n",
    "\n",
    "input_size=X_train.shape[1]\n",
    "# Establish the NN\n",
    "eps=150\n",
    "opt=RMSprop(lr=0.001)\n",
    "print (input_size)\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "model = Sequential() # feed-forward.\n",
    "#relu = keras.activations.relu(model, alpha=0.0001, max_value=None, threshold=0.0)\n",
    "\n",
    "model.add(Dense(input_size, activation='relu', input_dim=input_size))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history=model.fit(X_train, y_train, class_weight='balanced', epochs=eps)#, callbacks=cb)\n",
    "y_pred=model.predict(X_test)\n",
    "# binary classification based on probability.\n",
    "\n",
    "#y_pred[y_pred <= 0] = 0\n",
    "#y_pred[y_pred >= 0 and <= 1] = 1\n",
    "#y_pred[y_pred >= 1 and <= 2] = 2\n",
    "#y_pred[y_pred >= 2 and <= 3] = 3\n",
    "\n",
    "#print(confusion_matrix(y_test,y_pred))  \n",
    "#print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1.064e+03, 2.300e+01, 1.300e+01, 4.000e+00, 5.000e+00, 4.000e+00,\n",
       "         5.000e+00, 3.000e+00, 3.000e+00, 4.000e+00, 2.000e+00, 4.000e+00,\n",
       "         3.000e+00, 5.000e+00, 3.000e+00, 1.000e+00, 3.000e+00, 3.000e+00,\n",
       "         5.000e+00, 3.000e+00, 5.000e+00, 4.000e+00, 4.000e+00, 1.000e+01,\n",
       "         3.690e+02]),\n",
       "  array([1.188e+03, 4.000e+00, 6.000e+00, 5.000e+00, 4.000e+00, 3.000e+00,\n",
       "         2.000e+00, 3.000e+00, 2.000e+00, 1.000e+00, 1.000e+00, 4.000e+00,\n",
       "         2.000e+00, 2.000e+00, 3.000e+00, 4.000e+00, 4.000e+00, 2.000e+00,\n",
       "         2.000e+00, 5.000e+00, 4.000e+00, 3.000e+00, 1.200e+01, 1.300e+01,\n",
       "         2.730e+02]),\n",
       "  array([1.19e+03, 1.10e+01, 5.00e+00, 3.00e+00, 2.00e+00, 2.00e+00,\n",
       "         5.00e+00, 1.00e+00, 6.00e+00, 0.00e+00, 1.00e+00, 1.00e+00,\n",
       "         0.00e+00, 2.00e+00, 1.00e+00, 3.00e+00, 1.00e+00, 3.00e+00,\n",
       "         3.00e+00, 2.00e+00, 4.00e+00, 3.00e+00, 4.00e+00, 1.00e+01,\n",
       "         2.89e+02]),\n",
       "  array([1.027e+03, 1.800e+01, 3.000e+00, 3.000e+00, 3.000e+00, 4.000e+00,\n",
       "         2.000e+00, 3.000e+00, 2.000e+00, 2.000e+00, 2.000e+00, 3.000e+00,\n",
       "         0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 4.000e+00, 1.000e+00,\n",
       "         3.000e+00, 1.000e+00, 4.000e+00, 5.000e+00, 4.000e+00, 6.000e+00,\n",
       "         4.500e+02])],\n",
       " array([0.  , 0.04, 0.08, 0.12, 0.16, 0.2 , 0.24, 0.28, 0.32, 0.36, 0.4 ,\n",
       "        0.44, 0.48, 0.52, 0.56, 0.6 , 0.64, 0.68, 0.72, 0.76, 0.8 , 0.84,\n",
       "        0.88, 0.92, 0.96, 1.  ]),\n",
       " <a list of 4 Lists of Patches objects>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(x=y_pred, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_y = y_pred[y_pred <=  0] \n",
    "#y_pred[np.all(y_pred >= 0, y_pred <= 1)] = 1\n",
    "#y_pred[y_pred >= 1 and y_pred <= 2] = 2\n",
    "#y_pred[y_pred <= 3] = 3\n",
    "\n",
    "\n",
    "#np.all(y_pred >= 0, y_pred <= 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9681938e-01, 1.8912623e-08, 2.2535528e-06, 3.1783844e-03],\n",
       "       [1.0000000e+00, 1.8767247e-15, 7.7927246e-18, 2.2804840e-09],\n",
       "       [3.9428017e-01, 6.0544050e-01, 9.0352762e-05, 1.8894635e-04],\n",
       "       ...,\n",
       "       [9.9995887e-01, 1.0362184e-05, 2.9946254e-05, 8.1101774e-07],\n",
       "       [6.8656138e-13, 1.3806310e-28, 9.9999940e-01, 5.4442171e-07],\n",
       "       [9.9997592e-01, 1.3728231e-06, 1.6730010e-05, 5.9060972e-06]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1656,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  1 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.94      1.00      0.97        15\n",
      "           2       1.00      0.94      0.97        16\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.98      0.98      0.98        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## voting with neural network\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, Adamax, Nadam\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "from LossHistory import *\n",
    "import time\n",
    "\n",
    "\n",
    "C_value = 10\n",
    "use_probability = True\n",
    "multi_mode = 'ovo'\n",
    "svm_model_linear = svm.SVC(C=C_value, kernel='linear', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "svm_model_rbf = svm.SVC(C=C_value, kernel='rbf', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "svm_model_poly = svm.SVC(C=C_value, kernel='poly', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=20)\n",
    "linear_dis = LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('svc_1', svm_model_linear), ('ld', linear_dis), ('dt', clf2)],\n",
    "                       voting='soft')#, weights=[2, 1, 2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#clf1 = clf1.fit(X, y)\n",
    "#clf2 = clf2.fit(X, y)\n",
    "#clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X_train, y_train)\n",
    "\n",
    "predic = eclf.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predic))  \n",
    "print(classification_report(y_test,predic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np_clinic_1\n",
    "X_test = np_clinic_2\n",
    "y_train = np_class_ID_1\n",
    "y_test = np_class_ID_2\n",
    "input_size=X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np_non_var_1\n",
    "X_test = np_non_var_2\n",
    "y_train = np_class_ID_1\n",
    "y_test = np_class_ID_2\n",
    "input_size=X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "X_test = test\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1656"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 14  1]\n",
      " [ 0  0 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.93      0.97        15\n",
      "           2       0.94      1.00      0.97        16\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.98      0.98      0.98        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Adaboost \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "\n",
    "#iris = load_iris()\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf.fit(X_train,y_train )\n",
    "predic = clf.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predic))  \n",
    "print(classification_report(y_test,predic)) \n",
    "\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ada boost\n",
    "\n",
    "\n",
    "# clinically important\n",
    "[[38345  2142   708  2826]\n",
    " [ 1348   588   109     5]\n",
    " [  391     6  2738    85]\n",
    " [   63     0    32   293]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.87      0.91     44021\n",
    "           1       0.21      0.29      0.25      2050\n",
    "           2       0.76      0.85      0.80      3220\n",
    "           3       0.09      0.76      0.16       388\n",
    "\n",
    "    accuracy                           0.84     49679\n",
    "   macro avg       0.51      0.69      0.53     49679\n",
    "weighted avg       0.91      0.84      0.87     49679\n",
    "\n",
    "\n",
    "### non clinically \n",
    "[[41380   433  1301   907]\n",
    " [ 1929    62    59     0]\n",
    " [  432     8  2774     6]\n",
    " [  282     0   100     6]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.94      0.94     44021\n",
    "           1       0.12      0.03      0.05      2050\n",
    "           2       0.66      0.86      0.74      3220\n",
    "           3       0.01      0.02      0.01       388\n",
    "\n",
    "    accuracy                           0.89     49679\n",
    "   macro avg       0.43      0.46      0.44     49679\n",
    "weighted avg       0.88      0.89      0.88     49679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  1 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.94      1.00      0.97        15\n",
      "           2       1.00      0.94      0.97        16\n",
      "\n",
      "    accuracy                           0.98        50\n",
      "   macro avg       0.98      0.98      0.98        50\n",
      "weighted avg       0.98      0.98      0.98        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "     #random_state=0)\n",
    "\n",
    "#clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "#scores = cross_val_score(clf, X, y, cv=5)\n",
    "#scores.mean()                               \n",
    "\n",
    "\n",
    "#clf = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n",
    "#scores = cross_val_score(clf, X, y, cv=5)\n",
    "#scores.mean()                               \n",
    "\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=input_size, max_depth=None,min_samples_split=2, random_state=0)\n",
    "\n",
    "clf.fit(X_train,y_train )\n",
    "predic = clf.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predic))  \n",
    "print(classification_report(y_test,predic)) \n",
    "#scores = cross_val_score(clf, X, y, cv=5)\n",
    "#scores.mean() > 0.999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extreme random tree\n",
    "\n",
    "## clinically imporant\n",
    "[[40794     0  3140    87]\n",
    " [ 2005     2    43     0]\n",
    " [  399     0  2821     0]\n",
    " [   45     0   343     0]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.93      0.93     44021\n",
    "           1       1.00      0.00      0.00      2050\n",
    "           2       0.44      0.88      0.59      3220\n",
    "           3       0.00      0.00      0.00       388\n",
    "\n",
    "    accuracy                           0.88     49679\n",
    "   macro avg       0.60      0.45      0.38     49679\n",
    "weighted avg       0.91      0.88      0.87     49679\n",
    "\n",
    "\n",
    "## clinically not important\n",
    "[[39345     0  4676     0]\n",
    " [ 2000    11    39     0]\n",
    " [  335     0  2885     0]\n",
    " [   59     0   328     1]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.89      0.92     44021\n",
    "           1       1.00      0.01      0.01      2050\n",
    "           2       0.36      0.90      0.52      3220\n",
    "           3       1.00      0.00      0.01       388\n",
    "\n",
    "    accuracy                           0.85     49679\n",
    "   macro avg       0.83      0.45      0.36     49679\n",
    "weighted avg       0.91      0.85      0.85     49679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  0  0]\n",
      " [ 0 11  4]\n",
      " [ 0  2 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.85      0.73      0.79        15\n",
      "           2       0.78      0.88      0.82        16\n",
      "\n",
      "    accuracy                           0.88        50\n",
      "   macro avg       0.87      0.87      0.87        50\n",
      "weighted avg       0.88      0.88      0.88        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "##bagging\n",
    "clf = BaggingClassifier(linear_dis, max_samples=0.5, max_features=0.5)\n",
    "#iris = load_iris()\n",
    "\n",
    "clf.fit(X_train,y_train )\n",
    "predic = clf.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predic))  \n",
    "print(classification_report(y_test,predic)) \n",
    "\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "C_s = np.logspace(-10, 0, 10)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "for C in C_s:\n",
    "    clf.C = C\n",
    "    this_scores = cross_val_score(clf, X, y, cv=5, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "# Do the plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.semilogx(C_s, scores)\n",
    "plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\n",
    "plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\n",
    "locs, labels = plt.yticks()\n",
    "plt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nimch681\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n"
     ]
    }
   ],
   "source": [
    "from codes.python import metric\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from numpy import array\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "from wfdb import processing, plot\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from biosppy.signals import ecg\n",
    "from sklearn import metrics\n",
    "from codes.python import ecg_waveform_extractor as waveform\n",
    "import time as system_time\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import termcolor as colored\n",
    "from math import*\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from codes.python import post_process_features_ex as post_features\n",
    "\n",
    "from collections import Counter\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, Adamax, Nadam\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "#ls = [0,1,2,3]\n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "#patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "DB1_patients = pd.read_csv(\"DB1_patient_list.csv\")\n",
    "#DB1_patients = DB1_patients.drop([1872])\n",
    "DB1_patients = DB1_patients[DB1_patients['2'].isin(ls)]\n",
    "#DB1_patients = DB1_patients[DB1_patients['0'].isin(patient_l_1)]\n",
    "\n",
    "\n",
    "DB2_patients = pd.read_csv(\"DB2_patient_list.csv\")\n",
    "#DB2_patients= DB2_patients.drop([18692, 31865])\n",
    "DB2_patients = DB2_patients[DB2_patients['2'].isin(ls)]\n",
    "#DB2_patients = DB2_patients[DB2_patients['0'].isin(patient_l_2)]\n",
    "\n",
    "\n",
    "DB1 = pd.read_csv(\"DB1.csv\")\n",
    "#DB1 = DB1.drop([1872])\n",
    "DB1 = DB1[DB1['class_beat'].isin(ls)]\n",
    "#DB1 = DB1[DB1['patient'].isin(patient_l_1)]\n",
    "\n",
    "\n",
    "DB2 = pd.read_csv(\"DB2.csv\")\n",
    "#DB2 = DB2.drop([18692, 31865])\n",
    "DB2 = DB2[DB2['class_beat'].isin(ls)]\n",
    "#DB2 = DB2[DB2['patient'].isin(patient_l_2)]\n",
    "\n",
    "patients_ls_1 = DB1_patients.iloc[:,0]\n",
    "patients_ls_2 = DB2_patients.iloc[:,0]\n",
    "\n",
    "patients_ls_1_ID = DB1_patients.iloc[:,1]\n",
    "patients_ls_2_ID = DB2_patients.iloc[:,1]\n",
    "\n",
    "patients_ls_1_type = DB1_patients.iloc[:,2]\n",
    "patients_ls_2_type = DB2_patients.iloc[:,2]\n",
    "\n",
    "#patients_ls_1 = np.asarray(DB1_patients, dtype=int)\n",
    "patients_ls_1 = [int(i) for i in patients_ls_1]\n",
    "patients_ls_1 = np.asarray(patients_ls_1, dtype=int)\n",
    "patients_ls_1 = patients_ls_1.reshape(patients_ls_1.shape[0],1)\n",
    "#patients_ls_2 = np.asarray(DB2_patients, dtype=int)\n",
    "patients_ls_2 = [int(i) for i in patients_ls_2]\n",
    "patients_ls_2 = np.asarray(patients_ls_2, dtype=int)\n",
    "patients_ls_2 = patients_ls_2.reshape(patients_ls_2.shape[0],1)\n",
    "\n",
    "patients_ls_all = np.vstack((patients_ls_1,patients_ls_2))\n",
    "\n",
    "#patients_ls_all = patients_ls_all.reshape(7980,1)\n",
    "\n",
    "patients_ls_all = [int(i) for i in patients_ls_all]\n",
    "\n",
    "patients_ls_2 = [int(i) for i in patients_ls_2]\n",
    "patients_ls_1 = [int(i) for i in patients_ls_1]\n",
    "\n",
    "\n",
    "DB1_non_cli = pd.read_csv(\"DB1_non_clinic.csv\")\n",
    "#DB1_non_cli = DB1_non_cli.drop([1872])\n",
    "#DB1_non_cli = DB1_non_cli[DB1_non_cli['patient'].isin(patient_l_1)]\n",
    "\n",
    "\n",
    "DB1_non_cli = DB1_non_cli[DB1_non_cli['y0'].isin(ls)]\n",
    "\n",
    "\n",
    "DB2_non_cli = pd.read_csv(\"DB2_non_clinic.csv\")\n",
    "#DB2_non_cli= DB2_non_cli.drop([18692, 31865])\n",
    "DB2_non_cli = DB2_non_cli[DB2_non_cli['y0'].isin(ls)]\n",
    "#DB2_non_cli = DB2_non_cli[DB2_non_cli['patient'].isin(patient_l_2)]\n",
    "\n",
    "\n",
    "DB1_V1 = pd.read_csv(\"DB1_V1.csv\")\n",
    "#DB1_V1 = DB1_V1.drop([1872])\n",
    "DB1_V1 = DB1_V1[DB1_V1['class_beat'].isin(ls)]\n",
    "#DB1_V1 = DB1_V1[DB1_V1['patient'].isin(patient_l_1)]\n",
    "\n",
    "\n",
    "\n",
    "DB2_V1 = pd.read_csv(\"DB2_V1.csv\")\n",
    "#DB2_V1 = DB2_V1.drop([18692, 31865])\n",
    "DB2_V1 = DB2_V1[DB2_V1['class_beat'].isin(ls)]\n",
    "#DB2_V1 = DB2_V1[DB2_V1['patient'].isin(patient_l_2)]\n",
    "\n",
    "ls = []\n",
    "ls.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n",
    "DB1_dwt = pd.read_csv(\"DB1_DTW_MLII.csv\")\n",
    "#DB1_dwt = DB1_dwt.drop([1872])\n",
    "DB1_dwt = DB1_dwt[DB1_dwt['beat_type'].isin(ls)]\n",
    "#DB1_dwt = DB1_dwt[DB1_dwt['patient'].isin(patient_l_1)]\n",
    "\n",
    "\n",
    "DB2_dwt = pd.read_csv(\"DB2_DTW_MLII.csv\")\n",
    "#DB2_dwt = DB2_dwt.drop([18692, 31865])\n",
    "DB2_dwt = DB2_dwt[DB2_dwt['beat_type'].isin(ls)]\n",
    "#DB2_dwt = DB2_dwt[DB2_dwt['patient'].isin(patient_l_2)]\n",
    "\n",
    "\n",
    "DB1_dwt_V1 = pd.read_csv(\"DB1_DTW_V1.csv\")\n",
    "#DB1_dwt_V1 = DB1_dwt_V1.drop([1872])\n",
    "DB1_dwt_V1 = DB1_dwt_V1[DB1_dwt_V1['beat_type'].isin(ls)]\n",
    "#DB1_dwt_V1 = DB1_dwt_V1[DB1_dwt_V1['patient'].isin(patient_l_1)]\n",
    "\n",
    "\n",
    "\n",
    "DB2_dwt_V1 = pd.read_csv(\"DB2_DTW_V1.csv\")\n",
    "#DB2_dwt_V1 = DB2_dwt_V1.drop([18692, 31865])\n",
    "DB2_dwt_V1 = DB2_dwt_V1[DB2_dwt_V1['beat_type'].isin(ls)]\n",
    "#DB2_dwt_V1 = DB2_dwt_V1[DB2_dwt_V1['patient'].isin(patient_l_2)]\n",
    "\n",
    " #list(DB1.columns.values)\n",
    "variables_1 = DB1.iloc[:,0:130]\n",
    "class_beat_1 = DB1_patients.iloc[:,2]\n",
    "class_ID_1 = DB1_patients.iloc[:,1]\n",
    "\n",
    "np_variables_1 = np.asarray(variables_1)\n",
    "np_class_beat_1 = np.asarray(class_beat_1)\n",
    "np_class_ID_1 = np.asarray(class_ID_1)\n",
    "\n",
    "np_class_beat_1 = np_class_beat_1.reshape(np_class_beat_1.shape[0],1)\n",
    "np_class_ID_1 = np_class_ID_1.reshape(np_class_ID_1.shape[0],1)\n",
    "\n",
    "variables_2 = DB2.iloc[:,0:130]\n",
    "class_beat_2 = DB2_patients.iloc[:,2]\n",
    "class_ID_2 = DB2_patients.iloc[:,1]\n",
    "\n",
    "np_variables_2 = np.asarray(variables_2)\n",
    "np_class_beat_2 = np.asarray(class_beat_2)\n",
    "np_class_ID_2 = np.asarray(class_ID_2)\n",
    "\n",
    "np_class_beat_2 = np_class_beat_2.reshape(np_class_beat_2.shape[0],1)\n",
    "np_class_ID_2 = np_class_ID_2.reshape(np_class_ID_2.shape[0],1)\n",
    "\n",
    "DB_var_all = np.vstack((np_variables_1,np_variables_2))\n",
    "DB_class_all = np.vstack((np_class_ID_1,np_class_ID_2))\n",
    "DB_type_all = np.vstack((np_class_beat_1,np_class_beat_2))\n",
    "\n",
    "np_class_beat_1 = [str(i) for i in np_class_beat_1]\n",
    "np_class_ID_1 = [int(i) for i in np_class_ID_1]\n",
    "\n",
    "np_class_beat_1 = np.asarray(np_class_beat_1)\n",
    "np_class_ID_1 = np.asarray(np_class_ID_1)\n",
    "\n",
    "\n",
    "np_class_beat_2 = [str(i) for i in np_class_beat_2]\n",
    "np_class_ID_2 = [int(i) for i in np_class_ID_2]\n",
    "\n",
    "np_class_beat_2 = np.asarray(np_class_beat_2)\n",
    "np_class_ID_2 = np.asarray(np_class_ID_2)\n",
    "\n",
    "DB_type_all = [str(i) for i in DB_type_all]\n",
    "DB_class_all = [int(i) for i in DB_class_all]\n",
    "\n",
    "DB_type_all = np.asarray(DB_type_all)\n",
    "DB_class_all = np.asarray(DB_class_all)\n",
    "\n",
    "np_non_1 = np.asarray(DB1_non_cli)\n",
    "np_non_2 = np.asarray(DB2_non_cli)\n",
    "\n",
    "np_non_var_1 = np.asarray(DB1_non_cli.iloc[:,0:140])\n",
    "\n",
    "\n",
    "\n",
    "np_non_var_2 = np.asarray(DB2_non_cli.iloc[:,0:140])\n",
    "\n",
    "DB_var_non_all = np.vstack((np_non_var_1,np_non_var_2))\n",
    "\n",
    "dis_1 = DB1_dwt.iloc[:,0]\n",
    "\n",
    "\n",
    "dis_2 = DB2_dwt.iloc[:,0]\n",
    "\n",
    "dis_1_V1 = DB1_dwt_V1.iloc[:,0]\n",
    "\n",
    "\n",
    "dis_2_V1 = DB2_dwt_V1.iloc[:,0]\n",
    "\n",
    "dtw_1 = np.asarray(dis_1)\n",
    "dtw_1 = dtw_1.reshape(dtw_1.shape[0],1)\n",
    "\n",
    "dtw_2 = np.asarray(dis_2)\n",
    "dtw_2 = dtw_2.reshape(dtw_2.shape[0],1)\n",
    "\n",
    "dtw_1_V1 = np.asarray(dis_1_V1)\n",
    "dtw_1_V1 = dtw_1_V1.reshape(dtw_1_V1.shape[0],1)\n",
    "\n",
    "\n",
    "dtw_2_V1 = np.asarray(dis_2_V1)\n",
    "dtw_2_V1 = dtw_2_V1.reshape(dtw_2_V1.shape[0],1)\n",
    "\n",
    "v1_1 = DB1_V1.iloc[:,0:130]\n",
    "\n",
    "np_V1_1 = np.asarray(v1_1)\n",
    "\n",
    "\n",
    "v1_2 = DB2_V1.iloc[:,0:130]\n",
    "\n",
    "np_V1_2 = np.asarray(v1_2)\n",
    "\n",
    "DB_v_var_all = np.vstack((np_V1_1,np_V1_2))\n",
    "\n",
    "dtw_clinic_1 = np.hstack((dtw_1, dtw_1_V1))\n",
    "dtw_clinic_2 = np.hstack((dtw_2, dtw_2_V1))\n",
    "\n",
    "\n",
    "norm_dtw_clinic_1 = post_features.normalised_values_multiples(dtw_clinic_1)\n",
    "norm_dtw_clinic_2 = post_features.normalised_values_multiples(dtw_clinic_2)\n",
    "\n",
    "\n",
    "np_clinic_1 = np.hstack((np_variables_1, np_V1_1, dtw_clinic_1))\n",
    "np_clinic_2 = np.hstack((np_variables_2,np_V1_2, dtw_clinic_2))\n",
    "\n",
    "np_clinic_all = np.hstack((DB_var_all,DB_v_var_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "DB1_amp = pd.read_csv(\"DB1_amp_time.csv\")\n",
    "#DB1_amp = DB1_amp.drop([1872])\n",
    "#DB1_amp = DB1_amp[DB1_amp['patient'].isin(patient_l_1)]\n",
    "DB1_amp = DB1_amp[DB1_amp['col1080'].isin(ls)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DB2_amp = pd.read_csv(\"DB2_amp_time.csv\")\n",
    "#DB2_amp = DB2_amp.drop([18692, 31865])\n",
    "#DB2_amp = DB2_amp[DB2_amp['patient'].isin(patient_l_2)]\n",
    "DB2_amp = DB2_amp[DB2_amp['col1080'].isin(ls)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ampt_time_1 = DB1_amp.iloc[:,0:1082-2]\n",
    "\n",
    "\n",
    "ampt_time_2 = DB2_amp.iloc[:,0:1082-2]\n",
    "\n",
    "np_amp_1 = np.asarray(ampt_time_1)\n",
    "np_amp_2 = np.asarray(ampt_time_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
