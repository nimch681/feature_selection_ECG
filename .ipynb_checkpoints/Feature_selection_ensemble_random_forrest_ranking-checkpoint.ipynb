{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes.python import features_columns as col\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "from codes.python import TunedClassifier as classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n",
    "\n",
    "\n",
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])\n",
    "\n",
    "rank = pd.read_csv(\"database/features_ranking.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "\n",
    "rank_n_f = pd.read_csv(\"database/features_n_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_n_s = pd.read_csv(\"database/features_n_vs_s_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_n_v = pd.read_csv(\"database/features_n_vs_v_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_s_f = pd.read_csv(\"database/features_s_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_s_v = pd.read_csv(\"database/features_s_vs_v_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_v_f = pd.read_csv(\"database/features_v_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "\n",
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('database/nvs_lin.text', 'rb')\n",
    "n_s_ln = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/nvv_lin.text', 'rb')\n",
    "n_v_ln = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/nvf_lin.text', 'rb')\n",
    "n_f_ln = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/svv_lin.text', 'rb')\n",
    "s_v_ln = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/svf_lin.text', 'rb')\n",
    "s_f_ln = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/vvf_lin.text', 'rb')\n",
    "v_f_ln = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "import pickle\n",
    "f = open('database/nvs_log.text', 'rb')\n",
    "n_s_log = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/nvv_log.text', 'rb')\n",
    "n_v_log = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/nvf_log.text', 'rb')\n",
    "n_f_log = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/svv_log.text', 'rb')\n",
    "s_v_log = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/svf_log.text', 'rb')\n",
    "s_f_log = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/vvf_log.text', 'rb')\n",
    "v_f_log = pickle.load(f)[1]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_good' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-546e7c2d8a9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogisticRegress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_good\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_good\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"n_s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogisticRegress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"n_v\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogisticRegress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_v\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_v\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_good' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.01\n",
    "score_n_s = 0.01\n",
    "score_n_f = 0.015\n",
    "score_n_v = 0.01\n",
    "score_s_f = 0.01\n",
    "score_s_v = 0.015\n",
    "score_v_f = 0.01\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "n_s = rank_n_s[rank_n_s['rfscore'] >= score_n_s]['features'].values\n",
    "n_f = rank_n_f[rank_n_f['rfscore'] >= score_n_f]['features'].values\n",
    "n_v = rank_n_v[rank_n_v['rfscore'] >= score_n_v]['features'].values\n",
    "s_f = rank_s_f[rank_s_f['rfscore'] >= score_s_f]['features'].values\n",
    "s_v = rank_s_v[rank_s_v['rfscore'] >= score_s_v]['features'].values\n",
    "v_f = rank_v_f[rank_v_f['rfscore'] >= score_v_f]['features'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_linear\n",
      "all\n",
      "n_s\n",
      "[[45814   645     0     0]\n",
      " [ 1218  2002     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     46459\n",
      "           1       0.76      0.62      0.68      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     49679\n",
      "   macro avg       0.43      0.40      0.42     49679\n",
      "weighted avg       0.96      0.96      0.96     49679\n",
      "\n",
      "n_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46141   318     0     0]\n",
      " [ 1457  1763     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     46459\n",
      "           1       0.85      0.55      0.67      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     49679\n",
      "   macro avg       0.45      0.39      0.41     49679\n",
      "weighted avg       0.96      0.96      0.96     49679\n",
      "\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45905   554     0     0]\n",
      " [  838  2382     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     46459\n",
      "           1       0.81      0.74      0.77      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     49679\n",
      "   macro avg       0.45      0.43      0.44     49679\n",
      "weighted avg       0.97      0.97      0.97     49679\n",
      "\n",
      "s_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44355  2104     0     0]\n",
      " [ 2895   325     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95     46459\n",
      "           1       0.13      0.10      0.12      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.27      0.26      0.27     49679\n",
      "weighted avg       0.89      0.90      0.89     49679\n",
      "\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45883   576     0     0]\n",
      " [ 1852  1368     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     46459\n",
      "           1       0.70      0.42      0.53      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     49679\n",
      "   macro avg       0.42      0.35      0.38     49679\n",
      "weighted avg       0.94      0.95      0.95     49679\n",
      "\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46039   420     0     0]\n",
      " [  848  2372     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     46459\n",
      "           1       0.85      0.74      0.79      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     49679\n",
      "   macro avg       0.46      0.43      0.44     49679\n",
      "weighted avg       0.97      0.97      0.97     49679\n",
      "\n",
      "svm_poly\n",
      "all\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "y_train[y_train != 2] = 0\n",
    "y_train[y_train == 2] = 1\n",
    "\n",
    "y_test[y_test != 2] = 0\n",
    "y_test[y_test == 2] = 1\n",
    "\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "#classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "#classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random\")\n",
    "print(\"all\")\n",
    "#classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rr_int_pre', 'rr_int_pre_V', 'PR_int_V', 'RP_V', 'PQ_int_V',\n",
       "       'rr_int_all_V', 'rr_int_all', 'P_prominence', 'P_duration_V',\n",
       "       'P_height', 'dtw1', 'P_areas', 'R_amp9', 'S_amp9_V', 'P_duration',\n",
       "       'S_amp1_V'], dtype=object)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train = X_train_balanced\n",
    "\n",
    "y_train = good_features_y\n",
    "\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.logisticRegress(X_train[n_s_ln[0:16]], y_train, X_test[n_s_ln[0:16]], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "classifier.logisticRegress(np_non_var_1_old, y_train, np_non_var_2_old, y_test, jk=True,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = np.hstack((n_s,n_s_ln[0:16]))\n",
    "\n",
    "#new_row = np.hstack((n_s,s_v,s_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = np.unique(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PQ_int_V', 'PR_int', 'PR_int_V', 'P_areas', 'P_duration',\n",
       "       'P_duration_V', 'P_height', 'P_height_V', 'P_neg_duration_V',\n",
       "       'P_prominence', 'RP', 'RP_V', 'RT_int_50', 'R_amp1', 'R_amp9',\n",
       "       'R_areas', 'S_amp0_V', 'S_amp1_V', 'S_amp3', 'S_amp9_V',\n",
       "       'S_height', 'T_duration', 'T_neg_amp9', 'dtw1', 'rr_int_all',\n",
       "       'rr_int_all_V', 'rr_int_pre', 'rr_int_pre_V'], dtype=object)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs all results for various classifier\n",
      "ensamble ln\n",
      "all\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_good' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-162f196ab625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensamble ln\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoting_ensemble_lin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_good\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_good\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"n_s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoting_ensemble_lin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_good' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"n vs all results for various classifier\")\n",
    "\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "print(\"ensamble ln\")\n",
    "print(\"all\")\n",
    "classifier.voting_ensemble_lin(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble_lin(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble_lin(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble_lin(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble_lin(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble_lin(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble_lin(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "\n",
    "\n",
    "print(\"ensamble ln\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs all results for various classifier\n",
      "Log\n",
      "all\n",
      "[[38399   783   351  4488]\n",
      " [ 1493   183   329    45]\n",
      " [   91    77  2839   213]\n",
      " [   20     0    12   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91     44021\n",
      "           1       0.18      0.09      0.12      2050\n",
      "           2       0.80      0.88      0.84      3220\n",
      "           3       0.07      0.92      0.13       388\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     49679\n",
      "   macro avg       0.50      0.69      0.50     49679\n",
      "weighted avg       0.91      0.84      0.87     49679\n",
      "\n",
      "[0.4323372331587701, 1.9504222520303145, 0.45997139808317433]\n",
      "n_s\n",
      "[[41468  1081  1090   382]\n",
      " [  457  1402   186     5]\n",
      " [  411   377  2334    98]\n",
      " [  130     1    22   235]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     44021\n",
      "           1       0.49      0.68      0.57      2050\n",
      "           2       0.64      0.72      0.68      3220\n",
      "           3       0.33      0.61      0.42       388\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49679\n",
      "   macro avg       0.61      0.74      0.66     49679\n",
      "weighted avg       0.93      0.91      0.92     49679\n",
      "\n",
      "[0.6373095096068558, 2.541406752990806, 0.6363305989272787]\n",
      "n_v\n",
      "[[38495   663   571  4292]\n",
      " [ 1794   161    47    48]\n",
      " [  309   225  2254   432]\n",
      " [   52     0    14   322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91     44021\n",
      "           1       0.15      0.08      0.10      2050\n",
      "           2       0.78      0.70      0.74      3220\n",
      "           3       0.06      0.83      0.12       388\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     49679\n",
      "   macro avg       0.49      0.62      0.47     49679\n",
      "weighted avg       0.90      0.83      0.86     49679\n",
      "\n",
      "[0.369087550491611, 1.7130278706674344, 0.3986722590792348]\n",
      "n_f\n",
      "[[40132   866   760  2263]\n",
      " [ 1493   276   244    37]\n",
      " [  177   171  2846    26]\n",
      " [   38     0   128   222]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93     44021\n",
      "           1       0.21      0.13      0.16      2050\n",
      "           2       0.72      0.88      0.79      3220\n",
      "           3       0.09      0.57      0.15       388\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     49679\n",
      "   macro avg       0.49      0.63      0.51     49679\n",
      "weighted avg       0.91      0.88      0.89     49679\n",
      "\n",
      "[0.49455225541513814, 1.9441256058721723, 0.4902918284415906]\n",
      "s_v\n",
      "[[37197    25  5245  1554]\n",
      " [ 2015     0    30     5]\n",
      " [ 1881     8  1145   186]\n",
      " [   23     0   350    15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.17      0.36      0.23      3220\n",
      "           3       0.01      0.04      0.01       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.27      0.31      0.28     49679\n",
      "weighted avg       0.81      0.77      0.79     49679\n",
      "\n",
      "[0.1149043005572076, 0.5247185702358781, 0.12304197155808856]\n",
      "s_f\n",
      "[[39666   735   904  2716]\n",
      " [ 1607   327    87    29]\n",
      " [  587    46  2366   221]\n",
      " [   40     0    16   332]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     44021\n",
      "           1       0.30      0.16      0.21      2050\n",
      "           2       0.70      0.73      0.72      3220\n",
      "           3       0.10      0.86      0.18       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.51      0.66      0.51     49679\n",
      "weighted avg       0.90      0.86      0.87     49679\n",
      "\n",
      "[0.4300598421085633, 1.890873870326865, 0.45138915484513975]\n",
      "v_f\n",
      "[[42752   373   525   371]\n",
      " [ 1508   111   402    29]\n",
      " [  178   165  2620   257]\n",
      " [  232     1     3   152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96     44021\n",
      "           1       0.17      0.05      0.08      2050\n",
      "           2       0.74      0.81      0.77      3220\n",
      "           3       0.19      0.39      0.25       388\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     49679\n",
      "   macro avg       0.51      0.56      0.52     49679\n",
      "weighted avg       0.90      0.92      0.91     49679\n",
      "\n",
      "[0.5887435842627939, 1.776608337520022, 0.5164478343213997]\n",
      "Lin\n",
      "all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39772   356   118  3775]\n",
      " [ 1708    49   254    39]\n",
      " [  379    63  2487   291]\n",
      " [   23     0     9   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     44021\n",
      "           1       0.10      0.02      0.04      2050\n",
      "           2       0.87      0.77      0.82      3220\n",
      "           3       0.08      0.92      0.15       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.50      0.65      0.48     49679\n",
      "weighted avg       0.90      0.86      0.88     49679\n",
      "\n",
      "[0.4309214399351081, 1.768118353887931, 0.43647551420354547]\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41176   386  1658   801]\n",
      " [ 1737   262    51     0]\n",
      " [  838   318  1874   190]\n",
      " [   42     0    40   306]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     44021\n",
      "           1       0.27      0.13      0.17      2050\n",
      "           2       0.52      0.58      0.55      3220\n",
      "           3       0.24      0.79      0.36       388\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     49679\n",
      "   macro avg       0.49      0.61      0.51     49679\n",
      "weighted avg       0.88      0.88      0.88     49679\n",
      "\n",
      "[0.4276036170905601, 1.4982648848262756, 0.4010849191485645]\n",
      "n_v\n",
      "[[38080    18    70  5853]\n",
      " [ 1992     0     6    52]\n",
      " [  625     3  1883   709]\n",
      " [   28     0     3   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.96      0.58      0.73      3220\n",
      "           3       0.05      0.92      0.10       388\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     49679\n",
      "   macro avg       0.49      0.59      0.43     49679\n",
      "weighted avg       0.89      0.81      0.84     49679\n",
      "\n",
      "[0.30207436228521783, 1.5445175730177723, 0.3441018777698305]\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40933   436  1728   924]\n",
      " [ 1667    36   347     0]\n",
      " [  562    61  2558    39]\n",
      " [   62     0   255    71]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     44021\n",
      "           1       0.07      0.02      0.03      2050\n",
      "           2       0.52      0.79      0.63      3220\n",
      "           3       0.07      0.18      0.10       388\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     49679\n",
      "   macro avg       0.40      0.48      0.42     49679\n",
      "weighted avg       0.88      0.88      0.87     49679\n",
      "\n",
      "[0.4487324507781289, 1.4028355496402247, 0.39972066909409254]\n",
      "s_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38412     9  5416   184]\n",
      " [ 2024     0    25     1]\n",
      " [ 2289     1   794   136]\n",
      " [   29     0   327    32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.89     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.12      0.25      0.16      3220\n",
      "           3       0.09      0.08      0.09       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.28      0.30      0.28     49679\n",
      "weighted avg       0.80      0.79      0.80     49679\n",
      "\n",
      "[0.08136776155355151, 0.36758354614655053, 0.08663182404509456]\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39916   173   647  3285]\n",
      " [ 1928    16    82    24]\n",
      " [ 1256    10  1874    80]\n",
      " [   29     0    10   349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92     44021\n",
      "           1       0.08      0.01      0.01      2050\n",
      "           2       0.72      0.58      0.64      3220\n",
      "           3       0.09      0.90      0.17       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.45      0.60      0.44     49679\n",
      "weighted avg       0.87      0.85      0.86     49679\n",
      "\n",
      "[0.33151149870777397, 1.3873777799370228, 0.33917797184601484]\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43556   152   286    27]\n",
      " [ 1633   263   154     0]\n",
      " [  588    82  2341   209]\n",
      " [  287     0     2    99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     44021\n",
      "           1       0.53      0.13      0.21      2050\n",
      "           2       0.84      0.73      0.78      3220\n",
      "           3       0.30      0.26      0.27       388\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     49679\n",
      "   macro avg       0.65      0.52      0.56     49679\n",
      "weighted avg       0.92      0.93      0.92     49679\n",
      "\n",
      "[0.6049793419285331, 2.225664951030601, 0.5806977898430916]\n",
      "Random\n",
      "all\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-b7da24ea5f4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Random\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_good\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_good\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"n_s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;31m## Use this first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\TunedClassifier.py\u001b[0m in \u001b[0;36mrandomForest\u001b[1;34m(X_train, y_train, X_test, y_test, jk, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, min_impurity_decrease, max_features, max_leaf_nodes, class_weight, labels)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 333\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    802\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"n vs all results for various classifier\")\n",
    "\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "#test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "under_X_train = train\n",
    "#X_test = test\n",
    "under_y_train = train_labels\n",
    "#y_test = test_labels\n",
    "#input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "#X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "\n",
    "score = 0.01\n",
    "score_n_s = 0.01\n",
    "score_n_f = 0.015\n",
    "score_n_v = 0.01\n",
    "score_s_f = 0.01\n",
    "score_s_v = 0.015\n",
    "score_v_f = 0.01\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "n_s = rank_n_s[rank_n_s['rfscore'] >= score_n_s]['features'].values\n",
    "n_f = rank_n_f[rank_n_f['rfscore'] >= score_n_f]['features'].values\n",
    "n_v = rank_n_v[rank_n_v['rfscore'] >= score_n_v]['features'].values\n",
    "s_f = rank_s_f[rank_s_f['rfscore'] >= score_s_f]['features'].values\n",
    "s_v = rank_s_v[rank_s_v['rfscore'] >= score_s_v]['features'].values\n",
    "v_f = rank_v_f[rank_v_f['rfscore'] >= score_v_f]['features'].values\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "print(\"ensamble rf ln\")\n",
    "print(\"all\")\n",
    "classifier.voting_ensemble_lin_rf(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble_lin_rf(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble_lin_rf(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble_lin_rf(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble_lin_rf(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble_lin_rf(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble_lin_rf(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ensamble ln\")\n",
    "print(\"all\")\n",
    "classifier.voting_ensemble_lin(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble_lin(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble_lin(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble_lin(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble_lin(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble_lin(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble_lin(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "\n",
    "\n",
    "print(\"ensamble ln\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs all results for various classifier\n",
      "n_s\n",
      "[[40174   758  1971  1118]\n",
      " [  458   263  1317    12]\n",
      " [  360    35  2812    13]\n",
      " [  221     1   166     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94     44021\n",
      "           1       0.25      0.13      0.17      2050\n",
      "           2       0.45      0.87      0.59      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     49679\n",
      "   macro avg       0.42      0.48      0.43     49679\n",
      "weighted avg       0.90      0.87      0.88     49679\n",
      "\n",
      "[0.49374525176137724, 1.6991731620170545, 0.45926927113282046]\n"
     ]
    }
   ],
   "source": [
    "print(\"n vs all results for various classifier\")\n",
    "\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "\n",
    "score = 0.01\n",
    "score_n_s = 0.01\n",
    "score_n_f = 0.015\n",
    "score_n_v = 0.01\n",
    "score_s_f = 0.01\n",
    "score_s_v = 0.015\n",
    "score_v_f = 0.01\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "n_s = rank_n_s[rank_n_s['rfscore'] >= score_n_s]['features'].values\n",
    "n_f = rank_n_f[rank_n_f['rfscore'] >= score_n_f]['features'].values\n",
    "n_v = rank_n_v[rank_n_v['rfscore'] >= score_n_v]['features'].values\n",
    "s_f = rank_s_f[rank_s_f['rfscore'] >= score_s_f]['features'].values\n",
    "s_v = rank_s_v[rank_s_v['rfscore'] >= score_s_v]['features'].values\n",
    "v_f = rank_v_f[rank_v_f['rfscore'] >= score_v_f]['features'].values\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "y_pred_rand_n_s = classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])[2]\n",
    "X_test_n_1 =X_test[y_pred_rand_n_s != 0]\n",
    "y_test_n_1 =y_test[y_pred_rand_n_s != 0]\n",
    "\n",
    "\n",
    "little_x_test = X_test[y_pred_rand_n_s == 0]\n",
    "little_y_pred = y_pred_rand_n_s[y_pred_rand_n_s == 0]\n",
    "little_y_test = y_test[y_pred_rand_n_s == 0]                                                                                                                                                                                                                                                                                                                                              \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39303     0    46   692]\n",
      " [  373     0    18     0]\n",
      " [  116     0   248     1]\n",
      " [   60     0    19   131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98     40041\n",
      "           1       0.00      0.00      0.00       391\n",
      "           2       0.75      0.68      0.71       365\n",
      "           3       0.16      0.62      0.25       210\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     41007\n",
      "   macro avg       0.47      0.57      0.49     41007\n",
      "weighted avg       0.97      0.97      0.97     41007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-56b8fbaf1430>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mlittle_x_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlittle_x_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures_clinic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0my_pred_svm_n_f\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm_model_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_f\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlittle_x_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_f\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlittle_y_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\TunedClassifier.py\u001b[0m in \u001b[0;36msvm_model_linear\u001b[1;34m(X_train, y_train, X_test, y_test, jk, C, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, verbose, max_iter, decision_function_shape, random_state, labels)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mjk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_metrics\u001b[1;34m(yhat, labels, lb)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_kappa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mjkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkappa\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.125\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjkappa\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_j_index\u001b[1;34m(confusion_matrix)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mSes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mSev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mPs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "#test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "under_X_train = train\n",
    "#X_test = test\n",
    "under_y_train = train_labels\n",
    "#y_test = test_labels\n",
    "#input_size=X_train.shape[1]\n",
    "\n",
    "under_X_train = pd.DataFrame(under_X_train,columns=features_clinic)\n",
    "\n",
    "little_x_test = pd.DataFrame(little_x_test,columns=features_clinic)\n",
    "\n",
    "y_pred_svm_n_f =classifier.svm_model_linear(X_train[n_f], y_train, little_x_test[n_f], little_y_test,decision_function_shape='ovr', jk=True,labels=[0,1,2,3])[2]\n",
    "\n",
    "\n",
    "\n",
    "#X_test_n_2 =little_x_test[y_pred_svm_n_f != 0]\n",
    "#y_test_n_2 =little_y_test[y_pred_svm_n_f != 0]\n",
    "\n",
    "\n",
    "\n",
    "#y_pred_svm_n_f_final = y_pred_svm_n_f[y_pred_svm_n_f == 0]\n",
    "#y_test_svm_n_f_final = little_y_test[y_pred_svm_n_f == 0]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "#test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "under_X_train = train\n",
    "#X_test = test\n",
    "under_y_train = train_labels\n",
    "#y_test = test_labels\n",
    "#input_size=X_train.shape[1]\n",
    "\n",
    "under_X_train = pd.DataFrame(under_X_train,columns=features_clinic)\n",
    "\n",
    "little_x_test = pd.DataFrame(little_x_test,columns=features_clinic)\n",
    "\n",
    "y_pred_svm_n_f =classifier.svm_model_linear(X_train[n_s], y_train, little_x_test[n_s], little_y_test,decision_function_shape='ovr', jk=True,labels=[0,1,2,3])[2]\n",
    "\n",
    "\n",
    "\n",
    "#X_test_n_2 =little_x_test[y_pred_svm_n_f != 0]\n",
    "#y_test_n_2 =little_y_test[y_pred_svm_n_f != 0]\n",
    "\n",
    "\n",
    "\n",
    "#y_pred_svm_n_f_final = y_pred_svm_n_f[y_pred_svm_n_f == 0]\n",
    "#y_test_svm_n_f_final = little_y_test[y_pred_svm_n_f == 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "#test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "under_X_train = train\n",
    "#X_test = test\n",
    "under_y_train = train_labels\n",
    "#y_test = test_labels\n",
    "#input_size=X_train.shape[1]\n",
    "\n",
    "under_X_train = pd.DataFrame(under_X_train,columns=features_clinic)\n",
    "\n",
    "little_x_test = pd.DataFrame(little_x_test,columns=features_clinic)\n",
    "\n",
    "y_pred_svm_n_f =classifier.svm_model_linear(X_train[n_v], y_train, little_x_test[n_v], little_y_test,decision_function_shape='ovr', jk=True,labels=[0,1,2,3])[2]\n",
    "\n",
    "\n",
    "\n",
    "#X_test_n_2 =little_x_test[y_pred_svm_n_f != 0]\n",
    "#y_test_n_2 =little_y_test[y_pred_svm_n_f != 0]\n",
    "\n",
    "\n",
    "\n",
    "#y_pred_svm_n_f_final = y_pred_svm_n_f[y_pred_svm_n_f == 0]\n",
    "#y_test_svm_n_f_final = little_y_test[y_pred_svm_n_f == 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28750  2667   999  7625]\n",
      " [  228    71     3    89]\n",
      " [   30    34    75   226]\n",
      " [   11     5    14   180]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.72      0.83     40041\n",
      "           1       0.03      0.18      0.04       391\n",
      "           2       0.07      0.21      0.10       365\n",
      "           3       0.02      0.86      0.04       210\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     41007\n",
      "   macro avg       0.28      0.49      0.26     41007\n",
      "weighted avg       0.97      0.71      0.81     41007\n",
      "\n",
      "[0.052631073559515726, 0.48137655991934447, 0.08648760676967593]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "#test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "under_X_train = train\n",
    "#X_test = test\n",
    "under_y_train = train_labels\n",
    "#y_test = test_labels\n",
    "#input_size=X_train.shape[1]\n",
    "\n",
    "under_X_train = pd.DataFrame(under_X_train,columns=features_clinic)\n",
    "\n",
    "little_x_test = pd.DataFrame(little_x_test,columns=features_clinic)\n",
    "\n",
    "classifier.svm_model_linear(under_X_train[n_v], under_y_train, little_x_test[n_v], little_y_test, jk=True,labels=[0,1,2,3])[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39638   137   161   105]\n",
      " [  378    10     3     0]\n",
      " [  301    15    41     8]\n",
      " [  129     0     2    79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     40041\n",
      "           1       0.06      0.03      0.04       391\n",
      "           2       0.20      0.11      0.14       365\n",
      "           3       0.41      0.38      0.39       210\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     41007\n",
      "   macro avg       0.41      0.38      0.39     41007\n",
      "weighted avg       0.96      0.97      0.97     41007\n",
      "\n",
      "[0.17915172520288922, 0.39770024260559006, 0.13928839292714337]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17915172520288922, 0.39770024260559006, 0.13928839292714337]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "little_x_test = pd.DataFrame(little_x_test,columns=features_clinic)\n",
    "\n",
    "#y_pred_svm_n_f =\n",
    "classifier.voting_ensemble_lin_rf(X_train[n_s], y_train, little_x_test[n_s],little_y_test, jk=True,labels=[0,1,2,3])[2]\n",
    "\n",
    "\n",
    "\n",
    "#X_test_n_2 =little_x_test[y_pred_svm_n_f != 0]\n",
    "#y_test_n_2 =little_y_test[y_pred_svm_n_f != 0]\n",
    "\n",
    "\n",
    "\n",
    "#y_pred_svm_n_f_final = y_pred_svm_n_f[y_pred_svm_n_f == 0]\n",
    "#y_test_svm_n_f_final = little_y_test[y_pred_svm_n_f == 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2851 2592 1275 4760]\n",
      " [  80 1727   30   25]\n",
      " [  59  814 2062  255]\n",
      " [   6    4    5  362]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.25      0.39     11478\n",
      "           1       0.34      0.93      0.49      1862\n",
      "           2       0.61      0.65      0.63      3190\n",
      "           3       0.07      0.96      0.13       377\n",
      "\n",
      "   micro avg       0.41      0.41      0.41     16907\n",
      "   macro avg       0.49      0.70      0.41     16907\n",
      "weighted avg       0.80      0.41      0.44     16907\n",
      "\n",
      "[0.2690368193558319, 2.5215872601901257, 0.44971681720168166]\n"
     ]
    }
   ],
   "source": [
    "X_test_n_3 = np.vstack((X_test_n_1,X_test_n_2))\n",
    "\n",
    "y_test_n_1 = y_test_n_1.reshape(y_test_n_1.shape[0],1)\n",
    "y_test_n_2 = y_test_n_2.reshape(y_test_n_2.shape[0],1)\n",
    "\n",
    "X_test_n_3 = pd.DataFrame(X_test_n_3,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_test_n_3 = np.vstack((y_test_n_1,y_test_n_2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred_svm_n_s = classifier.svm_model_linear(under_X_train[n_s], under_y_train, X_test_n_3[n_s], y_test_n_3, jk=True,labels=[0,1,2,3])[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_s\")\n",
    "## Use this first\n",
    "y_pred = classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])[1]\n",
    "X_test_n_1 =X_test[y_pred != 1]\n",
    "y_test_n_1=y_test[y_pred != 1]\n",
    "\n",
    "y_pred_n = y_pred[y_pred == 1]\n",
    "y_true_1 = y_test[y_pred == 1]\n",
    "\n",
    "\n",
    "y_pred_voting_ensemble =voting_ensemble(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test_n_1[rank[rank['rfscore'] >= score]['features'].values]), np.asarray(y_test_n_1),  labels=[0,1,2,3])[1]\n",
    "\n",
    "X_test_n_2 =X_test_n_1[y_pred_voting_ensemble != 2]\n",
    "y_test_n_2=y_test_n_1[y_pred_voting_ensemble != 2]\n",
    "\n",
    "y_pred_voting_ensemble_n = y_pred_voting_ensemble[y_pred_voting_ensemble == 2]\n",
    "y_true_2 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "y_pred_linear_D =Linear_D(np.asarray(X_train[feature_good]), y_train,np.asarray( X_test_n_2[feature_good]), y_test_n_2,  labels=[0,1,2,3])[1]\n",
    "\n",
    "X_test_n_3 =X_test_n_2[y_pred_linear_D != 2]\n",
    "y_test_n_3=y_test_n_2[y_pred_linear_D != 2]\n",
    "\n",
    "y_pred_linear_D_2 = y_pred_linear_D[y_pred_linear_D == 2]\n",
    "y_true_3 = y_test_n_2[y_pred_linear_D == 2]\n",
    "\n",
    "#y_true_1 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "X_test_n_3 =X_test_n_2[y_pred_linear_D != 0]\n",
    "y_test_n_3=y_test_n_2[y_pred_linear_D != 0]\n",
    "\n",
    "y_pred_linear_D_0 = y_pred_linear_D[y_pred_linear_D == 0]\n",
    "y_true_4 = y_test_n_2[y_pred_linear_D == 0]\n",
    "\n",
    "y_svm = svm_model_poly(X_train_balanced, good_features_y, X_test_n_3, y_test_n_3, labels=[0,1,2,3])[1]\n",
    "\n",
    "\n",
    "X_test_n_4 =X_test_n_3[y_svm != 0]\n",
    "y_test_n_4=y_test_n_3[y_svm != 0]\n",
    "\n",
    "y_svm_n = y_svm[y_svm == 0]\n",
    "y_true_5 = y_test_n_3[y_svm == 0]\n",
    "\n",
    "y_v_e = Linear_D(X_train, y_train, X_test_n_4, y_test_n_4, labels=[0,1,2,3])[1]\n",
    "\n",
    "y_final_pred = np.hstack((y_pred_n,y_pred_voting_ensemble_n, y_pred_linear_D_2,y_pred_linear_D_0,y_svm_n,y_v_e))\n",
    "y_final_true = np.hstack((y_true_1,y_true_2,y_true_3,y_true_4,y_true_5,y_test_n_4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29945  3969  1327  8780]\n",
      " [  477   416    43  1114]\n",
      " [   74   504  1827   815]\n",
      " [   12     6    17   353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.68      0.80     44021\n",
      "           1       0.08      0.20      0.12      2050\n",
      "           2       0.57      0.57      0.57      3220\n",
      "           3       0.03      0.91      0.06       388\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     49679\n",
      "   macro avg       0.42      0.59      0.39     49679\n",
      "weighted avg       0.91      0.66      0.75     49679\n",
      "\n",
      "[0.22623646531220643, 1.4237533407951295, 0.2910874002554944]\n",
      "[[38495   663   571  4292]\n",
      " [ 1794   161    47    48]\n",
      " [  309   225  2254   432]\n",
      " [   52     0    14   322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91     44021\n",
      "           1       0.15      0.08      0.10      2050\n",
      "           2       0.78      0.70      0.74      3220\n",
      "           3       0.06      0.83      0.12       388\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     49679\n",
      "   macro avg       0.49      0.62      0.47     49679\n",
      "weighted avg       0.90      0.83      0.86     49679\n",
      "\n",
      "[0.369087550491611, 1.7130278706674344, 0.3986722590792348]\n",
      "[[38080    18    70  5853]\n",
      " [ 1992     0     6    52]\n",
      " [  625     3  1883   709]\n",
      " [   28     0     3   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.96      0.58      0.73      3220\n",
      "           3       0.05      0.92      0.10       388\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     49679\n",
      "   macro avg       0.49      0.59      0.43     49679\n",
      "weighted avg       0.89      0.81      0.84     49679\n",
      "\n",
      "[0.30207436228521783, 1.5445175730177723, 0.3441018777698305]\n"
     ]
    }
   ],
   "source": [
    "svm_pred=classifier.svm_model_linear(under_X_train[n_v], under_y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])[2]\n",
    "log_pred=classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])[2]\n",
    "lin_pred=classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr, HOS, myMorph training features\n",
    "train_rr=np.loadtxt('database/rr-train.txt', dtype=float)[:,1:] # extract class labels\n",
    "train_hos=np.loadtxt('database/hos_train.txt', dtype=float)\n",
    "train_mmorph=np.loadtxt('database/morph_train.txt', dtype=float)\n",
    "\n",
    "# rr, HOS, myMorph testing features\n",
    "test_rr=np.loadtxt('database/rr-test.txt', dtype=float)[:,1:] # extract class labels\n",
    "test_rr=np.loadtxt('database/rr-test.txt', dtype=float)[:,1:] # extract class labels\n",
    "test_hos=np.loadtxt('database/hos_test.txt', dtype=float)\n",
    "test_mmorph=np.loadtxt('database/morph_test.txt', dtype=float)\n",
    "\n",
    "# training and testing class labels.\n",
    "train_labels=np.loadtxt('database/labels_train.txt', dtype=str)\n",
    "test_labels=np.loadtxt('database/labels_test.txt', dtype=str)\n",
    "\n",
    "# aggregate all features.\n",
    "train=np.hstack((train_rr, train_hos))#, train_mmorph))\n",
    "test=np.hstack((test_rr, test_hos))#, test_mmorph))\n",
    "\n",
    "#train=np.hstack((train_hos, train_rr))\n",
    "#test=np.hstack((test_hos,test_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "\n",
    "X0 = train[train_labels == 'N']\n",
    "X1 = train[train_labels == 'S']\n",
    "X2 = train[train_labels == 'V']\n",
    "X3 = train[train_labels== 'F']\n",
    "\n",
    "col = 1\n",
    "#ls = [X0.iloc[:,col].values, X1.iloc[:,col].values, X2.iloc[:,col].values, X3.iloc[:,col].values]\n",
    "ls = [X0[:,col], X1[:,col], X2[:,col], X3[:,col]]\n",
    "\n",
    "labels = [\"N\", \"S\", \"V\", \"F\"]\n",
    "\n",
    "#plt.hist(ls, bins = 10, label = labels, color = ['green', 'blue', 'orange', 'Red'])\n",
    "\n",
    "\n",
    "plt.boxplot(ls, labels = labels)#, color = ['green', 'blue', 'orange', 'Red'])\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#ls = [X0.iloc[:,col].values, X1.iloc[:,col].values, X2.iloc[:,col].values, X3.iloc[:,col].values]\n",
    "ls = [X0[:,col], X1[:,col], X2[:,col], X3[:,col]]\n",
    "\n",
    "labels = labels = [\"N\", \"S\", \"V\", \"F\"]\n",
    "\n",
    "plt.hist(ls, bins = 10, label = labels, color = ['green', 'blue', 'orange', 'Red'])\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "feat = n_s\n",
    "X0 = X_train[y_train == 0][feat]\n",
    "X1 = X_train[y_train == 1][feat]\n",
    "X2 = X_train[y_train == 2][feat]\n",
    "X3 = X_train[y_train== 3][feat]\n",
    "\n",
    "col = 14\n",
    "ls = [X0.iloc[:,col].values, X1.iloc[:,col].values, X2.iloc[:,col].values, X3.iloc[:,col].values]\n",
    "#ls = [X0[:,0], X1[:,0], X2[:,0], X3[:,0]]\n",
    "#ls = [X0, X1, X2, X3]\n",
    "\n",
    "\n",
    "labels = [\"N\", \"S\", \"V\", \"F\"]\n",
    "\n",
    "#plt.hist(ls, bins = 10, label = labels, color = ['green', 'blue', 'orange', 'Red'])\n",
    "\n",
    "\n",
    "plt.boxplot(ls, labels = labels)#, color = ['green', 'blue', 'orange', 'Red'])\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#ls = [X0.iloc[:,col].values, X1.iloc[:,col].values, X2.iloc[:,col].values, X3.iloc[:,col].values]\n",
    "#ls = [X0[:,0], X1[:,0], X2[:,0], X3[:,0]]\n",
    "\n",
    "labels = labels = [\"N\", \"S\", \"V\", \"F\"]\n",
    "\n",
    "plt.hist(ls, bins = 10, label = labels, color = ['green', 'blue', 'orange', 'Red'])\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
