{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])\n",
    "\n",
    "rank = pd.read_csv(\"database/features_ranking.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "\n",
    "rank_n_f = pd.read_csv(\"database/features_n_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_n_s = pd.read_csv(\"database/features_n_vs_s_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_n_v = pd.read_csv(\"database/features_n_vs_v_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_s_f = pd.read_csv(\"database/features_s_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_s_v = pd.read_csv(\"database/features_s_vs_v_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_v_f = pd.read_csv(\"database/features_v_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>features</th>\n",
       "      <th>rfscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>neg_RQ</td>\n",
       "      <td>0.145052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>rr_int_post</td>\n",
       "      <td>0.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>229</td>\n",
       "      <td>rr_int_post_V</td>\n",
       "      <td>0.074422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>R_amp5</td>\n",
       "      <td>0.052419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>R_amp6</td>\n",
       "      <td>0.047788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>R_amp4</td>\n",
       "      <td>0.039433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Q_height</td>\n",
       "      <td>0.029093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Q_prominence</td>\n",
       "      <td>0.028858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>P_neg_prominence_V</td>\n",
       "      <td>0.026919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>P_neg_height_V</td>\n",
       "      <td>0.024287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>rr_int_pre</td>\n",
       "      <td>0.023055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>R_amp3</td>\n",
       "      <td>0.014145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>228</td>\n",
       "      <td>rr_int_pre_V</td>\n",
       "      <td>0.012811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>R_height</td>\n",
       "      <td>0.012208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>R_amp7</td>\n",
       "      <td>0.012033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>TR</td>\n",
       "      <td>0.011894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>R_duration</td>\n",
       "      <td>0.011826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>RT_int</td>\n",
       "      <td>0.011052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>R_prominence</td>\n",
       "      <td>0.010769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>101</td>\n",
       "      <td>rr_int_50</td>\n",
       "      <td>0.009664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>S_amp0</td>\n",
       "      <td>0.009351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>231</td>\n",
       "      <td>rr_int_50_V</td>\n",
       "      <td>0.008671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>neg_PR</td>\n",
       "      <td>0.007391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>261</td>\n",
       "      <td>dtw2</td>\n",
       "      <td>0.007321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>260</td>\n",
       "      <td>dtw1</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>232</td>\n",
       "      <td>rr_int_all_V</td>\n",
       "      <td>0.006682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>S_amp1</td>\n",
       "      <td>0.006123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>102</td>\n",
       "      <td>rr_int_all</td>\n",
       "      <td>0.005915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>T_neg_amp2</td>\n",
       "      <td>0.005618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>T_neg_amp9</td>\n",
       "      <td>0.005330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>218</td>\n",
       "      <td>T_neg_amp2_V</td>\n",
       "      <td>0.000283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>213</td>\n",
       "      <td>T_areas_V</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>Q_amp0_V</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Q_amp7</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>257</td>\n",
       "      <td>neg_PT_V</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>P_amp7</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>211</td>\n",
       "      <td>T_amp9_V</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>P_amp0_V</td>\n",
       "      <td>0.000257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>200</td>\n",
       "      <td>T_duration_V</td>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>239</td>\n",
       "      <td>PR_int_V</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>143</td>\n",
       "      <td>R_areas_V</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Q_amp6</td>\n",
       "      <td>0.000232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Q_amp8</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>223</td>\n",
       "      <td>T_neg_amp7_V</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>255</td>\n",
       "      <td>neg_ST_V</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>216</td>\n",
       "      <td>T_neg_amp0_V</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>T_duration</td>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>P_neg_amp7_V</td>\n",
       "      <td>0.000208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>227</td>\n",
       "      <td>T_neg_areas_V</td>\n",
       "      <td>0.000205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>183</td>\n",
       "      <td>P_amp9_V</td>\n",
       "      <td>0.000203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>R_height_V</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>191</td>\n",
       "      <td>P_neg_amp3_V</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>251</td>\n",
       "      <td>RP_V</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>217</td>\n",
       "      <td>T_neg_amp1_V</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>158</td>\n",
       "      <td>S_duration_V</td>\n",
       "      <td>0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>142</td>\n",
       "      <td>R_prominence_V</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>P_areas</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>Q_duration_V</td>\n",
       "      <td>0.000128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>189</td>\n",
       "      <td>P_neg_amp1_V</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>P_neg_areas_V</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        features   rfscore\n",
       "123         123          neg_RQ  0.145052\n",
       "99           99     rr_int_post  0.093900\n",
       "229         229   rr_int_post_V  0.074422\n",
       "7             7          R_amp5  0.052419\n",
       "8             8          R_amp6  0.047788\n",
       "..          ...             ...       ...\n",
       "142         142  R_prominence_V  0.000133\n",
       "55           55         P_areas  0.000133\n",
       "144         144    Q_duration_V  0.000128\n",
       "189         189    P_neg_amp1_V  0.000116\n",
       "199         199   P_neg_areas_V  0.000111\n",
       "\n",
       "[262 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_v_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "norm_dtw = [\"dtw1_v1\",\"dtw2_v1\"]\n",
    "classID = [\"classID\"]\n",
    "non_clinic = list(DB1_non_cli.iloc[:,0:140].columns.values)\n",
    "\n",
    "feature_norm_mill = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_mill.append(str(feat_labels[i]+\"_n_MLII\"))\n",
    "\n",
    "feature_norm_V1 = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_V1.append(str(feat_labels[i]+\"_n_V1\"))\n",
    "    \n",
    "\n",
    "c_ID = np.asarray(classID)\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "non_cli = np.asarray(non_clinic)\n",
    "norm_mlii = np.asarray(feature_norm_mill)\n",
    "norm_v1 = np.asarray(feature_norm_V1)\n",
    "norm_dtw = np.asarray(norm_dtw)\n",
    "\n",
    "\n",
    "features_clinic = np.hstack((f_M,f_V, f_d))\n",
    "features_clinic_norm = np.hstack((f_M,f_V, f_d,norm_mlii,norm_v1,norm_dtw ))\n",
    "#row=[]\n",
    "#for i in range(0,len(np_clinic_new_1)):\n",
    "    #row.append(i)\n",
    "    \n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "#X_train_balanced_norm = pd.DataFrame(X_train_norm,columns=features_clinic_norm)\n",
    "#X_test_norm = pd.DataFrame(X_test_norm,columns=features_clinic_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.01\n",
    "score_n_s = 0.01\n",
    "score_n_f = 0.015\n",
    "score_n_v = 0.01\n",
    "score_s_f = 0.01\n",
    "score_s_v = 0.015\n",
    "score_v_f = 0.01\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "n_s = rank_n_s[rank_n_s['rfscore'] >= score_n_s]['features'].values\n",
    "n_f = rank_n_f[rank_n_f['rfscore'] >= score_n_f]['features'].values\n",
    "n_v = rank_n_v[rank_n_v['rfscore'] >= score_n_v]['features'].values\n",
    "s_f = rank_s_f[rank_s_f['rfscore'] >= score_s_f]['features'].values\n",
    "s_v = rank_s_v[rank_s_v['rfscore'] >= score_s_v]['features'].values\n",
    "v_f = rank_v_f[rank_v_f['rfscore'] >= score_v_f]['features'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "['neg_RQ' 'rr_int_post' 'rr_int_post_V' 'R_amp5' 'R_amp6' 'R_amp4'\n",
      " 'Q_height' 'Q_prominence' 'P_neg_prominence_V' 'P_neg_height_V'\n",
      " 'rr_int_pre' 'R_amp3' 'rr_int_pre_V' 'R_height' 'R_amp7' 'TR'\n",
      " 'R_duration' 'RT_int' 'R_prominence']\n",
      "['rr_int_pre' 'rr_int_pre_V' 'PR_int_V' 'RP_V' 'PQ_int_V' 'rr_int_all_V'\n",
      " 'rr_int_all' 'P_prominence' 'P_duration_V' 'P_height' 'dtw1' 'P_areas'\n",
      " 'R_amp9' 'S_amp9_V' 'P_duration' 'S_amp1_V']\n",
      "['rr_int_pre_V' 'rr_int_pre' 'dtw1' 'R_duration' 'dtw2' 'P_prominence'\n",
      " 'P_height' 'neg_RQ' 'P_height_V' 'P_prominence_V' 'QRS_int' 'S_amp9_V']\n"
     ]
    }
   ],
   "source": [
    "print(len(v_f))\n",
    "print(v_f)\n",
    "print(n_s)\n",
    "print(n_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs all results for various classifier\n",
      "SVM\n",
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n",
      "XGboost\n",
      "[[34424  2201  2976  4420]\n",
      " [ 1189   469   383     9]\n",
      " [   49    75  3050    46]\n",
      " [   26     2    67   293]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86     44021\n",
      "           1       0.17      0.23      0.20      2050\n",
      "           2       0.47      0.95      0.63      3220\n",
      "           3       0.06      0.76      0.11       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.42      0.68      0.45     49679\n",
      "weighted avg       0.89      0.77      0.82     49679\n",
      "\n",
      "[0.34556103427813883, 1.8176868984700036, 0.3999913794478199]\n",
      "Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30605  5051  1006  7359]\n",
      " [  262  1441   296    51]\n",
      " [   46   250  2644   280]\n",
      " [   14     0    14   360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.70      0.82     44021\n",
      "           1       0.21      0.70      0.33      2050\n",
      "           2       0.67      0.82      0.74      3220\n",
      "           3       0.04      0.93      0.09       388\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     49679\n",
      "   macro avg       0.48      0.79      0.49     49679\n",
      "weighted avg       0.93      0.71      0.79     49679\n",
      "\n",
      "[0.32512357477278625, 2.4054564061636237, 0.4632438381568461]\n",
      "Linear_D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26769 10324   211  6717]\n",
      " [  317  1435   250    48]\n",
      " [   56   265  2534   365]\n",
      " [   17     0    11   360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.75     44021\n",
      "           1       0.12      0.70      0.20      2050\n",
      "           2       0.84      0.79      0.81      3220\n",
      "           3       0.05      0.93      0.09       388\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     49679\n",
      "   macro avg       0.50      0.76      0.47     49679\n",
      "weighted avg       0.93      0.63      0.73     49679\n",
      "\n",
      "[0.2526846194659283, 2.4492818710405277, 0.4325025436130301]\n",
      "SVM_LN\n",
      "[[34172  2882   617  6350]\n",
      " [  277  1495   240    38]\n",
      " [   70   335  2665   150]\n",
      " [   15     1    15   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87     44021\n",
      "           1       0.32      0.73      0.44      2050\n",
      "           2       0.75      0.83      0.79      3220\n",
      "           3       0.05      0.92      0.10       388\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     49679\n",
      "   macro avg       0.53      0.81      0.55     49679\n",
      "weighted avg       0.94      0.78      0.84     49679\n",
      "\n",
      "[0.40915665094099624, 2.627579154605376, 0.5330257197961701]\n",
      "voting_ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34493  2890   468  6170]\n",
      " [  280  1473   268    29]\n",
      " [   62   207  2805   146]\n",
      " [   16     0    15   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87     44021\n",
      "           1       0.32      0.72      0.45      2050\n",
      "           2       0.79      0.87      0.83      3220\n",
      "           3       0.05      0.92      0.10       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.54      0.82      0.56     49679\n",
      "weighted avg       0.94      0.79      0.85     49679\n",
      "\n",
      "[0.4242546814511974, 2.7007817216679695, 0.5497250559340949]\n",
      "ada\n",
      "Linear_d all samples\n",
      "[[39772   356   118  3775]\n",
      " [ 1708    49   254    39]\n",
      " [  379    63  2487   291]\n",
      " [   23     0     9   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     44021\n",
      "           1       0.10      0.02      0.04      2050\n",
      "           2       0.87      0.77      0.82      3220\n",
      "           3       0.08      0.92      0.15       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.50      0.65      0.48     49679\n",
      "weighted avg       0.90      0.86      0.88     49679\n",
      "\n",
      "[0.4309214399351081, 1.768118353887931, 0.43647551420354547]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " 0.43647551420354547)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"n vs all results for various classifier\")\n",
    "\n",
    "\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[feature_good]), good_features_y, np.asarray(X_test[feature_good]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[feature_good], good_features_y, X_test[feature_good], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[feature_good], good_features_y, X_test[feature_good], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[feature_good]), good_features_y, np.asarray(X_test[feature_good]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "voting_ensemble(np.asarray(X_train_balanced[feature_good]), good_features_y,np.asarray( X_test[feature_good]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[n_s]), good_features_y,np.asarray( X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "#print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "#feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "print(\"Linear_d all samples\")\n",
    "Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38399   783   351  4488]\n",
      " [ 1493   183   329    45]\n",
      " [   91    77  2839   213]\n",
      " [   20     0    12   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91     44021\n",
      "           1       0.18      0.09      0.12      2050\n",
      "           2       0.80      0.88      0.84      3220\n",
      "           3       0.07      0.92      0.13       388\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     49679\n",
      "   macro avg       0.50      0.69      0.50     49679\n",
      "weighted avg       0.91      0.84      0.87     49679\n",
      "\n",
      "[0.4323372331587701, 1.9504222520303145, 0.45997139808317433]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=10, max_iter=100,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "           solver='warn', tol=0.0001, verbose=0, warm_start=False),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " 0.45997139808317433)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs s results for various classifier\n",
      "SVM\n",
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n",
      "XGboost\n",
      "[[37025  1600  1842  3554]\n",
      " [  395   372  1236    47]\n",
      " [  149    77  2960    34]\n",
      " [  119     0   266     3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.84      0.91     44021\n",
      "           1       0.18      0.18      0.18      2050\n",
      "           2       0.47      0.92      0.62      3220\n",
      "           3       0.00      0.01      0.00       388\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     49679\n",
      "   macro avg       0.41      0.49      0.43     49679\n",
      "weighted avg       0.91      0.81      0.85     49679\n",
      "\n",
      "[0.4087591045303665, 1.7518131968012998, 0.4233562018653457]\n",
      "Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35340  3093  1051  4537]\n",
      " [  262  1742    16    30]\n",
      " [   65  1339  1530   286]\n",
      " [   14     5     4   365]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.89     44021\n",
      "           1       0.28      0.85      0.42      2050\n",
      "           2       0.59      0.48      0.53      3220\n",
      "           3       0.07      0.94      0.13       388\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     49679\n",
      "   macro avg       0.48      0.77      0.49     49679\n",
      "weighted avg       0.93      0.78      0.84     49679\n",
      "\n",
      "[0.39184319502667486, 2.1950693123858067, 0.47030526156156327]\n",
      "Linear_D\n",
      "[[34824  3829   605  4763]\n",
      " [  272  1726    31    21]\n",
      " [   82  1510  1343   285]\n",
      " [   19     7     4   358]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.88     44021\n",
      "           1       0.24      0.84      0.38      2050\n",
      "           2       0.68      0.42      0.52      3220\n",
      "           3       0.07      0.92      0.12       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.49      0.74      0.47     49679\n",
      "weighted avg       0.93      0.77      0.83     49679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36609318738878444, 2.1803497326219206, 0.45559031027213226]\n",
      "SVM_LN\n",
      "[[34552  2755  2251  4463]\n",
      " [  276  1728    22    24]\n",
      " [  180  1002  1799   239]\n",
      " [   21     2     3   362]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87     44021\n",
      "           1       0.31      0.84      0.46      2050\n",
      "           2       0.44      0.56      0.49      3220\n",
      "           3       0.07      0.93      0.13       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.45      0.78      0.49     49679\n",
      "weighted avg       0.92      0.77      0.83     49679\n",
      "\n",
      "[0.37943178803339184, 2.1580210632546546, 0.45946852692352774]\n",
      "voting_ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36127  2665  1019  4210]\n",
      " [  266  1738    22    24]\n",
      " [  180   910  1896   234]\n",
      " [   20     2     5   361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.82      0.90     44021\n",
      "           1       0.33      0.85      0.47      2050\n",
      "           2       0.64      0.59      0.62      3220\n",
      "           3       0.07      0.93      0.14       388\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     49679\n",
      "   macro avg       0.51      0.80      0.53     49679\n",
      "weighted avg       0.93      0.81      0.85     49679\n",
      "\n",
      "[0.43133041836368746, 2.4080833644170343, 0.516675629733973]\n",
      "ada\n",
      "Linear_d all samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41176   386  1658   801]\n",
      " [ 1737   262    51     0]\n",
      " [  838   318  1874   190]\n",
      " [   42     0    40   306]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     44021\n",
      "           1       0.27      0.13      0.17      2050\n",
      "           2       0.52      0.58      0.55      3220\n",
      "           3       0.24      0.79      0.36       388\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     49679\n",
      "   macro avg       0.49      0.61      0.51     49679\n",
      "weighted avg       0.88      0.88      0.88     49679\n",
      "\n",
      "[0.4276036170905601, 1.4982648848262756, 0.4010849191485645]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " 0.4010849191485645)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"n vs s results for various classifier\")\n",
    "\n",
    "\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[n_s]), good_features_y, np.asarray(X_test[n_s]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[n_s], good_features_y, X_test[n_s], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[n_s], good_features_y, X_test[n_s], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[n_s]), good_features_y, np.asarray(X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "voting_ensemble(np.asarray(X_train_balanced[n_s]), good_features_y,np.asarray( X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[n_s]), good_features_y,np.asarray( X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "#print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "#feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "print(\"Linear_d all samples\")\n",
    "Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41468  1081  1090   382]\n",
      " [  457  1402   186     5]\n",
      " [  411   377  2334    98]\n",
      " [  130     1    22   235]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     44021\n",
      "           1       0.49      0.68      0.57      2050\n",
      "           2       0.64      0.72      0.68      3220\n",
      "           3       0.33      0.61      0.42       388\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49679\n",
      "   macro avg       0.61      0.74      0.66     49679\n",
      "weighted avg       0.93      0.91      0.92     49679\n",
      "\n",
      "[0.6373095096068558, 2.541406752990806, 0.6363305989272787]\n",
      "[[43039     0   739   243]\n",
      " [ 1988     0    62     0]\n",
      " [ 1029     0  2141    50]\n",
      " [  158     0    62   168]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.71      0.66      0.69      3220\n",
      "           3       0.36      0.43      0.40       388\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49679\n",
      "   macro avg       0.50      0.52      0.51     49679\n",
      "weighted avg       0.87      0.91      0.89     49679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-6e184ec7d7fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogisticRegress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msvm_model_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-8d93f1216b84>\u001b[0m in \u001b[0;36msvm_model_linear\u001b[1;34m(X_train, y_train, X_test, y_test, C, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, verbose, max_iter, decision_function_shape, random_state, labels)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msvm_model_linear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_metrics\u001b[1;34m(yhat, labels, lb)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_kappa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mjkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkappa\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.125\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjkappa\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_j_index\u001b[1;34m(confusion_matrix)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mSes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mSev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mPs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, labels=[0,1,2,3])\n",
    "\n",
    "svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model_linear = svm.SVC(kernel='linear')\n",
    "svm_model_linear.fit(np.asarray(X_train[s_v]), y_train)#, np.asarray(X_test[n_s]), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42062     0  1959     0]\n",
      " [ 2033     0    17     0]\n",
      " [ 2907     0   313     0]\n",
      " [  106     0   282     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.12      0.10      0.11      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "    accuracy                           0.85     49679\n",
      "   macro avg       0.25      0.26      0.26     49679\n",
      "weighted avg       0.80      0.85      0.83     49679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Larry Lewis\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1487c0872aa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_metrics\u001b[1;34m(yhat, labels, lb)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_kappa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mjkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkappa\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.125\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjkappa\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_j_index\u001b[1;34m(confusion_matrix)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mSes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mSev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mPs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "labels=[0,1,2,3]\n",
    "y_pred = svm_model_linear.predict(X_test[s_v])    \n",
    "print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metric.get_metrics(y_pred,y_test,lb=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs v results for various classifier\n",
      "SVM\n",
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n",
      "XGboost\n",
      "[[36036  2118  1312  4555]\n",
      " [ 1071   351   620     8]\n",
      " [   71   186  2728   235]\n",
      " [   27     6    44   311]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.82      0.89     44021\n",
      "           1       0.13      0.17      0.15      2050\n",
      "           2       0.58      0.85      0.69      3220\n",
      "           3       0.06      0.80      0.11       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.44      0.66      0.46     49679\n",
      "weighted avg       0.90      0.79      0.84     49679\n",
      "\n",
      "[0.3693069080718327, 1.7302617526882016, 0.4009361731219415]\n",
      "Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29708  5365  1357  7591]\n",
      " [  865   487    28   670]\n",
      " [   71   427  1821   901]\n",
      " [   12     7    21   348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.67      0.80     44021\n",
      "           1       0.08      0.24      0.12      2050\n",
      "           2       0.56      0.57      0.56      3220\n",
      "           3       0.04      0.90      0.07       388\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     49679\n",
      "   macro avg       0.41      0.59      0.39     49679\n",
      "weighted avg       0.90      0.65      0.75     49679\n",
      "\n",
      "[0.2119379706878744, 1.4448638856662783, 0.286576971052222]\n",
      "Linear_D\n",
      "[[28962  6935   313  7811]\n",
      " [ 1186   511    16   337]\n",
      " [   58   499  1744   919]\n",
      " [   14    10     2   362]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.66      0.78     44021\n",
      "           1       0.06      0.25      0.10      2050\n",
      "           2       0.84      0.54      0.66      3220\n",
      "           3       0.04      0.93      0.07       388\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     49679\n",
      "   macro avg       0.48      0.60      0.40     49679\n",
      "weighted avg       0.91      0.64      0.74     49679\n",
      "\n",
      "[0.19067781709267842, 1.6956014565786768, 0.3072890906186738]\n",
      "SVM_LN\n",
      "[[31181  4254  1069  7517]\n",
      " [ 1134   415    42   459]\n",
      " [   83   521  1768   848]\n",
      " [   14     6     7   361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.71      0.82     44021\n",
      "           1       0.08      0.20      0.11      2050\n",
      "           2       0.61      0.55      0.58      3220\n",
      "           3       0.04      0.93      0.08       388\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     49679\n",
      "   macro avg       0.42      0.60      0.40     49679\n",
      "weighted avg       0.90      0.68      0.77     49679\n",
      "\n",
      "[0.22119509117021854, 1.4439890900843, 0.2910961818456468]\n",
      "voting_ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31838  4034  1137  7012]\n",
      " [ 1152   414    45   439]\n",
      " [   91   415  1972   742]\n",
      " [   17     6    17   348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.83     44021\n",
      "           1       0.09      0.20      0.12      2050\n",
      "           2       0.62      0.61      0.62      3220\n",
      "           3       0.04      0.90      0.08       388\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     49679\n",
      "   macro avg       0.43      0.61      0.41     49679\n",
      "weighted avg       0.90      0.70      0.78     49679\n",
      "\n",
      "[0.23999164629519465, 1.5212871466220617, 0.31015671647535503]\n",
      "ada\n",
      "Linear_d all samples\n",
      "[[38080    18    70  5853]\n",
      " [ 1992     0     6    52]\n",
      " [  625     3  1883   709]\n",
      " [   28     0     3   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.90     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.96      0.58      0.73      3220\n",
      "           3       0.05      0.92      0.10       388\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     49679\n",
      "   macro avg       0.49      0.59      0.43     49679\n",
      "weighted avg       0.89      0.81      0.84     49679\n",
      "\n",
      "[0.30207436228521783, 1.5445175730177723, 0.3441018777698305]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " 0.3441018777698305)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"n vs v results for various classifier\")\n",
    "\n",
    "\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[n_v]), good_features_y, np.asarray(X_test[n_v]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[n_v], good_features_y, X_test[n_v], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[n_v], good_features_y, X_test[n_v], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[n_v]), good_features_y, np.asarray(X_test[n_v]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "voting_ensemble(np.asarray(X_train_balanced[n_v]), good_features_y,np.asarray( X_test[n_v]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[n_s]), good_features_y,np.asarray( X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "#print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "#feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "print(\"Linear_d all samples\")\n",
    "Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38495   663   571  4292]\n",
      " [ 1794   161    47    48]\n",
      " [  309   225  2254   432]\n",
      " [   52     0    14   322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91     44021\n",
      "           1       0.15      0.08      0.10      2050\n",
      "           2       0.78      0.70      0.74      3220\n",
      "           3       0.06      0.83      0.12       388\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     49679\n",
      "   macro avg       0.49      0.62      0.47     49679\n",
      "weighted avg       0.90      0.83      0.86     49679\n",
      "\n",
      "[0.369087550491611, 1.7130278706674344, 0.3986722590792348]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=10, max_iter=100,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "           solver='warn', tol=0.0001, verbose=0, warm_start=False),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " 0.3986722590792348)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs f results for various classifier\n",
      "SVM\n"
     ]
    }
   ],
   "source": [
    "print(\"n vs f results for various classifier\")\n",
    "\n",
    "\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[n_f]), good_features_y, np.asarray(X_test[n_f]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[n_f], good_features_y, X_test[n_f], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[n_f], good_features_y, X_test[n_f], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[n_f]), good_features_y, np.asarray(X_test[n_f]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "voting_ensemble(np.asarray(X_train_balanced[n_f]), good_features_y,np.asarray( X_test[n_f]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[n_s]), good_features_y,np.asarray( X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "#print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "#feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "print(\"Linear_d all samples\")\n",
    "Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"s vs v results for various classifier\")\n",
    "\n",
    "\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[s_v]), good_features_y, np.asarray(X_test[s_v]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[s_v], good_features_y, X_test[s_v], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[s_v], good_features_y, X_test[s_v], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[s_v]), good_features_y, np.asarray(X_test[s_v]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "voting_ensemble(np.asarray(X_train_balanced[s_v]), good_features_y,np.asarray( X_test[s_v]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[n_s]), good_features_y,np.asarray( X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "#print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "#feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "print(\"Linear_d all samples\")\n",
    "Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Larry Lewis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Larry Lewis\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37197    25  5241  1558]\n",
      " [ 2015     0    30     5]\n",
      " [ 1881     8  1145   186]\n",
      " [   23     0   350    15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.17      0.36      0.23      3220\n",
      "           3       0.01      0.04      0.01       388\n",
      "\n",
      "    accuracy                           0.77     49679\n",
      "   macro avg       0.27      0.31      0.28     49679\n",
      "weighted avg       0.81      0.77      0.79     49679\n",
      "\n",
      "[0.11492007775841549, 0.524818557530069, 0.12306235857046638]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=10, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " 0.12306235857046638)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['R_amp9', 'R_amp8', 'R_areas', 'R_amp7', 'R_amp2', 'rr_int_all_V',\n",
       "       'S_duration', 'rr_int_all', 'rr_int_50_V', 'R_prominence',\n",
       "       'rr_int_50', 'PQ_int_50', 'R_amp3', 'R_amp1', 'R_height',\n",
       "       'R_amp2_V', 'R_amp3_V', 'rr_int_post', 'neg_PR', 'S_height',\n",
       "       'S_prominence'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s vs f results for various classifier\n",
      "SVM\n",
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n",
      "XGboost\n",
      "[[28318  1867  8738  5098]\n",
      " [ 1190   140   667    53]\n",
      " [  437    98  2664    21]\n",
      " [   12     2   342    32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.64      0.77     44021\n",
      "           1       0.07      0.07      0.07      2050\n",
      "           2       0.21      0.83      0.34      3220\n",
      "           3       0.01      0.08      0.01       388\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     49679\n",
      "   macro avg       0.31      0.41      0.30     49679\n",
      "weighted avg       0.85      0.63      0.70     49679\n",
      "\n",
      "[0.1656087989785762, 1.1767153540642357, 0.22989381874731757]\n",
      "Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24053 11106  1475  7387]\n",
      " [ 1094   779    85    92]\n",
      " [  234   132  2647   207]\n",
      " [   10     7    18   353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.55      0.69     44021\n",
      "           1       0.06      0.38      0.11      2050\n",
      "           2       0.63      0.82      0.71      3220\n",
      "           3       0.04      0.91      0.08       388\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     49679\n",
      "   macro avg       0.42      0.66      0.40     49679\n",
      "weighted avg       0.88      0.56      0.67     49679\n",
      "\n",
      "[0.17079767814603286, 1.893345657662342, 0.3220670462808092]\n",
      "Linear_D\n",
      "[[24169 10818   999  8035]\n",
      " [ 1177   659   165    49]\n",
      " [  344   219  2380   277]\n",
      " [   14     4    11   359]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.55      0.69     44021\n",
      "           1       0.06      0.32      0.10      2050\n",
      "           2       0.67      0.74      0.70      3220\n",
      "           3       0.04      0.93      0.08       388\n",
      "\n",
      "   micro avg       0.55      0.55      0.55     49679\n",
      "   macro avg       0.43      0.63      0.39     49679\n",
      "weighted avg       0.88      0.55      0.66     49679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1534823747117315, 1.7863982419300082, 0.3000409675971168]\n",
      "SVM_LN\n",
      "[[24754 11124  1719  6424]\n",
      " [ 1121   793    42    94]\n",
      " [  280    68  2786    86]\n",
      " [   15     1    26   346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.56      0.71     44021\n",
      "           1       0.07      0.39      0.11      2050\n",
      "           2       0.61      0.87      0.72      3220\n",
      "           3       0.05      0.89      0.09       388\n",
      "\n",
      "   micro avg       0.58      0.58      0.58     49679\n",
      "   macro avg       0.42      0.68      0.41     49679\n",
      "weighted avg       0.88      0.58      0.68     49679\n",
      "\n",
      "[0.18110285754598957, 1.927435258052646, 0.3314808360295755]\n",
      "voting_ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27777  8512  1375  6357]\n",
      " [ 1225   686    61    78]\n",
      " [  234    63  2837    86]\n",
      " [   14     1    27   346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.63      0.76     44021\n",
      "           1       0.07      0.33      0.12      2050\n",
      "           2       0.66      0.88      0.75      3220\n",
      "           3       0.05      0.89      0.10       388\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     49679\n",
      "   macro avg       0.43      0.68      0.43     49679\n",
      "weighted avg       0.89      0.64      0.73     49679\n",
      "\n",
      "[0.21751400249750358, 1.9495235652644198, 0.3524474469068043]\n",
      "ada\n",
      "Linear_d all samples\n",
      "[[39916   173   647  3285]\n",
      " [ 1928    16    82    24]\n",
      " [ 1256    10  1874    80]\n",
      " [   29     0    10   349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92     44021\n",
      "           1       0.08      0.01      0.01      2050\n",
      "           2       0.72      0.58      0.64      3220\n",
      "           3       0.09      0.90      0.17       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.45      0.60      0.44     49679\n",
      "weighted avg       0.87      0.85      0.86     49679\n",
      "\n",
      "[0.33151149870777397, 1.3873777799370228, 0.33917797184601484]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " 0.33917797184601484)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"s vs f results for various classifier\")\n",
    "\n",
    "\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[s_f]), good_features_y, np.asarray(X_test[s_f]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[s_f], good_features_y, X_test[s_f], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[s_f], good_features_y, X_test[s_f], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[s_f]), good_features_y, np.asarray(X_test[s_f]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "voting_ensemble(np.asarray(X_train_balanced[s_f]), good_features_y,np.asarray( X_test[s_f]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[n_s]), good_features_y,np.asarray( X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "#print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "#feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "print(\"Linear_d all samples\")\n",
    "Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, labels=[0,1,2,3])\n",
    "svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v vs f results for various classifier\n",
      "SVM\n",
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n",
      "XGboost\n",
      "[[34995  2277  1624  5125]\n",
      " [ 1410   204   426    10]\n",
      " [   65   170  2827   158]\n",
      " [   17     0    15   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.79      0.87     44021\n",
      "           1       0.08      0.10      0.09      2050\n",
      "           2       0.58      0.88      0.70      3220\n",
      "           3       0.06      0.92      0.12       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.42      0.67      0.44     49679\n",
      "weighted avg       0.89      0.77      0.82     49679\n",
      "\n",
      "[0.3306256681027137, 1.6322968559762685, 0.3693499410483904]\n",
      "Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35025  3144   692  5160]\n",
      " [ 1282   652    80    36]\n",
      " [   42   282  2660   236]\n",
      " [   23     2    17   346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.80      0.87     44021\n",
      "           1       0.16      0.32      0.21      2050\n",
      "           2       0.77      0.83      0.80      3220\n",
      "           3       0.06      0.89      0.11       388\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     49679\n",
      "   macro avg       0.49      0.71      0.50     49679\n",
      "weighted avg       0.91      0.78      0.83     49679\n",
      "\n",
      "[0.3536483597946771, 2.075177698589769, 0.4362213922210597]\n",
      "Linear_D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35690  3831   511  3989]\n",
      " [ 1269   650    87    44]\n",
      " [   36   428  2356   400]\n",
      " [   74     5    12   297]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.81      0.88     44021\n",
      "           1       0.13      0.32      0.19      2050\n",
      "           2       0.79      0.73      0.76      3220\n",
      "           3       0.06      0.77      0.12       388\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     49679\n",
      "   macro avg       0.49      0.66      0.49     49679\n",
      "weighted avg       0.91      0.78      0.84     49679\n",
      "\n",
      "[0.3484094613177523, 1.975361127439436, 0.4211248715888056]\n",
      "SVM_LN\n",
      "[[36556  1760   633  5072]\n",
      " [ 1321   620    62    47]\n",
      " [  210   215  2568   227]\n",
      " [   14     3    11   360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89     44021\n",
      "           1       0.24      0.30      0.27      2050\n",
      "           2       0.78      0.80      0.79      3220\n",
      "           3       0.06      0.93      0.12       388\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     49679\n",
      "   macro avg       0.51      0.71      0.52     49679\n",
      "weighted avg       0.91      0.81      0.85     49679\n",
      "\n",
      "[0.3843785427328184, 2.1229613011059927, 0.45755943400465826]\n",
      "voting_ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36728  1755   567  4971]\n",
      " [ 1331   607    85    27]\n",
      " [   74   215  2727   204]\n",
      " [   14     1    13   360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89     44021\n",
      "           1       0.24      0.30      0.26      2050\n",
      "           2       0.80      0.85      0.82      3220\n",
      "           3       0.06      0.93      0.12       388\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     49679\n",
      "   macro avg       0.52      0.73      0.53     49679\n",
      "weighted avg       0.92      0.81      0.86     49679\n",
      "\n",
      "[0.40304107841299547, 2.182396282797802, 0.47432007455622294]\n",
      "ada\n",
      "Linear_d all samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43556   152   286    27]\n",
      " [ 1633   263   154     0]\n",
      " [  588    82  2341   209]\n",
      " [  287     0     2    99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     44021\n",
      "           1       0.53      0.13      0.21      2050\n",
      "           2       0.84      0.73      0.78      3220\n",
      "           3       0.30      0.26      0.27       388\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     49679\n",
      "   macro avg       0.65      0.52      0.56     49679\n",
      "weighted avg       0.92      0.93      0.92     49679\n",
      "\n",
      "[0.6049793419285331, 2.225664951030601, 0.5806977898430916]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " array([2, 0, 0, ..., 0, 0, 0]),\n",
       " 0.5806977898430916)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"v vs f results for various classifier\")\n",
    "\n",
    "\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[v_f]), good_features_y, np.asarray(X_test[v_f]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[v_f], good_features_y, X_test[v_f], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[v_f], good_features_y, X_test[v_f], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[v_f]), good_features_y, np.asarray(X_test[v_f]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "voting_ensemble(np.asarray(X_train_balanced[v_f]), good_features_y,np.asarray( X_test[v_f]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[n_s]), good_features_y,np.asarray( X_test[n_s]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "#print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "#feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "print(\"Linear_d all samples\")\n",
    "Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43883     0   138     0]\n",
      " [ 1729     0   321     0]\n",
      " [  747     0  2473     0]\n",
      " [  386     0     2     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.84      0.77      0.80      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     49679\n",
      "   macro avg       0.45      0.44      0.44     49679\n",
      "weighted avg       0.89      0.93      0.91     49679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-77a47c299a25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, labels=[0,1,2,3])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msvm_model_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv_f\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv_f\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-8d93f1216b84>\u001b[0m in \u001b[0;36msvm_model_linear\u001b[1;34m(X_train, y_train, X_test, y_test, C, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, verbose, max_iter, decision_function_shape, random_state, labels)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msvm_model_linear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_metrics\u001b[1;34m(yhat, labels, lb)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_kappa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mjkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkappa\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.125\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjkappa\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_j_index\u001b[1;34m(confusion_matrix)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mSes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mSev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mPs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "#logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, labels=[0,1,2,3])\n",
    "svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42752   373   525   371]\n",
      " [ 1508   111   402    29]\n",
      " [  178   165  2620   257]\n",
      " [  232     1     3   152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96     44021\n",
      "           1       0.17      0.05      0.08      2050\n",
      "           2       0.74      0.81      0.77      3220\n",
      "           3       0.19      0.39      0.25       388\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     49679\n",
      "   macro avg       0.51      0.56      0.52     49679\n",
      "weighted avg       0.90      0.92      0.91     49679\n",
      "\n",
      "[0.5887435842627939, 1.776608337520022, 0.5164478343213997]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=10, max_iter=100,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "           solver='warn', tol=0.0001, verbose=0, warm_start=False),\n",
       " array([2, 0, 0, ..., 0, 0, 0]),\n",
       " 0.5164478343213997)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## small C from 1 to 3 seems to be good for f class recalled\n",
    "## degree 3 is better all around\n",
    "## Tol at 0.01 seems best\n",
    "\n",
    "def svm_model_poly(X_train, y_train, X_test, y_test, C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                        coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3]):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test,lb=labels))\n",
    "    \n",
    "    return svm_model_linear,y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def Linear_D(X_train, y_train, X_test, y_test, labels = [0,1,2,3]):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred, labels = labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return clf,y_pred, metric.get_metrics(y_pred,y_test,lb=labels)[2]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "## numner of estimators leaves at 1000 \n",
    "\n",
    "def randomForest(X_train, y_train, X_test, y_test,n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, max_features=50, max_leaf_nodes=None, class_weight='balanced', labels = [0,1,2,3]):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,min_weight_fraction_leaf=min_weight_fraction_leaf, class_weight=class_weight, min_impurity_decrease=min_impurity_decrease,max_features=max_features)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return rf, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "### xgbo is good for normal classes too\n",
    "## do this one next \n",
    "## XGB is very goos for V class\n",
    "## XGboost tree classifcation is good with 5 or more tree depth\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def xgboost(X_train,y_train, X_test, y_test, max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, process_type='default', predictor= 'cpu_predictor', labels = [0,1,2,3]): \n",
    "    model = XGBClassifier(max_depth = max_depth, eta = eta, gamma = gamma,min_child_weight=min_child_weight,subsample=subsample, max_delta_step= max_delta_step,colsample_bytree=colsample_bytree,colsample_bylevel=colsample_bylevel,colsample_bynode=colsample_bynode,alpha=alpha,reg_lambda=reg_lambda,grow_policy=grow_policy, tree_method=tree_method,refresh_leaf=refresh_leaf,process_type=process_type, predictor= predictor,objective = 'reg:logistic')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return model, y_pred,metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## L1 one is good , l2 would be used\n",
    "## very good at f class recall\n",
    "## solver = ['newton-cg', 'lbfgs', 'liblinear'are all good, ensemble with all would be good\n",
    "\n",
    "## false dual is better\n",
    "def logisticRegress(X_train, y_train, X_test, y_test, penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, labels = [0,1,2,3]):\n",
    "    lr = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        # dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "    lr.fit(X_train, y_train.ravel())  \n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return lr,y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def ada(X_train, y_train, X_test, y_test,labels = [0,1,2,3]):\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10,  kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    ada = AdaBoostClassifier(base_estimator=svm_model_poly, n_estimators=50 )\n",
    "    ada.fit(X_train, y_train)  \n",
    "    y_pred = ada.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return ada, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "\n",
    "\n",
    "def svm_model_linear(X_train, y_train, X_test, y_test, C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3] ):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape=decision_function_shape, random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return svm_model_linear, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "def voting_ensemble(X_train, y_train, X_test,y_test, labels=[0,1,2,3]):\n",
    "    svm_model_linear = svm.SVC(C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                            coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                            coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, \n",
    "                                min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, \n",
    "                                max_features=50, max_leaf_nodes=None, class_weight='balanced')\n",
    "\n",
    "\n",
    "    lr = LogisticRegression( penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, \n",
    "                            class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn',\n",
    "                            verbose=0, warm_start=False, n_jobs=None)\n",
    "\n",
    "\n",
    "    xgb = XGBClassifier(max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, \n",
    "                        subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, \n",
    "                        reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, \n",
    "                        process_type='default', predictor= 'cpu_predictor',objective = 'reg:logistic')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[ ('xgb',xgb), ('lr',lr), ('svm_ln',svm_model_linear)], voting='hard')#, weights=[2,2,2,2])\n",
    "\n",
    "\n",
    "\n",
    "    #clf1 = clf1.fit(X, y)\n",
    "    #clf2 = clf2.fit(X, y)\n",
    "    #clf3 = clf3.fit(X, y)\n",
    "    eclf = eclf.fit(X_train, y_train)\n",
    "\n",
    "    predict = eclf.predict(X_test)\n",
    "    #scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "    #scores.mean()                             \n",
    "\n",
    "    print(confusion_matrix(y_test,predict, labels=labels))  \n",
    "    print(classification_report(y_test,predict, labels=labels))\n",
    "    print(metric.get_metrics(predict.ravel(),y_test.ravel(),lb=labels))\n",
    "    \n",
    "    return eclf, predict, metric.get_metrics(predict.ravel(),y_test.ravel(), lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
