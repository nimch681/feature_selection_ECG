{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes.python import features_columns as col\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "from codes.python import TunedClassifier as classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n",
    "\n",
    "\n",
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])\n",
    "\n",
    "rank = pd.read_csv(\"database/features_ranking.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "\n",
    "rank_n_f = pd.read_csv(\"database/features_n_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_n_s = pd.read_csv(\"database/features_n_vs_s_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_n_v = pd.read_csv(\"database/features_n_vs_v_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_s_f = pd.read_csv(\"database/features_s_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_s_v = pd.read_csv(\"database/features_s_vs_v_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "rank_v_f = pd.read_csv(\"database/features_v_vs_f_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "\n",
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log\n",
      "all\n",
      "[[14197   928     0     0]\n",
      " [  231  1470     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     15125\n",
      "           1       0.61      0.86      0.72      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     16826\n",
      "   macro avg       0.40      0.45      0.42     16826\n",
      "weighted avg       0.95      0.93      0.94     16826\n",
      "\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13550  1575     0     0]\n",
      " [  200  1501     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94     15125\n",
      "           1       0.49      0.88      0.63      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     16826\n",
      "   macro avg       0.37      0.44      0.39     16826\n",
      "weighted avg       0.94      0.89      0.91     16826\n",
      "\n",
      "n_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13359  1766     0     0]\n",
      " [  218  1483     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93     15125\n",
      "           1       0.46      0.87      0.60      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     16826\n",
      "   macro avg       0.36      0.44      0.38     16826\n",
      "weighted avg       0.93      0.88      0.90     16826\n",
      "\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14124  1001     0     0]\n",
      " [  222  1479     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96     15125\n",
      "           1       0.60      0.87      0.71      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     16826\n",
      "   macro avg       0.40      0.45      0.42     16826\n",
      "weighted avg       0.95      0.93      0.93     16826\n",
      "\n",
      "s_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12136  2989     0     0]\n",
      " [  638  1063     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.80      0.87     15125\n",
      "           1       0.26      0.62      0.37      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     16826\n",
      "   macro avg       0.30      0.36      0.31     16826\n",
      "weighted avg       0.88      0.78      0.82     16826\n",
      "\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12953  2172     0     0]\n",
      " [  427  1274     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91     15125\n",
      "           1       0.37      0.75      0.50      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     16826\n",
      "   macro avg       0.33      0.40      0.35     16826\n",
      "weighted avg       0.91      0.85      0.87     16826\n",
      "\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14006  1119     0     0]\n",
      " [  273  1428     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95     15125\n",
      "           1       0.56      0.84      0.67      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     16826\n",
      "   macro avg       0.39      0.44      0.41     16826\n",
      "weighted avg       0.94      0.92      0.92     16826\n",
      "\n",
      "Lin\n",
      "all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14947   178     0     0]\n",
      " [  664  1037     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     15125\n",
      "           1       0.85      0.61      0.71      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     16826\n",
      "   macro avg       0.45      0.40      0.42     16826\n",
      "weighted avg       0.95      0.95      0.95     16826\n",
      "\n",
      "n_s\n",
      "[[14822   303     0     0]\n",
      " [  600  1101     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     15125\n",
      "           1       0.78      0.65      0.71      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     16826\n",
      "   macro avg       0.44      0.41      0.42     16826\n",
      "weighted avg       0.94      0.95      0.94     16826\n",
      "\n",
      "n_v\n",
      "[[14838   287     0     0]\n",
      " [  705   996     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97     15125\n",
      "           1       0.78      0.59      0.67      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16826\n",
      "   macro avg       0.43      0.39      0.41     16826\n",
      "weighted avg       0.94      0.94      0.94     16826\n",
      "\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14900   225     0     0]\n",
      " [  698  1003     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     15125\n",
      "           1       0.82      0.59      0.68      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     16826\n",
      "   macro avg       0.44      0.39      0.41     16826\n",
      "weighted avg       0.94      0.95      0.94     16826\n",
      "\n",
      "s_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14950   175     0     0]\n",
      " [ 1192   509     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96     15125\n",
      "           1       0.74      0.30      0.43      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     16826\n",
      "   macro avg       0.42      0.32      0.35     16826\n",
      "weighted avg       0.91      0.92      0.90     16826\n",
      "\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14995   130     0     0]\n",
      " [  943   758     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     15125\n",
      "           1       0.85      0.45      0.59      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16826\n",
      "   macro avg       0.45      0.36      0.39     16826\n",
      "weighted avg       0.93      0.94      0.93     16826\n",
      "\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14968   157     0     0]\n",
      " [  814   887     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     15125\n",
      "           1       0.85      0.52      0.65      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16826\n",
      "   macro avg       0.45      0.38      0.40     16826\n",
      "weighted avg       0.94      0.94      0.94     16826\n",
      "\n",
      "randomforest\n",
      "all\n",
      "[[15042    83     0     0]\n",
      " [   76  1625     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     15125\n",
      "           1       0.95      0.96      0.95      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     16826\n",
      "   macro avg       0.49      0.49      0.49     16826\n",
      "weighted avg       0.99      0.99      0.99     16826\n",
      "\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15018   107     0     0]\n",
      " [  101  1600     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     15125\n",
      "           1       0.94      0.94      0.94      1701\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     16826\n",
      "   macro avg       0.48      0.48      0.48     16826\n",
      "weighted avg       0.99      0.99      0.99     16826\n",
      "\n",
      "n_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "## Train test from DB1\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_test = X_test[2000:4000]\n",
    "#y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 0] = 1\n",
    "y_train[y_train == 0] = 0\n",
    "\n",
    "y_test[y_test != 0] = 1\n",
    "y_test[y_test == 0] = 0\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train test from DB1\n",
    "## Train test from DB1\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_test = X_test[2000:4000]\n",
    "#y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 1] = 0\n",
    "y_train[y_train == 1] = 1\n",
    "\n",
    "y_test[y_test != 1] = 0\n",
    "y_test[y_test == 1] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train test from DB1\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_test = X_test[2000:4000]\n",
    "#y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 2] = 0\n",
    "y_train[y_train == 2] = 1\n",
    "\n",
    "y_test[y_test != 2] = 0\n",
    "y_test[y_test == 2] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train test from DB1\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_test = X_test[2000:4000]\n",
    "#y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 3] = 0\n",
    "y_train[y_train == 3] = 1\n",
    "\n",
    "y_test[y_test != 3] = 0\n",
    "y_test[y_test == 3] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, test, y_train, ytest = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "X_test = X_test[2000:4000]\n",
    "y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 0] = 1\n",
    "y_train[y_train == 0] = 0\n",
    "\n",
    "y_test[y_test != 0] = 1\n",
    "y_test[y_test == 0] = 0\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, test, y_train, ytest = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "X_test = X_test[2000:4000]\n",
    "y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 1] = 0\n",
    "y_train[y_train == 1] = 1\n",
    "\n",
    "y_test[y_test != 1] = 0\n",
    "y_test[y_test == 1] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, test, y_train, ytest = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "X_test = X_test[2000:4000]\n",
    "y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 2] = 0\n",
    "y_train[y_train == 2] = 1\n",
    "\n",
    "y_test[y_test != 2] = 0\n",
    "y_test[y_test == 2] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, test, y_train, ytest = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "X_test = X_test[2000:4000]\n",
    "y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 3] = 0\n",
    "y_train[y_train == 3] = 1\n",
    "\n",
    "y_test[y_test != 3] = 0\n",
    "y_test[y_test == 3] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, test, y_train, ytest = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_test = X_test[2000:4000]\n",
    "#y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 0] =1\n",
    "y_train[y_train == 0] = 0\n",
    "\n",
    "y_test[y_test != 0] = 1\n",
    "y_test[y_test == 0] = 0\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, test, y_train, ytest = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_test = X_test[2000:4000]\n",
    "#y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 1] = 0\n",
    "y_train[y_train == 1] = 1\n",
    "\n",
    "y_test[y_test != 1] = 0\n",
    "y_test[y_test == 1] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, test, y_train, ytest = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_test = X_test[2000:4000]\n",
    "#y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 2] = 0\n",
    "y_train[y_train == 2] = 1\n",
    "\n",
    "y_test[y_test != 2] = 0\n",
    "y_test[y_test == 2] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, test, y_train, ytest = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_test = X_test[2000:4000]\n",
    "#y_test = y_test[2000:4000]\n",
    "#train, X_test, ytrain, y_test = train_test_split(X_test, y_test, test_size=0.80, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 3] = 0\n",
    "y_train[y_train == 3] = 1\n",
    "\n",
    "y_test[y_test != 3] = 0\n",
    "y_test[y_test == 3] = 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "classifier.svm_model_poly(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "classifier.xgboost(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "classifier.ada(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=False ,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
