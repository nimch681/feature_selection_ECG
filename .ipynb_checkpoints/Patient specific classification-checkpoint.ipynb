{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls2.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Inter-patient\n",
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Patient_specific\n",
    "np_clinic_1, np_clinic_2,np_non_var_1, np_non_var_2, np_class_ID_1, np_class_ID_2 = DF.get_all_dataframe_patient_specific(200,patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### put these into different files and do cross validation on random data\n",
    "\n",
    "X_train = np_clinic_1\n",
    "X_test = np_clinic_2\n",
    "y_train = np_class_ID_1\n",
    "y_test = np_class_ID_2\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np_class_ID_1_old\n",
    "y_test = np_class_ID_2_old\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np_non_var_1\n",
    "X_test = np_non_var_2\n",
    "y_train = np_class_ID_1\n",
    "y_test = np_class_ID_2\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np_non_var_1_old\n",
    "X_test = np_non_var_2_old\n",
    "y_train = np_class_ID_1_old\n",
    "y_test = np_class_ID_2_old\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "#shuffle(y_test)\n",
    "shuffle(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45809 976 3788 414\n",
      "45809 976 3788 414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45809 976 3788 414\n",
      "41228 878 3409 372\n",
      "41228 878 3409 372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "old_beats_train = y_train\n",
    "n_beat = X_train[y_train==0]\n",
    "s_beat = X_train[y_train==1]\n",
    "v_beat = X_train[y_train==2]\n",
    "f_beat = X_train[y_train==3]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = y_train[y_train==0]\n",
    "s_beat_y = y_train[y_train==1]\n",
    "v_beat_y = y_train[y_train==2]\n",
    "f_beat_y = y_train[y_train==3]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "outdetect = IsolationForest(max_features = input_size)\n",
    "\n",
    "outliers_n = outdetect.fit_predict(n_beat)\n",
    "outliers_s = outdetect.fit_predict(s_beat)\n",
    "outliers_v = outdetect.fit_predict(v_beat)\n",
    "outliers_f = outdetect.fit_predict(f_beat)\n",
    "\n",
    "print(len(outliers_n), len(outliers_s), len(outliers_v),len(outliers_f) )\n",
    "\n",
    "n_beat = n_beat[outliers_n==1]\n",
    "s_beat = s_beat[outliers_s==1]\n",
    "v_beat = v_beat[outliers_v==1]\n",
    "f_beat = f_beat[outliers_f==1]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = n_beat_y[outliers_n==1]\n",
    "s_beat_y = s_beat_y[outliers_s==1]  \n",
    "v_beat_y = v_beat_y[outliers_v==1]\n",
    "f_beat_y = f_beat_y[outliers_f==1]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "n_beat_y = n_beat_y.reshape(n_beat_y.shape[0],1)\n",
    "s_beat_y = s_beat_y.reshape(s_beat_y.shape[0],1)\n",
    "v_beat_y = v_beat_y.reshape(v_beat_y.shape[0],1)\n",
    "f_beat_y = f_beat_y.reshape(f_beat_y.shape[0],1)\n",
    "\n",
    "\n",
    "X_train = np.vstack((n_beat,s_beat, v_beat, f_beat))\n",
    "y_train = np.vstack((n_beat_y,s_beat_y, v_beat_y, f_beat_y))\n",
    "\n",
    "\n",
    "#outlers_test = outdetect.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44021 2050 3220 388\n",
      "44021 2050 3220 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44021 2050 3220 388\n",
      "39619 1845 2898 349\n",
      "39619 1845 2898 349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "old_beats_train = y_test\n",
    "n_beat = X_test[y_test==0]\n",
    "s_beat = X_test[y_test==1]\n",
    "v_beat = X_test[y_test==2]\n",
    "f_beat = X_test[y_test==3]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = y_test[y_test==0]\n",
    "s_beat_y = y_test[y_test==1]\n",
    "v_beat_y = y_test[y_test==2]\n",
    "f_beat_y = y_test[y_test==3]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "outdetect = IsolationForest(max_features = input_size)\n",
    "\n",
    "outliers_n = outdetect.fit_predict(n_beat)\n",
    "outliers_s = outdetect.fit_predict(s_beat)\n",
    "outliers_v = outdetect.fit_predict(v_beat)\n",
    "outliers_f = outdetect.fit_predict(f_beat)\n",
    "\n",
    "print(len(outliers_n), len(outliers_s), len(outliers_v),len(outliers_f) )\n",
    "\n",
    "n_beat = n_beat[outliers_n==1]\n",
    "s_beat = s_beat[outliers_s==1]\n",
    "v_beat = v_beat[outliers_v==1]\n",
    "f_beat = f_beat[outliers_f==1]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = n_beat_y[outliers_n==1]\n",
    "s_beat_y = s_beat_y[outliers_s==1]\n",
    "v_beat_y = v_beat_y[outliers_v==1]\n",
    "f_beat_y = f_beat_y[outliers_f==1]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "n_beat_y = n_beat_y.reshape(n_beat_y.shape[0],1)\n",
    "s_beat_y = s_beat_y.reshape(s_beat_y.shape[0],1)\n",
    "v_beat_y = v_beat_y.reshape(v_beat_y.shape[0],1)\n",
    "f_beat_y = f_beat_y.reshape(f_beat_y.shape[0],1)\n",
    "\n",
    "\n",
    "X_test = np.vstack((n_beat,s_beat, v_beat, f_beat))\n",
    "y_test = np.vstack((n_beat_y,s_beat_y, v_beat_y, f_beat_y))\n",
    "\n",
    "\n",
    "#outlers_test = outdetect.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\nkf.get_n_splits(X_test) # returns the number of splitting iterations in the cross-validator\\n\\n\\nX_test_list = []\\ny_test_list = []\\nfor one, two in kf.split(X_test):\\n    \\n    X_test_list.append(X_test[one])\\n    X_test_list.append(X_test[two])\\n    y_test_list.append(y_test[one])\\n    y_test_list.append(y_test[two])\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold # import KFold\n",
    "kf = KFold(n_splits=10) # Define the split - into 2 folds \n",
    "kf.get_n_splits(X_train, y_train)#, group) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "for one, two in kf.split(X_train, y_train):\n",
    "    \n",
    "    X_train_list.append(X_train[one])\n",
    "    #X_train_list.append(X_train[two])\n",
    "    y_train_list.append(y_train[one])\n",
    "    #y_train_list.append(y_train[two])\n",
    "\n",
    "\"\"\"\"\"\n",
    "kf.get_n_splits(X_test) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "for one, two in kf.split(X_test):\n",
    "    \n",
    "    X_test_list.append(X_test[one])\n",
    "    X_test_list.append(X_test[two])\n",
    "    y_test_list.append(y_test[one])\n",
    "    y_test_list.append(y_test[two])\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.50000000e-02, 1.71700000e+00, 1.27209826e+00, ...,\n",
       "        5.72000000e-01, 4.94365770e+01, 3.24134449e+01],\n",
       "       [4.40000000e-02, 1.76700000e+00, 1.31646640e+00, ...,\n",
       "        6.59000000e-01, 4.47620178e+01, 2.95263425e+01],\n",
       "       [5.00000000e-02, 1.65600000e+00, 1.26706064e+00, ...,\n",
       "        6.25000000e-01, 3.98122876e+01, 2.84777532e+01],\n",
       "       ...,\n",
       "       [4.40000000e-02, 2.03100000e+00, 1.51055365e+00, ...,\n",
       "        2.67000000e-01, 3.34084533e+01, 1.23166004e+01],\n",
       "       [4.20000000e-02, 1.31200000e+00, 9.10227012e-01, ...,\n",
       "        4.47000000e-01, 1.91703140e+01, 1.14237112e+01],\n",
       "       [3.90000000e-02, 2.51200000e+00, 1.99452763e+00, ...,\n",
       "        5.61000000e-01, 3.73188354e+01, 1.25373348e+01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "\n",
    "v_stack_train, v_stack_labels=ada.fit_sample(X_train_list[0], y_train_list[0])\n",
    "v_stack_labels = v_stack_labels.reshape(v_stack_labels.shape[0],1)\n",
    "for i in range(1,len(X_train_list)):\n",
    "    \n",
    "    ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "\n",
    "    train, train_labels=ada.fit_sample(X_train_list[i], y_train_list[i])\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "\n",
    "    v_stack_train = np.vstack((v_stack_train, train))\n",
    "    v_stack_labels = np.vstack((v_stack_labels, train_labels))\n",
    "    v_stack_labels = v_stack_labels.reshape(v_stack_labels.shape[0],1)\n",
    "\n",
    "#test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = v_stack_train\n",
    "#X_test = test\n",
    "y_train = v_stack_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]\n",
    "#print(\"0:\", Counter(y_train == 0), \"3:\", Counter(y_train == 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v_stack_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np_clinic_all,DB_class_all, test_size=0.33, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(DB_var_non_all,DB_class_all, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34072  1562  2326  1659]\n",
      " [  828   931    73    13]\n",
      " [  448    71  2311    68]\n",
      " [  109     0     9   231]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91     39619\n",
      "           1       0.36      0.50      0.42      1845\n",
      "           2       0.49      0.80      0.61      2898\n",
      "           3       0.12      0.66      0.20       349\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     44711\n",
      "   macro avg       0.48      0.71      0.53     44711\n",
      "weighted avg       0.90      0.84      0.86     44711\n",
      "\n",
      "[0.44298629708919973, 2.154880483902555, 0.49085320903241925]\n"
     ]
    }
   ],
   "source": [
    "### voting classification\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "C_value = 10\n",
    "use_probability = True\n",
    "multi_mode = 'ovo'\n",
    "svm_model_linear = svm.SVC(C=C_value, kernel='linear', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "svm_model_rbf = svm.SVC(C=C_value, kernel='rbf', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "svm_model_poly = svm.SVC(C=C_value, kernel='poly', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linearclass = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.001, C=10, multi_class='crammer_singer', fit_intercept=True, intercept_scaling=10, class_weight='balanced')\n",
    "\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=20)\n",
    "linear_dis = LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "#eclf = VotingClassifier(estimators=[('svc_1', svm_model_linear),('svc_2',svm_model_rbf), ('svc_3', svm_model_poly), ('knn', clf2), ('dt', clf1), ('ld',linear_dis)],\n",
    "                       #voting='hard', weights=[2,1,1, 1, 1,2])\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('svc_1', svm_model_linear),('svc_2',svm_model_rbf), ('svc_3', svm_model_poly), ('svc_linear',linearclass)],\n",
    "                       voting='hard')#, #weights=[2,1,1, 1, 1,2])\n",
    "\n",
    "\n",
    "#clf1 = clf1.fit(X, y)\n",
    "#clf2 = clf2.fit(X, y)\n",
    "#clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X_train, y_train)\n",
    "\n",
    "predic = eclf.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predic))  \n",
    "print(classification_report(y_test,predic))\n",
    "print(metric.get_metrics(predic, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_metrics(predic, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patient specific non clicically imporant\n",
    "[[255   4   4  29]\n",
    " [ 70 214   3   5]\n",
    " [ 11   4 263  14]\n",
    " [ 14   1   7 270]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.73      0.87      0.79       292\n",
    "           1       0.96      0.73      0.83       292\n",
    "           2       0.95      0.90      0.92       292\n",
    "           3       0.85      0.92      0.89       292\n",
    "\n",
    "    accuracy                           0.86      1168\n",
    "   macro avg       0.87      0.86      0.86      1168\n",
    "weighted avg       0.87      0.86      0.86      1168\n",
    "\n",
    "#patient specific non clicically imporant 200 beats\n",
    "\n",
    "[[353  11   2  20]\n",
    " [ 86 287   2  11]\n",
    " [ 19   7 344  16]\n",
    " [ 18   0  10 358]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.74      0.91      0.82       386\n",
    "           1       0.94      0.74      0.83       386\n",
    "           2       0.96      0.89      0.92       386\n",
    "           3       0.88      0.93      0.91       386\n",
    "\n",
    "    accuracy                           0.87      1544\n",
    "   macro avg       0.88      0.87      0.87      1544\n",
    "weighted avg       0.88      0.87      0.87      1544\n",
    "\n",
    "[0.8255613126079447, 3.536592487212716, 0.8548547172055618]\n",
    "\n",
    "\n",
    "## 200 beats clicinally imporant \n",
    "[[365   9   1  11]\n",
    " [ 74 294   5  13]\n",
    " [  5   6 350  25]\n",
    " [ 20   0   7 359]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.95      0.86       386\n",
    "           1       0.95      0.76      0.85       386\n",
    "           2       0.96      0.91      0.93       386\n",
    "           3       0.88      0.93      0.90       386\n",
    "\n",
    "    accuracy                           0.89      1544\n",
    "   macro avg       0.90      0.89      0.89      1544\n",
    "weighted avg       0.90      0.89      0.89      1544\n",
    "\n",
    "[0.8480138169257341, 3.5840374208867227,    0.87]\n",
    "\n",
    "\n",
    "\n",
    "## 100 beats clicinally imporant \n",
    "\n",
    "[[371   4   2  10]\n",
    " [ 75 304   3   5]\n",
    " [  9   5 351  22]\n",
    " [ 15   1   8 363]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.96      0.87       387\n",
    "           1       0.97      0.79      0.87       387\n",
    "           2       0.96      0.91      0.93       387\n",
    "           3       0.91      0.94      0.92       387\n",
    "\n",
    "    accuracy                           0.90      1548\n",
    "   macro avg       0.91      0.90      0.90      1548\n",
    "weighted avg       0.91      0.90      0.90      1548\n",
    "\n",
    "[0.8630490956072352, 3.6249450404760726, 0.8846426778631267]\n",
    "\n",
    "\n",
    "## 50 beats clicinally imporant \n",
    "\n",
    "\n",
    "[[352  11   2  22]\n",
    " [ 75 301   1  10]\n",
    " [  8   7 360  12]\n",
    " [ 18   0  10 359]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.78      0.91      0.84       387\n",
    "           1       0.94      0.78      0.85       387\n",
    "           2       0.97      0.93      0.95       387\n",
    "           3       0.89      0.93      0.91       387\n",
    "\n",
    "    accuracy                           0.89      1548\n",
    "   macro avg       0.89      0.89      0.89      1548\n",
    "weighted avg       0.89      0.89      0.89      1548\n",
    "\n",
    "[0.8484065460809647, 3.6167314567120212, 0.8762947051294849]\n",
    "\n",
    "\n",
    "## hard cincially important\n",
    "\n",
    "[[313  11  33  31]\n",
    " [134 217  28   9]\n",
    " [ 21  20 301  46]\n",
    " [ 54   0  10 324]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.60      0.81      0.69       388\n",
    "           1       0.88      0.56      0.68       388\n",
    "           2       0.81      0.78      0.79       388\n",
    "           3       0.79      0.84      0.81       388\n",
    "\n",
    "    accuracy                           0.74      1552\n",
    "   macro avg       0.77      0.74      0.74      1552\n",
    "weighted avg       0.77      0.74      0.74      1552\n",
    "\n",
    "[0.6589347079037801, 3.0191913313379892, 0.7068662703691386]\n",
    "   \n",
    "## hard cincially important\n",
    "[[338   9  11  30]\n",
    " [199 173  10   6]\n",
    " [ 33  13 286  56]\n",
    " [143   0   9 236]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.47      0.87      0.61       388\n",
    "           1       0.89      0.45      0.59       388\n",
    "           2       0.91      0.74      0.81       388\n",
    "           3       0.72      0.61      0.66       388\n",
    "\n",
    "    accuracy                           0.67      1552\n",
    "   macro avg       0.75      0.67      0.67      1552\n",
    "weighted avg       0.75      0.67      0.67      1552\n",
    "\n",
    "##Random \n",
    "\n",
    "[[170  53  83  82]\n",
    " [181  44  82  81]\n",
    " [166  51  82  89]\n",
    " [196  47  69  76]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.24      0.44      0.31       388\n",
    "           1       0.23      0.11      0.15       388\n",
    "           2       0.26      0.21      0.23       388\n",
    "           3       0.23      0.20      0.21       388\n",
    "\n",
    "    accuracy                           0.24      1552\n",
    "   macro avg       0.24      0.24      0.23      1552\n",
    "weighted avg       0.24      0.24      0.23      1552\n",
    "\n",
    "## Random first one only\n",
    "[[40 29 43 27]\n",
    " [38 33 33 35]\n",
    " [42 32 28 37]\n",
    " [36 36 29 38]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.26      0.29      0.27       139\n",
    "           1       0.25      0.24      0.25       139\n",
    "           2       0.21      0.20      0.21       139\n",
    "           3       0.28      0.27      0.28       139\n",
    "\n",
    "    accuracy                           0.25       556\n",
    "   macro avg       0.25      0.25      0.25       556\n",
    "weighted avg       0.25      0.25      0.25       556\n",
    "\n",
    "\n",
    "[[261  12   4  15]\n",
    " [ 73 204   5  10]\n",
    " [ 18   2 256  16]\n",
    " [ 16   0   5 271]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.89      0.79       292\n",
    "           1       0.94      0.70      0.80       292\n",
    "           2       0.95      0.88      0.91       292\n",
    "           3       0.87      0.93      0.90       292\n",
    "\n",
    "    accuracy                           0.85      1168\n",
    "   macro avg       0.87      0.85      0.85      1168\n",
    "weighted avg       0.87      0.85      0.85      1168\n",
    "\n",
    "[0.7990867579908675, 3.4592704304153346, 0.8319521827973506]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def linear_svc(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='crammer_singer', fit_intercept=True, intercept_scaling=1, class_weight='balanced')\n",
    "    linearclass = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='crammer_singer', fit_intercept=True, intercept_scaling=1, class_weight='balanced')\n",
    "    linearclass.fit(X_train, y_train)\n",
    "    y_pred = linearclass.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test))\n",
    "    \n",
    "    return metric.get_metrics(y_pred,y_test)[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "C_value = 10\n",
    "use_probability = True\n",
    "multi_mode = 'ovo'\n",
    "svm_model_linear = svm.SVC(C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "\n",
    "svm_model_linear.fit(X_train, y_train)\n",
    "y_pred = svm_model_linear.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(get_metrics(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)  \n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metric.get_metrics(y_pred,y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CINICALLY IMPORTANT Ld PATIENT SPECIFIC 500 beats\n",
    "\n",
    "[0.8105022831050229, 3.5426613831955103, 0.8480838144519502]\n",
    "\n",
    "[[239  25   3  25]\n",
    " [ 34 242   2  14]\n",
    " [ 16   6 247  23]\n",
    " [ 14   2   6 270]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.82      0.80       292\n",
    "           1       0.88      0.83      0.85       292\n",
    "           2       0.96      0.85      0.90       292\n",
    "           3       0.81      0.92      0.87       292\n",
    "\n",
    "    accuracy                           0.85      1168\n",
    "   macro avg       0.86      0.85      0.86      1168\n",
    "weighted avg       0.86      0.85      0.86      1168\n",
    "\n",
    "\n",
    "## CINICALLY IMPORTANT Ld PATIENT SPECIFIC 200 beats\n",
    "\n",
    "[[39499   289   108   108]\n",
    " [  571  1297    30     3]\n",
    " [  308    45  2443   170]\n",
    " [   68     1     8   309]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.99      0.98     40004\n",
    "           1       0.79      0.68      0.73      1901\n",
    "           2       0.94      0.82      0.88      2966\n",
    "           3       0.52      0.80      0.63       386\n",
    "\n",
    "    accuracy                           0.96     45257\n",
    "   macro avg       0.81      0.82      0.81     45257\n",
    "weighted avg       0.96      0.96      0.96     45257\n",
    "\n",
    "[0.8154891252234552, 3.2442786908654684, 0.8132793989699112]\n",
    "\n",
    "## ## CINICALLY IMPORTANT Ld PATIENT SPECIFIC 100 beats\n",
    "\n",
    "[[41518   273   107    97]\n",
    " [  578  1363    30     3]\n",
    " [  324    44  2563   170]\n",
    " [   70     1     8   308]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.99      0.98     41995\n",
    "           1       0.81      0.69      0.75      1974\n",
    "           2       0.95      0.83      0.88      3101\n",
    "           3       0.53      0.80      0.64       387\n",
    "\n",
    "    accuracy                           0.96     47457\n",
    "   macro avg       0.82      0.83      0.81     47457\n",
    "weighted avg       0.96      0.96      0.96     47457\n",
    "\n",
    "[0.822502043287163, 3.2742656057347928, 0.8205342223604306]\n",
    "\n",
    "\n",
    "\n",
    "## ## CINICALLY IMPORTANT Ld PATIENT SPECIFIC 50 beats\n",
    "\n",
    "[[42536   263   105    98]\n",
    " [  582  1396    30     3]\n",
    " [  323    46  2618   170]\n",
    " [   73     1     8   305]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.99      0.98     43002\n",
    "           1       0.82      0.69      0.75      2011\n",
    "           2       0.95      0.83      0.88      3157\n",
    "           3       0.53      0.79      0.63       387\n",
    "\n",
    "    accuracy                           0.96     48557\n",
    "   macro avg       0.82      0.83      0.81     48557\n",
    "weighted avg       0.97      0.96      0.96     48557\n",
    "\n",
    "[0.8257275532860733, 3.289945856907006, 0.8241070087564124]\n",
    "\n",
    "## ## CINICALLY IMPORTANT Ld PATIENT SPECIFIC all beats\n",
    "\n",
    "[[38659   801  1786  2775]\n",
    " [ 1507   480    32    31]\n",
    " [  246   154  2565   255]\n",
    " [   47     3     7   331]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.88      0.92     44021\n",
    "           1       0.33      0.23      0.28      2050\n",
    "           2       0.58      0.80      0.67      3220\n",
    "           3       0.10      0.85      0.18       388\n",
    "\n",
    "    accuracy                           0.85     49679\n",
    "   macro avg       0.49      0.69      0.51     49679\n",
    "weighted avg       0.90      0.85      0.87     49679\n",
    "\n",
    "[0.4319916898828909, 1.948809592726481, 0.45959704403225554]\n",
    "\n",
    "\n",
    "\n",
    "[[35844  1266  3898  3013]\n",
    " [ 1690    54   178   128]\n",
    " [ 2610   103   278   229]\n",
    " [  315    15    36    22]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.81      0.85     44021\n",
    "           1       0.04      0.03      0.03      2050\n",
    "           2       0.06      0.09      0.07      3220\n",
    "           3       0.01      0.06      0.01       388\n",
    "\n",
    "    accuracy                           0.73     49679\n",
    "   macro avg       0.25      0.25      0.24     49679\n",
    "weighted avg       0.79      0.73      0.76     49679\n",
    "\n",
    "[[38659   801  1786  2775]\n",
    " [ 1507   480    32    31]\n",
    " [  246   154  2565   255]\n",
    " [   47     3     7   331]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.88      0.92     44021\n",
    "           1       0.33      0.23      0.28      2050\n",
    "           2       0.58      0.80      0.67      3220\n",
    "           3       0.10      0.85      0.18       388\n",
    "\n",
    "    accuracy                           0.85     49679\n",
    "   macro avg       0.49      0.69      0.51     49679\n",
    "weighted avg       0.90      0.85      0.87     49679\n",
    "\n",
    "[[40675  2239  1008    99]\n",
    " [ 1900    95    49     6]\n",
    " [ 3015   140    64     1]\n",
    " [  363    19     5     1]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.92      0.90     44021\n",
    "           1       0.04      0.05      0.04      2050\n",
    "           2       0.06      0.02      0.03      3220\n",
    "           3       0.01      0.00      0.00       388\n",
    "\n",
    "    accuracy                           0.82     49679\n",
    "   macro avg       0.25      0.25      0.24     49679\n",
    "weighted avg       0.79      0.82      0.80     49679\n",
    "\n",
    "\n",
    "[[246  26   5  15]\n",
    " [ 39 240   5   8]\n",
    " [ 16  17 237  22]\n",
    " [ 17   2   6 267]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.77      0.84      0.81       292\n",
    "           1       0.84      0.82      0.83       292\n",
    "           2       0.94      0.81      0.87       292\n",
    "           3       0.86      0.91      0.88       292\n",
    "\n",
    "    accuracy                           0.85      1168\n",
    "   macro avg       0.85      0.85      0.85      1168\n",
    "weighted avg       0.85      0.85      0.85      1168\n",
    "\n",
    "[0.7968036529680366, 3.4124258002741437, 0.8249550515182862]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, Adamax, Nadam\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "from LossHistory import *\n",
    "import time\n",
    "\n",
    "\n",
    "bin_labels = y_train.copy()\n",
    "bin_labels[y_train != 0]=1\n",
    "bin_labels[y_train == 0]=0\n",
    "bin_labels=np.array(bin_labels, dtype=int)\n",
    "\n",
    "bin_labels_test = y_test.copy()\n",
    "bin_labels_test[y_test != 0]=1\n",
    "bin_labels_test[y_test == 0]=0\n",
    "bin_labels_test=np.array(bin_labels_test, dtype=int)\n",
    "\n",
    "class_weights = {}\n",
    "for c in range(0,2):\n",
    "    class_weights.update({c:len(bin_labels) / float(np.count_nonzero(bin_labels == c))})\n",
    "    \n",
    "input_size=X_train.shape[1]\n",
    "# Establish the NN\n",
    "eps=150\n",
    "opt=RMSprop(lr=0.001)\n",
    "print (input_size)\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "model =  Sequential()\n",
    "model.add(Dense(input_size, activation='sigmoid', input_dim=input_size))\n",
    "# hidden layers.\n",
    "model.add(Dense(50, activation='sigmoid'))\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "\n",
    "# output layer.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# overall structure.\n",
    "model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "loss_history=LossHistory()\n",
    "cb=[loss_history,LearningRateScheduler(step_decay)]\n",
    "\n",
    "# train neural network.\n",
    "history=model.fit(X_train, bin_labels, class_weight=class_weights, epochs=eps, callbacks=cb)\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred=y_pred.astype(int)\n",
    "y_pred=y_pred.ravel()\n",
    "#predictions.append(y_pred)\n",
    "\n",
    "print(confusion_matrix(bin_labels_test,y_pred))  \n",
    "print(classification_report(bin_labels_test,y_pred))\n",
    "#print(get_metrics(predic, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#np_clinic_2 = np.hstack((np_variables_2,np_V1_2, dtw_clinic_2))\n",
    "\n",
    "norm_var_1_clinic = post_features.normalised_values_multiples(np_clinic_1)\n",
    "norm_var_2_clinic = post_features.normalised_values_multiples(np_clinic_2)\n",
    "\n",
    "norm_var_1_non = post_features.normalised_values_multiples(np_non_var_1)\n",
    "norm_var_2_non = post_features.normalised_values_multiples(np_non_var_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_clinic_1_class = np.hstack((np_class_ID_1,np_clinic_1))\n",
    "#np_clinic_2_class = np.hstack((np_class_ID_2,np_clinic_2))\n",
    "\n",
    "np_clinic_1_class = np.hstack((np_class_ID_1,np_clinic_1,norm_var_1_clinic))\n",
    "np_clinic_2_class = np.hstack((np_class_ID_2,np_clinic_2,norm_var_2_clinic))\n",
    "\n",
    "#np_clinic_1_class = np.hstack((np_class_ID_1, np_non_var_1))\n",
    "#np_clinic_2_class = np.hstack((np_class_ID_2, np_non_var_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "norm_dtw = [\"dtw1_v1\",\"dtw2_v1\"]\n",
    "classID = [\"classID\"]\n",
    "non_clinic = list(DB1_non_cli.iloc[:,0:140].columns.values)\n",
    "\n",
    "feature_norm_mill = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_mill.append(str(feat_labels[i]+\"_n_MLII\"))\n",
    "\n",
    "feature_norm_V1 = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_V1.append(str(feat_labels[i]+\"_n_V1\"))\n",
    "    \n",
    "\n",
    "c_ID = np.asarray(classID)\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "non_cli = np.asarray(non_clinic)\n",
    "norm_mlii = np.asarray(feature_norm_mill)\n",
    "norm_v1 = np.asarray(feature_norm_V1)\n",
    "norm_dtw = np.asarray(norm_dtw)\n",
    "\n",
    "\n",
    "#features_clinic = np.hstack((c_ID,non_cli))#f_M,f_V, f_d,non_cli,norm_mlii, norm_v1,norm_dtw))\n",
    "features_clinic = np.hstack((c_ID,f_M,f_V, f_d,norm_mlii, norm_v1,norm_dtw))\n",
    "\n",
    "row=[]\n",
    "for i in range(0,len(np_clinic_1_class)):\n",
    "    row.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(np_clinic_1_class,columns=features_clinic)\n",
    "df2 = pd.DataFrame(np_clinic_2_class,columns=features_clinic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymrmr as mrmr\n",
    "\n",
    "rank=mrmr.mRMR(df, 'MID', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_clinic_1 = df[rank]\n",
    "np_clinic_2 = df2[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def reject_outliers(data, m=4):\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4],[6,7],[8,9]])\n",
    "y = np.array([1, 2, 3, 4,5,6])\n",
    "kf = KFold(n_splits=3)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)  \n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
