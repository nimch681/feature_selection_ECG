{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls2.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'database/DB1_patient_list.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2fd74efee324>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##Inter-patient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp_clinic_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_clinic_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp_non_var_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_non_var_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_class_ID_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp_class_ID_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatients_ls_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatients_ls_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB1_V1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB2_V1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB1_non_cli\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB2_non_cli\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB1_dwt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB2_dwt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB1_dwt_V1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDB2_dwt_V1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatient_l_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpatient_ls_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpatient_l_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpatient_ls_2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mls2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mls2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Github\\feature_selection_ECG\\feature_selection_ECG\\codes\\python\\load_DF_beats.py\u001b[0m in \u001b[0;36mget_all_dataframe\u001b[1;34m(patient_l_1, patient_l_2, ls, ls2)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mDB1_patients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"database/DB1_patient_list.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m#DB1_patients = DB1_patients.drop([1872])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mDB1_patients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDB1_patients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDB1_patients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'database/DB1_patient_list.csv' does not exist"
     ]
    }
   ],
   "source": [
    "##Inter-patient\n",
    "np_clinic_1, np_clinic_2,np_non_var_1, np_non_var_2, np_class_ID_1, np_class_ID_2, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Patient_specific\n",
    "np_clinic_1, np_clinic_2,np_non_var_1, np_non_var_2, np_class_ID_1, np_class_ID_2 = DF.get_all_dataframe_patient_specific(200,patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### put these into different files and do cross validation on random data\n",
    "\n",
    "X_train = np_clinic_1\n",
    "X_test = np_clinic_2\n",
    "y_train = np_class_ID_1\n",
    "y_test = np_class_ID_2\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1656"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np_non_var_1\n",
    "X_test = np_non_var_2\n",
    "y_train = np_class_ID_1\n",
    "y_test = np_class_ID_2\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "#shuffle(y_test)\n",
    "shuffle(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=2 greater than the number of groups: 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-40631988edca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_train_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my_train_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mX_train_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    329\u001b[0m                 .format(self.n_splits, n_samples))\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BaseKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mBy\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelegates\u001b[0m \u001b[0mto\u001b[0m \u001b[0m_iter_test_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \"\"\"\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_test_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mtest_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mtest_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_test_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    511\u001b[0m             raise ValueError(\"Cannot have number of splits n_splits=%d greater\"\n\u001b[0;32m    512\u001b[0m                              \u001b[1;34m\" than the number of groups: %d.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m                              % (self.n_splits, n_groups))\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;31m# Weight groups by their number of occurrences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=2 greater than the number of groups: 1."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold # import KFold\n",
    "kf = GroupKFold(n_splits=2) # Define the split - into 2 folds \n",
    "group = np.ones(3312)\n",
    "kf.get_n_splits(X_train, y_train)#, group) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "for one, two in kf.split(X_train, y_train, group):\n",
    "    \n",
    "    X_train_list.append(X_train[one])\n",
    "    X_train_list.append(X_train[two])\n",
    "    y_train_list.append(y_train[one])\n",
    "    y_train_list.append(y_train[two])\n",
    "\n",
    "\"\"\"\"\"\n",
    "kf.get_n_splits(X_test) # returns the number of splitting iterations in the cross-validator\n",
    "\n",
    "\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "for one, two in kf.split(X_test):\n",
    "    \n",
    "    X_test_list.append(X_test[one])\n",
    "    X_test_list.append(X_test[two])\n",
    "    y_test_list.append(y_test[one])\n",
    "    y_test_list.append(y_test[two])\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.get_n_splits(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "\n",
    "v_stack_train, v_stack_labels=ada.fit_sample(X_train_list[0], y_train_list[0])\n",
    "v_stack_labels = v_stack_labels.reshape(v_stack_labels.shape[0],1)\n",
    "for i in range(1,len(X_train_list)):\n",
    "    \n",
    "    ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "\n",
    "    train, train_labels=ada.fit_sample(X_train_list[i], y_train_list[i])\n",
    "    train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "\n",
    "\n",
    "    v_stack_train = np.vstack((v_stack_train, train))\n",
    "    v_stack_labels = np.vstack((v_stack_labels, train_labels))\n",
    "    v_stack_labels = v_stack_labels.reshape(v_stack_labels.shape[0],1)\n",
    "\n",
    "#test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = v_stack_train\n",
    "#X_test = test\n",
    "y_train = v_stack_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]\n",
    "#print(\"0:\", Counter(y_train == 0), \"3:\", Counter(y_train == 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='auto')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8280"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_stack_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np_clinic_all,DB_class_all, test_size=0.33, random_state=42)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(DB_var_non_all,DB_class_all, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33555  1073  2994  6399]\n",
      " [ 1468   505    42    35]\n",
      " [  169   143  2478   430]\n",
      " [   22     1    10   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.76      0.85     44021\n",
      "           1       0.29      0.25      0.27      2050\n",
      "           2       0.45      0.77      0.57      3220\n",
      "           3       0.05      0.91      0.09       388\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     49679\n",
      "   macro avg       0.44      0.67      0.44     49679\n",
      "weighted avg       0.89      0.74      0.80     49679\n",
      "\n",
      "[0.2892749867033267, 1.757758307452959, 0.3643572817832832]\n"
     ]
    }
   ],
   "source": [
    "### voting classification\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "C_value = 10\n",
    "use_probability = True\n",
    "multi_mode = 'ovo'\n",
    "svm_model_linear = svm.SVC(C=C_value, kernel='linear', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "svm_model_rbf = svm.SVC(C=C_value, kernel='rbf', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "svm_model_poly = svm.SVC(C=C_value, kernel='poly', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linearclass = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='crammer_singer', fit_intercept=True, intercept_scaling=1, class_weight='balanced')\n",
    "\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=20)\n",
    "linear_dis = LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "#eclf = VotingClassifier(estimators=[('svc_1', svm_model_linear),('svc_2',svm_model_rbf), ('svc_3', svm_model_poly), ('knn', clf2), ('dt', clf1), ('ld',linear_dis)],\n",
    "                       #voting='hard', weights=[2,1,1, 1, 1,2])\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('svc_1', svm_model_linear),('svc_2',svm_model_rbf), ('svc_3', svm_model_poly), ('svc_linear',linearclass)],\n",
    "                       voting='hard')#, #weights=[2,1,1, 1, 1,2])\n",
    "\n",
    "\n",
    "#clf1 = clf1.fit(X, y)\n",
    "#clf2 = clf2.fit(X, y)\n",
    "#clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X_train, y_train)\n",
    "\n",
    "predic = eclf.predict(X_test)\n",
    "#scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "#scores.mean()                             \n",
    "\n",
    "print(confusion_matrix(y_test,predic))  \n",
    "print(classification_report(y_test,predic))\n",
    "print(metric.get_metrics(predic, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6589347079037801, 3.0191913313379892, 0.7068662703691386]\n"
     ]
    }
   ],
   "source": [
    "print(get_metrics(predic, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patient specific non clicically imporant\n",
    "[[255   4   4  29]\n",
    " [ 70 214   3   5]\n",
    " [ 11   4 263  14]\n",
    " [ 14   1   7 270]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.73      0.87      0.79       292\n",
    "           1       0.96      0.73      0.83       292\n",
    "           2       0.95      0.90      0.92       292\n",
    "           3       0.85      0.92      0.89       292\n",
    "\n",
    "    accuracy                           0.86      1168\n",
    "   macro avg       0.87      0.86      0.86      1168\n",
    "weighted avg       0.87      0.86      0.86      1168\n",
    "\n",
    "#patient specific non clicically imporant 200 beats\n",
    "\n",
    "[[353  11   2  20]\n",
    " [ 86 287   2  11]\n",
    " [ 19   7 344  16]\n",
    " [ 18   0  10 358]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.74      0.91      0.82       386\n",
    "           1       0.94      0.74      0.83       386\n",
    "           2       0.96      0.89      0.92       386\n",
    "           3       0.88      0.93      0.91       386\n",
    "\n",
    "    accuracy                           0.87      1544\n",
    "   macro avg       0.88      0.87      0.87      1544\n",
    "weighted avg       0.88      0.87      0.87      1544\n",
    "\n",
    "[0.8255613126079447, 3.536592487212716, 0.8548547172055618]\n",
    "\n",
    "\n",
    "## 200 beats clicinally imporant \n",
    "[[365   9   1  11]\n",
    " [ 74 294   5  13]\n",
    " [  5   6 350  25]\n",
    " [ 20   0   7 359]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.95      0.86       386\n",
    "           1       0.95      0.76      0.85       386\n",
    "           2       0.96      0.91      0.93       386\n",
    "           3       0.88      0.93      0.90       386\n",
    "\n",
    "    accuracy                           0.89      1544\n",
    "   macro avg       0.90      0.89      0.89      1544\n",
    "weighted avg       0.90      0.89      0.89      1544\n",
    "\n",
    "[0.8480138169257341, 3.5840374208867227,    0.87]\n",
    "\n",
    "\n",
    "\n",
    "## 100 beats clicinally imporant \n",
    "\n",
    "[[371   4   2  10]\n",
    " [ 75 304   3   5]\n",
    " [  9   5 351  22]\n",
    " [ 15   1   8 363]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.96      0.87       387\n",
    "           1       0.97      0.79      0.87       387\n",
    "           2       0.96      0.91      0.93       387\n",
    "           3       0.91      0.94      0.92       387\n",
    "\n",
    "    accuracy                           0.90      1548\n",
    "   macro avg       0.91      0.90      0.90      1548\n",
    "weighted avg       0.91      0.90      0.90      1548\n",
    "\n",
    "[0.8630490956072352, 3.6249450404760726, 0.8846426778631267]\n",
    "\n",
    "\n",
    "## 50 beats clicinally imporant \n",
    "\n",
    "\n",
    "[[352  11   2  22]\n",
    " [ 75 301   1  10]\n",
    " [  8   7 360  12]\n",
    " [ 18   0  10 359]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.78      0.91      0.84       387\n",
    "           1       0.94      0.78      0.85       387\n",
    "           2       0.97      0.93      0.95       387\n",
    "           3       0.89      0.93      0.91       387\n",
    "\n",
    "    accuracy                           0.89      1548\n",
    "   macro avg       0.89      0.89      0.89      1548\n",
    "weighted avg       0.89      0.89      0.89      1548\n",
    "\n",
    "[0.8484065460809647, 3.6167314567120212, 0.8762947051294849]\n",
    "\n",
    "\n",
    "## hard cincially important\n",
    "\n",
    "[[313  11  33  31]\n",
    " [134 217  28   9]\n",
    " [ 21  20 301  46]\n",
    " [ 54   0  10 324]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.60      0.81      0.69       388\n",
    "           1       0.88      0.56      0.68       388\n",
    "           2       0.81      0.78      0.79       388\n",
    "           3       0.79      0.84      0.81       388\n",
    "\n",
    "    accuracy                           0.74      1552\n",
    "   macro avg       0.77      0.74      0.74      1552\n",
    "weighted avg       0.77      0.74      0.74      1552\n",
    "\n",
    "[0.6589347079037801, 3.0191913313379892, 0.7068662703691386]\n",
    "   \n",
    "## hard cincially important\n",
    "[[338   9  11  30]\n",
    " [199 173  10   6]\n",
    " [ 33  13 286  56]\n",
    " [143   0   9 236]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.47      0.87      0.61       388\n",
    "           1       0.89      0.45      0.59       388\n",
    "           2       0.91      0.74      0.81       388\n",
    "           3       0.72      0.61      0.66       388\n",
    "\n",
    "    accuracy                           0.67      1552\n",
    "   macro avg       0.75      0.67      0.67      1552\n",
    "weighted avg       0.75      0.67      0.67      1552\n",
    "\n",
    "##Random \n",
    "\n",
    "[[170  53  83  82]\n",
    " [181  44  82  81]\n",
    " [166  51  82  89]\n",
    " [196  47  69  76]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.24      0.44      0.31       388\n",
    "           1       0.23      0.11      0.15       388\n",
    "           2       0.26      0.21      0.23       388\n",
    "           3       0.23      0.20      0.21       388\n",
    "\n",
    "    accuracy                           0.24      1552\n",
    "   macro avg       0.24      0.24      0.23      1552\n",
    "weighted avg       0.24      0.24      0.23      1552\n",
    "\n",
    "## Random first one only\n",
    "[[40 29 43 27]\n",
    " [38 33 33 35]\n",
    " [42 32 28 37]\n",
    " [36 36 29 38]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.26      0.29      0.27       139\n",
    "           1       0.25      0.24      0.25       139\n",
    "           2       0.21      0.20      0.21       139\n",
    "           3       0.28      0.27      0.28       139\n",
    "\n",
    "    accuracy                           0.25       556\n",
    "   macro avg       0.25      0.25      0.25       556\n",
    "weighted avg       0.25      0.25      0.25       556\n",
    "\n",
    "\n",
    "[[261  12   4  15]\n",
    " [ 73 204   5  10]\n",
    " [ 18   2 256  16]\n",
    " [ 16   0   5 271]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.89      0.79       292\n",
    "           1       0.94      0.70      0.80       292\n",
    "           2       0.95      0.88      0.91       292\n",
    "           3       0.87      0.93      0.90       292\n",
    "\n",
    "    accuracy                           0.85      1168\n",
    "   macro avg       0.87      0.85      0.85      1168\n",
    "weighted avg       0.87      0.85      0.85      1168\n",
    "\n",
    "[0.7990867579908675, 3.4592704304153346, 0.8319521827973506]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[257  21  33  77]\n",
      " [231 124  12  21]\n",
      " [  6  25 298  59]\n",
      " [ 20   3  14 351]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.66      0.57       388\n",
      "           1       0.72      0.32      0.44       388\n",
      "           2       0.83      0.77      0.80       388\n",
      "           3       0.69      0.90      0.78       388\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      1552\n",
      "   macro avg       0.69      0.66      0.65      1552\n",
      "weighted avg       0.69      0.66      0.65      1552\n",
      "\n",
      "[0.5515463917525772, 2.6391257653171514, 0.6056639165409325]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linearclass = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='crammer_singer', fit_intercept=True, intercept_scaling=1, class_weight='balanced')\n",
    "linearclass.fit(X_train, y_train)\n",
    "y_pred = linearclass.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(get_metrics(y_pred,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[297  20   5  66]\n",
      " [239 131   4  14]\n",
      " [ 39  29 246  74]\n",
      " [ 38   2   8 340]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.77      0.59       388\n",
      "           1       0.72      0.34      0.46       388\n",
      "           2       0.94      0.63      0.76       388\n",
      "           3       0.69      0.88      0.77       388\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      1552\n",
      "   macro avg       0.71      0.65      0.64      1552\n",
      "weighted avg       0.71      0.65      0.64      1552\n",
      "\n",
      "[0.5378006872852233, 2.6267909210463403, 0.5972492087734043]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "C_value = 10\n",
    "use_probability = True\n",
    "multi_mode = 'ovo'\n",
    "svm_model_linear = svm.SVC(C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                    coef0=0.0, shrinking=True, probability=use_probability, tol=0.001, \n",
    "                    cache_size=200, verbose=False, \n",
    "                    max_iter=-1, decision_function_shape=multi_mode, random_state=None)\n",
    "\n",
    "\n",
    "svm_model_linear.fit(X_train, y_train)\n",
    "y_pred = svm_model_linear.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(get_metrics(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38659   801  1786  2775]\n",
      " [ 1507   480    32    31]\n",
      " [  246   154  2565   255]\n",
      " [   47     3     7   331]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92     44021\n",
      "           1       0.33      0.23      0.28      2050\n",
      "           2       0.58      0.80      0.67      3220\n",
      "           3       0.10      0.85      0.18       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.49      0.69      0.51     49679\n",
      "weighted avg       0.90      0.85      0.87     49679\n",
      "\n",
      "[0.4319916898828909, 1.948809592726481, 0.45959704403225554]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)  \n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metric.get_metrics(y_pred,y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CINICALLY IMPORTANT Ld PATIENT SPECIFIC 500 beats\n",
    "\n",
    "[0.8105022831050229, 3.5426613831955103, 0.8480838144519502]\n",
    "\n",
    "[[239  25   3  25]\n",
    " [ 34 242   2  14]\n",
    " [ 16   6 247  23]\n",
    " [ 14   2   6 270]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.82      0.80       292\n",
    "           1       0.88      0.83      0.85       292\n",
    "           2       0.96      0.85      0.90       292\n",
    "           3       0.81      0.92      0.87       292\n",
    "\n",
    "    accuracy                           0.85      1168\n",
    "   macro avg       0.86      0.85      0.86      1168\n",
    "weighted avg       0.86      0.85      0.86      1168\n",
    "\n",
    "\n",
    "## CINICALLY IMPORTANT Ld PATIENT SPECIFIC 200 beats\n",
    "\n",
    "[[39499   289   108   108]\n",
    " [  571  1297    30     3]\n",
    " [  308    45  2443   170]\n",
    " [   68     1     8   309]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.99      0.98     40004\n",
    "           1       0.79      0.68      0.73      1901\n",
    "           2       0.94      0.82      0.88      2966\n",
    "           3       0.52      0.80      0.63       386\n",
    "\n",
    "    accuracy                           0.96     45257\n",
    "   macro avg       0.81      0.82      0.81     45257\n",
    "weighted avg       0.96      0.96      0.96     45257\n",
    "\n",
    "[0.8154891252234552, 3.2442786908654684, 0.8132793989699112]\n",
    "\n",
    "## ## CINICALLY IMPORTANT Ld PATIENT SPECIFIC 100 beats\n",
    "\n",
    "[[41518   273   107    97]\n",
    " [  578  1363    30     3]\n",
    " [  324    44  2563   170]\n",
    " [   70     1     8   308]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.99      0.98     41995\n",
    "           1       0.81      0.69      0.75      1974\n",
    "           2       0.95      0.83      0.88      3101\n",
    "           3       0.53      0.80      0.64       387\n",
    "\n",
    "    accuracy                           0.96     47457\n",
    "   macro avg       0.82      0.83      0.81     47457\n",
    "weighted avg       0.96      0.96      0.96     47457\n",
    "\n",
    "[0.822502043287163, 3.2742656057347928, 0.8205342223604306]\n",
    "\n",
    "\n",
    "\n",
    "## ## CINICALLY IMPORTANT Ld PATIENT SPECIFIC 50 beats\n",
    "\n",
    "[[42536   263   105    98]\n",
    " [  582  1396    30     3]\n",
    " [  323    46  2618   170]\n",
    " [   73     1     8   305]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.99      0.98     43002\n",
    "           1       0.82      0.69      0.75      2011\n",
    "           2       0.95      0.83      0.88      3157\n",
    "           3       0.53      0.79      0.63       387\n",
    "\n",
    "    accuracy                           0.96     48557\n",
    "   macro avg       0.82      0.83      0.81     48557\n",
    "weighted avg       0.97      0.96      0.96     48557\n",
    "\n",
    "[0.8257275532860733, 3.289945856907006, 0.8241070087564124]\n",
    "\n",
    "## ## CINICALLY IMPORTANT Ld PATIENT SPECIFIC all beats\n",
    "\n",
    "[[38659   801  1786  2775]\n",
    " [ 1507   480    32    31]\n",
    " [  246   154  2565   255]\n",
    " [   47     3     7   331]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.88      0.92     44021\n",
    "           1       0.33      0.23      0.28      2050\n",
    "           2       0.58      0.80      0.67      3220\n",
    "           3       0.10      0.85      0.18       388\n",
    "\n",
    "    accuracy                           0.85     49679\n",
    "   macro avg       0.49      0.69      0.51     49679\n",
    "weighted avg       0.90      0.85      0.87     49679\n",
    "\n",
    "[0.4319916898828909, 1.948809592726481, 0.45959704403225554]\n",
    "\n",
    "\n",
    "\n",
    "[[35844  1266  3898  3013]\n",
    " [ 1690    54   178   128]\n",
    " [ 2610   103   278   229]\n",
    " [  315    15    36    22]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.81      0.85     44021\n",
    "           1       0.04      0.03      0.03      2050\n",
    "           2       0.06      0.09      0.07      3220\n",
    "           3       0.01      0.06      0.01       388\n",
    "\n",
    "    accuracy                           0.73     49679\n",
    "   macro avg       0.25      0.25      0.24     49679\n",
    "weighted avg       0.79      0.73      0.76     49679\n",
    "\n",
    "[[38659   801  1786  2775]\n",
    " [ 1507   480    32    31]\n",
    " [  246   154  2565   255]\n",
    " [   47     3     7   331]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.88      0.92     44021\n",
    "           1       0.33      0.23      0.28      2050\n",
    "           2       0.58      0.80      0.67      3220\n",
    "           3       0.10      0.85      0.18       388\n",
    "\n",
    "    accuracy                           0.85     49679\n",
    "   macro avg       0.49      0.69      0.51     49679\n",
    "weighted avg       0.90      0.85      0.87     49679\n",
    "\n",
    "[[40675  2239  1008    99]\n",
    " [ 1900    95    49     6]\n",
    " [ 3015   140    64     1]\n",
    " [  363    19     5     1]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.89      0.92      0.90     44021\n",
    "           1       0.04      0.05      0.04      2050\n",
    "           2       0.06      0.02      0.03      3220\n",
    "           3       0.01      0.00      0.00       388\n",
    "\n",
    "    accuracy                           0.82     49679\n",
    "   macro avg       0.25      0.25      0.24     49679\n",
    "weighted avg       0.79      0.82      0.80     49679\n",
    "\n",
    "\n",
    "[[246  26   5  15]\n",
    " [ 39 240   5   8]\n",
    " [ 16  17 237  22]\n",
    " [ 17   2   6 267]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.77      0.84      0.81       292\n",
    "           1       0.84      0.82      0.83       292\n",
    "           2       0.94      0.81      0.87       292\n",
    "           3       0.86      0.91      0.88       292\n",
    "\n",
    "    accuracy                           0.85      1168\n",
    "   macro avg       0.85      0.85      0.85      1168\n",
    "weighted avg       0.85      0.85      0.85      1168\n",
    "\n",
    "[0.7968036529680366, 3.4124258002741437, 0.8249550515182862]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "Epoch 1/150\n",
      "61987/61987 [==============================] - 4s 62us/step - loss: 0.5450 - acc: 0.9092\n",
      "Epoch 2/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.3678 - acc: 0.9433\n",
      "Epoch 3/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.2952 - acc: 0.9600\n",
      "Epoch 4/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.2611 - acc: 0.9660\n",
      "Epoch 5/150\n",
      "61987/61987 [==============================] - 3s 54us/step - loss: 0.2479 - acc: 0.9694\n",
      "Epoch 6/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.2298 - acc: 0.9718\n",
      "Epoch 7/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.2132 - acc: 0.9737\n",
      "Epoch 8/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.2106 - acc: 0.9752\n",
      "Epoch 9/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.2008 - acc: 0.9763\n",
      "Epoch 10/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1997 - acc: 0.9762\n",
      "Epoch 11/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1857 - acc: 0.9786\n",
      "Epoch 12/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1884 - acc: 0.9791\n",
      "Epoch 13/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1865 - acc: 0.9799\n",
      "Epoch 14/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1603 - acc: 0.9831\n",
      "Epoch 15/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1624 - acc: 0.9829\n",
      "Epoch 16/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1591 - acc: 0.9832\n",
      "Epoch 17/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1563 - acc: 0.9836\n",
      "Epoch 18/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1592 - acc: 0.9845\n",
      "Epoch 19/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1572 - acc: 0.9842\n",
      "Epoch 20/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1543 - acc: 0.9845\n",
      "Epoch 21/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1488 - acc: 0.9846\n",
      "Epoch 22/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1500 - acc: 0.9849\n",
      "Epoch 23/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1484 - acc: 0.9850\n",
      "Epoch 24/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1472 - acc: 0.9856\n",
      "Epoch 25/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1465 - acc: 0.9856\n",
      "Epoch 26/150\n",
      "61987/61987 [==============================] - 4s 61us/step - loss: 0.1451 - acc: 0.9858\n",
      "Epoch 27/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1435 - acc: 0.9858\n",
      "Epoch 28/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1334 - acc: 0.9871\n",
      "Epoch 29/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1279 - acc: 0.9873\n",
      "Epoch 30/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1327 - acc: 0.9873\n",
      "Epoch 31/150\n",
      "61987/61987 [==============================] - 4s 65us/step - loss: 0.1343 - acc: 0.9869\n",
      "Epoch 32/150\n",
      "61987/61987 [==============================] - 3s 54us/step - loss: 0.1317 - acc: 0.9875\n",
      "Epoch 33/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1283 - acc: 0.9874\n",
      "Epoch 34/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1287 - acc: 0.9870\n",
      "Epoch 35/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1268 - acc: 0.9875\n",
      "Epoch 36/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1278 - acc: 0.9874\n",
      "Epoch 37/150\n",
      "61987/61987 [==============================] - 4s 61us/step - loss: 0.1238 - acc: 0.9875\n",
      "Epoch 38/150\n",
      "61987/61987 [==============================] - 4s 62us/step - loss: 0.1258 - acc: 0.9874\n",
      "Epoch 39/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1265 - acc: 0.9872\n",
      "Epoch 40/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1267 - acc: 0.9878\n",
      "Epoch 41/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1267 - acc: 0.9877\n",
      "Epoch 42/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1189 - acc: 0.9886\n",
      "Epoch 43/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1168 - acc: 0.9886\n",
      "Epoch 44/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1177 - acc: 0.9886\n",
      "Epoch 45/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1162 - acc: 0.9889\n",
      "Epoch 46/150\n",
      "61987/61987 [==============================] - 4s 66us/step - loss: 0.1142 - acc: 0.9889\n",
      "Epoch 47/150\n",
      "61987/61987 [==============================] - 4s 69us/step - loss: 0.1141 - acc: 0.9891\n",
      "Epoch 48/150\n",
      "61987/61987 [==============================] - 4s 64us/step - loss: 0.1171 - acc: 0.9889\n",
      "Epoch 49/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1171 - acc: 0.9891\n",
      "Epoch 50/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1160 - acc: 0.9885\n",
      "Epoch 51/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1152 - acc: 0.9883\n",
      "Epoch 52/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1156 - acc: 0.9889\n",
      "Epoch 53/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1154 - acc: 0.9884\n",
      "Epoch 54/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1158 - acc: 0.9886\n",
      "Epoch 55/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1152 - acc: 0.9887\n",
      "Epoch 56/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1137 - acc: 0.9893\n",
      "Epoch 57/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1125 - acc: 0.9889\n",
      "Epoch 58/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1121 - acc: 0.9891\n",
      "Epoch 59/150\n",
      "61987/61987 [==============================] - 4s 62us/step - loss: 0.1133 - acc: 0.9888\n",
      "Epoch 60/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1138 - acc: 0.9891\n",
      "Epoch 61/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1135 - acc: 0.9889\n",
      "Epoch 62/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1137 - acc: 0.9893\n",
      "Epoch 63/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1142 - acc: 0.9892\n",
      "Epoch 64/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1155 - acc: 0.9887\n",
      "Epoch 65/150\n",
      "61987/61987 [==============================] - 4s 69us/step - loss: 0.1131 - acc: 0.9886\n",
      "Epoch 66/150\n",
      "61987/61987 [==============================] - 4s 71us/step - loss: 0.1142 - acc: 0.9890\n",
      "Epoch 67/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1148 - acc: 0.9888\n",
      "Epoch 68/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1142 - acc: 0.9889\n",
      "Epoch 69/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1140 - acc: 0.9890\n",
      "Epoch 70/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1120 - acc: 0.9896\n",
      "Epoch 71/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1113 - acc: 0.9897\n",
      "Epoch 72/150\n",
      "61987/61987 [==============================] - 4s 65us/step - loss: 0.1114 - acc: 0.9892\n",
      "Epoch 73/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1119 - acc: 0.9896\n",
      "Epoch 74/150\n",
      "61987/61987 [==============================] - 4s 64us/step - loss: 0.1106 - acc: 0.9894\n",
      "Epoch 75/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1112 - acc: 0.9894\n",
      "Epoch 76/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1106 - acc: 0.9897\n",
      "Epoch 77/150\n",
      "61987/61987 [==============================] - 4s 64us/step - loss: 0.1095 - acc: 0.9898\n",
      "Epoch 78/150\n",
      "61987/61987 [==============================] - 4s 62us/step - loss: 0.1101 - acc: 0.9895\n",
      "Epoch 79/150\n",
      "61987/61987 [==============================] - 4s 62us/step - loss: 0.1093 - acc: 0.9897\n",
      "Epoch 80/150\n",
      "61987/61987 [==============================] - 4s 67us/step - loss: 0.1090 - acc: 0.9895\n",
      "Epoch 81/150\n",
      "61987/61987 [==============================] - 4s 68us/step - loss: 0.1095 - acc: 0.9901\n",
      "Epoch 82/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1080 - acc: 0.9896\n",
      "Epoch 83/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1083 - acc: 0.9901\n",
      "Epoch 84/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1069 - acc: 0.9903\n",
      "Epoch 85/150\n",
      "61987/61987 [==============================] - 4s 73us/step - loss: 0.1058 - acc: 0.9898\n",
      "Epoch 86/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1065 - acc: 0.9900\n",
      "Epoch 87/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1063 - acc: 0.9904\n",
      "Epoch 88/150\n",
      "61987/61987 [==============================] - 4s 70us/step - loss: 0.1072 - acc: 0.9905\n",
      "Epoch 89/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1062 - acc: 0.9900\n",
      "Epoch 90/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1070 - acc: 0.9905\n",
      "Epoch 91/150\n",
      "61987/61987 [==============================] - 3s 55us/step - loss: 0.1068 - acc: 0.9904\n",
      "Epoch 92/150\n",
      "61987/61987 [==============================] - 4s 61us/step - loss: 0.1067 - acc: 0.9903\n",
      "Epoch 93/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1070 - acc: 0.9903\n",
      "Epoch 94/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1064 - acc: 0.9904\n",
      "Epoch 95/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1064 - acc: 0.9902\n",
      "Epoch 96/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1064 - acc: 0.9904\n",
      "Epoch 97/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1066 - acc: 0.9902\n",
      "Epoch 98/150\n",
      "61987/61987 [==============================] - 3s 51us/step - loss: 0.1061 - acc: 0.9906\n",
      "Epoch 99/150\n",
      "61987/61987 [==============================] - 3s 52us/step - loss: 0.1061 - acc: 0.9906\n",
      "Epoch 100/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1064 - acc: 0.9906\n",
      "Epoch 101/150\n",
      "61987/61987 [==============================] - 4s 65us/step - loss: 0.1057 - acc: 0.9904\n",
      "Epoch 102/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1063 - acc: 0.9905\n",
      "Epoch 103/150\n",
      "61987/61987 [==============================] - 3s 54us/step - loss: 0.1063 - acc: 0.9904\n",
      "Epoch 104/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1065 - acc: 0.9905\n",
      "Epoch 105/150\n",
      "61987/61987 [==============================] - 4s 61us/step - loss: 0.1066 - acc: 0.9905\n",
      "Epoch 106/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1064 - acc: 0.9904\n",
      "Epoch 107/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1063 - acc: 0.9904\n",
      "Epoch 108/150\n",
      "61987/61987 [==============================] - 4s 62us/step - loss: 0.1063 - acc: 0.9904\n",
      "Epoch 109/150\n",
      "61987/61987 [==============================] - 4s 68us/step - loss: 0.1067 - acc: 0.9905\n",
      "Epoch 110/150\n",
      "61987/61987 [==============================] - 4s 69us/step - loss: 0.1065 - acc: 0.9904\n",
      "Epoch 111/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1064 - acc: 0.9905\n",
      "Epoch 112/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1062 - acc: 0.9906\n",
      "Epoch 113/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1068 - acc: 0.9908\n",
      "Epoch 114/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1054 - acc: 0.9904\n",
      "Epoch 115/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1061 - acc: 0.9904\n",
      "Epoch 116/150\n",
      "61987/61987 [==============================] - 4s 65us/step - loss: 0.1074 - acc: 0.9909\n",
      "Epoch 117/150\n",
      "61987/61987 [==============================] - 4s 62us/step - loss: 0.1066 - acc: 0.9905\n",
      "Epoch 118/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1062 - acc: 0.9903\n",
      "Epoch 119/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1061 - acc: 0.9904\n",
      "Epoch 120/150\n",
      "61987/61987 [==============================] - 4s 65us/step - loss: 0.1072 - acc: 0.9908\n",
      "Epoch 121/150\n",
      "61987/61987 [==============================] - 4s 65us/step - loss: 0.1068 - acc: 0.9906\n",
      "Epoch 122/150\n",
      "61987/61987 [==============================] - 4s 61us/step - loss: 0.1068 - acc: 0.9907\n",
      "Epoch 123/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1068 - acc: 0.9907\n",
      "Epoch 124/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1069 - acc: 0.9907\n",
      "Epoch 125/150\n",
      "61987/61987 [==============================] - 4s 65us/step - loss: 0.1068 - acc: 0.9907\n",
      "Epoch 126/150\n",
      "61987/61987 [==============================] - 4s 61us/step - loss: 0.1062 - acc: 0.9905\n",
      "Epoch 127/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1069 - acc: 0.9908\n",
      "Epoch 128/150\n",
      "61987/61987 [==============================] - 4s 68us/step - loss: 0.1060 - acc: 0.9904\n",
      "Epoch 129/150\n",
      "61987/61987 [==============================] - 4s 63us/step - loss: 0.1066 - acc: 0.9906\n",
      "Epoch 130/150\n",
      "61987/61987 [==============================] - 4s 67us/step - loss: 0.1068 - acc: 0.9906\n",
      "Epoch 131/150\n",
      "61987/61987 [==============================] - 4s 68us/step - loss: 0.1067 - acc: 0.9905\n",
      "Epoch 132/150\n",
      "61987/61987 [==============================] - 4s 62us/step - loss: 0.1069 - acc: 0.9905\n",
      "Epoch 133/150\n",
      "61987/61987 [==============================] - 4s 61us/step - loss: 0.1069 - acc: 0.9905\n",
      "Epoch 134/150\n",
      "61987/61987 [==============================] - 4s 57us/step - loss: 0.1067 - acc: 0.9905\n",
      "Epoch 135/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1069 - acc: 0.9906\n",
      "Epoch 136/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1069 - acc: 0.9905\n",
      "Epoch 137/150\n",
      "61987/61987 [==============================] - 3s 56us/step - loss: 0.1066 - acc: 0.9904\n",
      "Epoch 138/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1066 - acc: 0.9904\n",
      "Epoch 139/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1072 - acc: 0.9906\n",
      "Epoch 140/150\n",
      "61987/61987 [==============================] - ETA: 0s - loss: 0.1066 - acc: 0.990 - 4s 59us/step - loss: 0.1066 - acc: 0.9904\n",
      "Epoch 141/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1069 - acc: 0.9906\n",
      "Epoch 142/150\n",
      "61987/61987 [==============================] - 4s 69us/step - loss: 0.1069 - acc: 0.9905\n",
      "Epoch 143/150\n",
      "61987/61987 [==============================] - 4s 61us/step - loss: 0.1064 - acc: 0.9905\n",
      "Epoch 144/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1077 - acc: 0.9908\n",
      "Epoch 145/150\n",
      "61987/61987 [==============================] - 4s 59us/step - loss: 0.1066 - acc: 0.9905\n",
      "Epoch 146/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1065 - acc: 0.9905\n",
      "Epoch 147/150\n",
      "61987/61987 [==============================] - 4s 58us/step - loss: 0.1072 - acc: 0.9906\n",
      "Epoch 148/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1067 - acc: 0.9905\n",
      "Epoch 149/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1070 - acc: 0.9906\n",
      "Epoch 150/150\n",
      "61987/61987 [==============================] - 4s 60us/step - loss: 0.1070 - acc: 0.9906\n",
      "[[33060  1074]\n",
      " [  459  4064]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     34134\n",
      "           1       0.79      0.90      0.84      4523\n",
      "\n",
      "    accuracy                           0.96     38657\n",
      "   macro avg       0.89      0.93      0.91     38657\n",
      "weighted avg       0.96      0.96      0.96     38657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, Adamax, Nadam\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "from LossHistory import *\n",
    "import time\n",
    "\n",
    "\n",
    "bin_labels = y_train.copy()\n",
    "bin_labels[y_train != 0]=1\n",
    "bin_labels[y_train == 0]=0\n",
    "bin_labels=np.array(bin_labels, dtype=int)\n",
    "\n",
    "bin_labels_test = y_test.copy()\n",
    "bin_labels_test[y_test != 0]=1\n",
    "bin_labels_test[y_test == 0]=0\n",
    "bin_labels_test=np.array(bin_labels_test, dtype=int)\n",
    "\n",
    "class_weights = {}\n",
    "for c in range(0,2):\n",
    "    class_weights.update({c:len(bin_labels) / float(np.count_nonzero(bin_labels == c))})\n",
    "    \n",
    "input_size=X_train.shape[1]\n",
    "# Establish the NN\n",
    "eps=150\n",
    "opt=RMSprop(lr=0.001)\n",
    "print (input_size)\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "model =  Sequential()\n",
    "model.add(Dense(input_size, activation='sigmoid', input_dim=input_size))\n",
    "# hidden layers.\n",
    "model.add(Dense(50, activation='sigmoid'))\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "\n",
    "# output layer.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# overall structure.\n",
    "model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "loss_history=LossHistory()\n",
    "cb=[loss_history,LearningRateScheduler(step_decay)]\n",
    "\n",
    "# train neural network.\n",
    "history=model.fit(X_train, bin_labels, class_weight=class_weights, epochs=eps, callbacks=cb)\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred=y_pred.astype(int)\n",
    "y_pred=y_pred.ravel()\n",
    "#predictions.append(y_pred)\n",
    "\n",
    "print(confusion_matrix(bin_labels_test,y_pred))  \n",
    "print(classification_report(bin_labels_test,y_pred))\n",
    "#print(get_metrics(predic, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_clinic_2 = np.hstack((np_variables_2,np_V1_2, dtw_clinic_2))\n",
    "\n",
    "norm_var_1_clinic = post_features.normalised_values_multiples(np_clinic_1)\n",
    "norm_var_2_clinic = post_features.normalised_values_multiples(np_clinic_2)\n",
    "\n",
    "norm_var_1_non = post_features.normalised_values_multiples(np_non_var_1)\n",
    "norm_var_2_non = post_features.normalised_values_multiples(np_non_var_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_clinic_1_class = np.hstack((np_class_ID_1,np_clinic_1))\n",
    "#np_clinic_2_class = np.hstack((np_class_ID_2,np_clinic_2))\n",
    "\n",
    "np_clinic_1_class = np.hstack((np_class_ID_1,np_clinic_1,norm_var_1_clinic))\n",
    "np_clinic_2_class = np.hstack((np_class_ID_2,np_clinic_2,norm_var_2_clinic))\n",
    "\n",
    "#np_clinic_1_class = np.hstack((np_class_ID_1, np_non_var_1))\n",
    "#np_clinic_2_class = np.hstack((np_class_ID_2, np_non_var_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "norm_dtw = [\"dtw1_v1\",\"dtw2_v1\"]\n",
    "classID = [\"classID\"]\n",
    "non_clinic = list(DB1_non_cli.iloc[:,0:140].columns.values)\n",
    "\n",
    "feature_norm_mill = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_mill.append(str(feat_labels[i]+\"_n_MLII\"))\n",
    "\n",
    "feature_norm_V1 = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_V1.append(str(feat_labels[i]+\"_n_V1\"))\n",
    "    \n",
    "\n",
    "c_ID = np.asarray(classID)\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "non_cli = np.asarray(non_clinic)\n",
    "norm_mlii = np.asarray(feature_norm_mill)\n",
    "norm_v1 = np.asarray(feature_norm_V1)\n",
    "norm_dtw = np.asarray(norm_dtw)\n",
    "\n",
    "\n",
    "#features_clinic = np.hstack((c_ID,non_cli))#f_M,f_V, f_d,non_cli,norm_mlii, norm_v1,norm_dtw))\n",
    "features_clinic = np.hstack((c_ID,f_M,f_V, f_d,norm_mlii, norm_v1,norm_dtw))\n",
    "\n",
    "row=[]\n",
    "for i in range(0,len(np_clinic_1_class)):\n",
    "    row.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np_clinic_1_class,columns=features_clinic)\n",
    "df2 = pd.DataFrame(np_clinic_2_class,columns=features_clinic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymrmr as mrmr\n",
    "\n",
    "rank=mrmr.mRMR(df, 'MID', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dtw1',\n",
       " 'R_duration_n_MLII',\n",
       " 'PQ_int_n_V1',\n",
       " 'T_amp8',\n",
       " 'dtw2_v1',\n",
       " 'R_duration_n_V1',\n",
       " 'P_amp1_n_MLII',\n",
       " 'P_duration_n_V1',\n",
       " 'T_neg_amp6',\n",
       " 'neg_RQ_n_MLII']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_clinic_1 = df[rank]\n",
    "np_clinic_2 = df2[rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot broadcast operands together.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1466\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'float'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ae390d50dcb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreject_outliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDB1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-524d8c4f6129>\u001b[0m in \u001b[0;36mreject_outliers\u001b[1;34m(data, m)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreject_outliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[0;32m   1522\u001b[0m             return _combine_series_frame(self, other, na_op,\n\u001b[0;32m   1523\u001b[0m                                          \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m                                          level=level, try_cast=True)\n\u001b[0m\u001b[0;32m   1525\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36m_combine_series_frame\u001b[1;34m(self, other, func, fill_value, axis, level, try_cast)\u001b[0m\n\u001b[0;32m   1407\u001b[0m         \u001b[1;31m# default axis is columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         return self._combine_match_columns(other, func, level=level,\n\u001b[1;32m-> 1409\u001b[1;33m                                            try_cast=try_cast)\n\u001b[0m\u001b[0;32m   1410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_combine_match_columns\u001b[1;34m(self, other, func, level, try_cast)\u001b[0m\n\u001b[0;32m   4768\u001b[0m         new_data = left._data.eval(func=func, other=right,\n\u001b[0;32m   4769\u001b[0m                                    \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4770\u001b[1;33m                                    try_cast=try_cast)\n\u001b[0m\u001b[0;32m   4771\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   3685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3686\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3687\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3581\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, func, other, errors, try_cast, mgr)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m         \u001b[1;31m# if we have an invalid shape/broadcast error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(other)\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m             \u001b[1;31m# mask if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mna_op\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1479\u001b[0m                     \u001b[1;31m# Without specifically raising here we get mismatched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m                     \u001b[1;31m# errors in Py3 (TypeError) vs Py2 (ValueError)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1481\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot broadcast operands together.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m                 \u001b[0myrav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myrav\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot broadcast operands together."
     ]
    }
   ],
   "source": [
    "new = reject_outliers(DB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def reject_outliers(data, m=4):\n",
    "    return data[abs(data - np.mean(data)) < m * np.std(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48683"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n",
      "done norm\n",
      "row 140 done\n",
      "done norm\n",
      "row 141 done\n",
      "done norm\n",
      "row 142 done\n",
      "done norm\n",
      "row 143 done\n",
      "done norm\n",
      "row 144 done\n",
      "done norm\n",
      "row 145 done\n",
      "done norm\n",
      "row 146 done\n",
      "done norm\n",
      "row 147 done\n",
      "done norm\n",
      "row 148 done\n",
      "done norm\n",
      "row 149 done\n",
      "done norm\n",
      "row 150 done\n",
      "done norm\n",
      "row 151 done\n",
      "done norm\n",
      "row 152 done\n",
      "done norm\n",
      "row 153 done\n",
      "done norm\n",
      "row 154 done\n",
      "done norm\n",
      "row 155 done\n",
      "done norm\n",
      "row 156 done\n",
      "done norm\n",
      "row 157 done\n",
      "done norm\n",
      "row 158 done\n",
      "done norm\n",
      "row 159 done\n",
      "done norm\n",
      "row 160 done\n",
      "done norm\n",
      "row 161 done\n",
      "done norm\n",
      "row 162 done\n",
      "done norm\n",
      "row 163 done\n",
      "done norm\n",
      "row 164 done\n",
      "done norm\n",
      "row 165 done\n",
      "done norm\n",
      "row 166 done\n",
      "done norm\n",
      "row 167 done\n",
      "done norm\n",
      "row 168 done\n",
      "done norm\n",
      "row 169 done\n",
      "done norm\n",
      "row 170 done\n",
      "done norm\n",
      "row 171 done\n",
      "done norm\n",
      "row 172 done\n",
      "done norm\n",
      "row 173 done\n",
      "done norm\n",
      "row 174 done\n",
      "done norm\n",
      "row 175 done\n",
      "done norm\n",
      "row 176 done\n",
      "done norm\n",
      "row 177 done\n",
      "done norm\n",
      "row 178 done\n",
      "done norm\n",
      "row 179 done\n",
      "done norm\n",
      "row 180 done\n",
      "done norm\n",
      "row 181 done\n",
      "done norm\n",
      "row 182 done\n",
      "done norm\n",
      "row 183 done\n",
      "done norm\n",
      "row 184 done\n",
      "done norm\n",
      "row 185 done\n",
      "done norm\n",
      "row 186 done\n",
      "done norm\n",
      "row 187 done\n",
      "done norm\n",
      "row 188 done\n",
      "done norm\n",
      "row 189 done\n",
      "done norm\n",
      "row 190 done\n",
      "done norm\n",
      "row 191 done\n",
      "done norm\n",
      "row 192 done\n",
      "done norm\n",
      "row 193 done\n",
      "done norm\n",
      "row 194 done\n",
      "done norm\n",
      "row 195 done\n",
      "done norm\n",
      "row 196 done\n",
      "done norm\n",
      "row 197 done\n",
      "done norm\n",
      "row 198 done\n",
      "done norm\n",
      "row 199 done\n",
      "done norm\n",
      "row 200 done\n",
      "done norm\n",
      "row 201 done\n",
      "done norm\n",
      "row 202 done\n",
      "done norm\n",
      "row 203 done\n",
      "done norm\n",
      "row 204 done\n",
      "done norm\n",
      "row 205 done\n",
      "done norm\n",
      "row 206 done\n",
      "done norm\n",
      "row 207 done\n",
      "done norm\n",
      "row 208 done\n",
      "done norm\n",
      "row 209 done\n",
      "done norm\n",
      "row 210 done\n",
      "done norm\n",
      "row 211 done\n",
      "done norm\n",
      "row 212 done\n",
      "done norm\n",
      "row 213 done\n",
      "done norm\n",
      "row 214 done\n",
      "done norm\n",
      "row 215 done\n",
      "done norm\n",
      "row 216 done\n",
      "done norm\n",
      "row 217 done\n",
      "done norm\n",
      "row 218 done\n",
      "done norm\n",
      "row 219 done\n",
      "done norm\n",
      "row 220 done\n",
      "done norm\n",
      "row 221 done\n",
      "done norm\n",
      "row 222 done\n",
      "done norm\n",
      "row 223 done\n",
      "done norm\n",
      "row 224 done\n",
      "done norm\n",
      "row 225 done\n",
      "done norm\n",
      "row 226 done\n",
      "done norm\n",
      "row 227 done\n",
      "done norm\n",
      "row 228 done\n",
      "done norm\n",
      "row 229 done\n",
      "done norm\n",
      "row 230 done\n",
      "done norm\n",
      "row 231 done\n",
      "done norm\n",
      "row 232 done\n",
      "done norm\n",
      "row 233 done\n",
      "done norm\n",
      "row 234 done\n",
      "done norm\n",
      "row 235 done\n",
      "done norm\n",
      "row 236 done\n",
      "done norm\n",
      "row 237 done\n",
      "done norm\n",
      "row 238 done\n",
      "done norm\n",
      "row 239 done\n",
      "done norm\n",
      "row 240 done\n",
      "done norm\n",
      "row 241 done\n",
      "done norm\n",
      "row 242 done\n",
      "done norm\n",
      "row 243 done\n",
      "done norm\n",
      "row 244 done\n",
      "done norm\n",
      "row 245 done\n",
      "done norm\n",
      "row 246 done\n",
      "done norm\n",
      "row 247 done\n",
      "done norm\n",
      "row 248 done\n",
      "done norm\n",
      "row 249 done\n",
      "done norm\n",
      "row 250 done\n",
      "done norm\n",
      "row 251 done\n",
      "done norm\n",
      "row 252 done\n",
      "done norm\n",
      "row 253 done\n",
      "done norm\n",
      "row 254 done\n",
      "done norm\n",
      "row 255 done\n",
      "done norm\n",
      "row 256 done\n",
      "done norm\n",
      "row 257 done\n",
      "done norm\n",
      "row 258 done\n",
      "done norm\n",
      "row 259 done\n",
      "done norm\n",
      "row 260 done\n",
      "done norm\n",
      "row 261 done\n",
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n",
      "done norm\n",
      "row 140 done\n",
      "done norm\n",
      "row 141 done\n",
      "done norm\n",
      "row 142 done\n",
      "done norm\n",
      "row 143 done\n",
      "done norm\n",
      "row 144 done\n",
      "done norm\n",
      "row 145 done\n",
      "done norm\n",
      "row 146 done\n",
      "done norm\n",
      "row 147 done\n",
      "done norm\n",
      "row 148 done\n",
      "done norm\n",
      "row 149 done\n",
      "done norm\n",
      "row 150 done\n",
      "done norm\n",
      "row 151 done\n",
      "done norm\n",
      "row 152 done\n",
      "done norm\n",
      "row 153 done\n",
      "done norm\n",
      "row 154 done\n",
      "done norm\n",
      "row 155 done\n",
      "done norm\n",
      "row 156 done\n",
      "done norm\n",
      "row 157 done\n",
      "done norm\n",
      "row 158 done\n",
      "done norm\n",
      "row 159 done\n",
      "done norm\n",
      "row 160 done\n",
      "done norm\n",
      "row 161 done\n",
      "done norm\n",
      "row 162 done\n",
      "done norm\n",
      "row 163 done\n",
      "done norm\n",
      "row 164 done\n",
      "done norm\n",
      "row 165 done\n",
      "done norm\n",
      "row 166 done\n",
      "done norm\n",
      "row 167 done\n",
      "done norm\n",
      "row 168 done\n",
      "done norm\n",
      "row 169 done\n",
      "done norm\n",
      "row 170 done\n",
      "done norm\n",
      "row 171 done\n",
      "done norm\n",
      "row 172 done\n",
      "done norm\n",
      "row 173 done\n",
      "done norm\n",
      "row 174 done\n",
      "done norm\n",
      "row 175 done\n",
      "done norm\n",
      "row 176 done\n",
      "done norm\n",
      "row 177 done\n",
      "done norm\n",
      "row 178 done\n",
      "done norm\n",
      "row 179 done\n",
      "done norm\n",
      "row 180 done\n",
      "done norm\n",
      "row 181 done\n",
      "done norm\n",
      "row 182 done\n",
      "done norm\n",
      "row 183 done\n",
      "done norm\n",
      "row 184 done\n",
      "done norm\n",
      "row 185 done\n",
      "done norm\n",
      "row 186 done\n",
      "done norm\n",
      "row 187 done\n",
      "done norm\n",
      "row 188 done\n",
      "done norm\n",
      "row 189 done\n",
      "done norm\n",
      "row 190 done\n",
      "done norm\n",
      "row 191 done\n",
      "done norm\n",
      "row 192 done\n",
      "done norm\n",
      "row 193 done\n",
      "done norm\n",
      "row 194 done\n",
      "done norm\n",
      "row 195 done\n",
      "done norm\n",
      "row 196 done\n",
      "done norm\n",
      "row 197 done\n",
      "done norm\n",
      "row 198 done\n",
      "done norm\n",
      "row 199 done\n",
      "done norm\n",
      "row 200 done\n",
      "done norm\n",
      "row 201 done\n",
      "done norm\n",
      "row 202 done\n",
      "done norm\n",
      "row 203 done\n",
      "done norm\n",
      "row 204 done\n",
      "done norm\n",
      "row 205 done\n",
      "done norm\n",
      "row 206 done\n",
      "done norm\n",
      "row 207 done\n",
      "done norm\n",
      "row 208 done\n",
      "done norm\n",
      "row 209 done\n",
      "done norm\n",
      "row 210 done\n",
      "done norm\n",
      "row 211 done\n",
      "done norm\n",
      "row 212 done\n",
      "done norm\n",
      "row 213 done\n",
      "done norm\n",
      "row 214 done\n",
      "done norm\n",
      "row 215 done\n",
      "done norm\n",
      "row 216 done\n",
      "done norm\n",
      "row 217 done\n",
      "done norm\n",
      "row 218 done\n",
      "done norm\n",
      "row 219 done\n",
      "done norm\n",
      "row 220 done\n",
      "done norm\n",
      "row 221 done\n",
      "done norm\n",
      "row 222 done\n",
      "done norm\n",
      "row 223 done\n",
      "done norm\n",
      "row 224 done\n",
      "done norm\n",
      "row 225 done\n",
      "done norm\n",
      "row 226 done\n",
      "done norm\n",
      "row 227 done\n",
      "done norm\n",
      "row 228 done\n",
      "done norm\n",
      "row 229 done\n",
      "done norm\n",
      "row 230 done\n",
      "done norm\n",
      "row 231 done\n",
      "done norm\n",
      "row 232 done\n",
      "done norm\n",
      "row 233 done\n",
      "done norm\n",
      "row 234 done\n",
      "done norm\n",
      "row 235 done\n",
      "done norm\n",
      "row 236 done\n",
      "done norm\n",
      "row 237 done\n",
      "done norm\n",
      "row 238 done\n",
      "done norm\n",
      "row 239 done\n",
      "done norm\n",
      "row 240 done\n",
      "done norm\n",
      "row 241 done\n",
      "done norm\n",
      "row 242 done\n",
      "done norm\n",
      "row 243 done\n",
      "done norm\n",
      "row 244 done\n",
      "done norm\n",
      "row 245 done\n",
      "done norm\n",
      "row 246 done\n",
      "done norm\n",
      "row 247 done\n",
      "done norm\n",
      "row 248 done\n",
      "done norm\n",
      "row 249 done\n",
      "done norm\n",
      "row 250 done\n",
      "done norm\n",
      "row 251 done\n",
      "done norm\n",
      "row 252 done\n",
      "done norm\n",
      "row 253 done\n",
      "done norm\n",
      "row 254 done\n",
      "done norm\n",
      "row 255 done\n",
      "done norm\n",
      "row 256 done\n",
      "done norm\n",
      "row 257 done\n",
      "done norm\n",
      "row 258 done\n",
      "done norm\n",
      "row 259 done\n",
      "done norm\n",
      "row 260 done\n",
      "done norm\n",
      "row 261 done\n",
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n",
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
