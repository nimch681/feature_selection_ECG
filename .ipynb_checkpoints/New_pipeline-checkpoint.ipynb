{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n"
     ]
    }
   ],
   "source": [
    "np_clinic_1, np_clinic_2,np_non_var_1, np_non_var_2, np_class_ID_1, np_class_ID_2 = DF.get_all_dataframe_patient_specific(500,patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1 = [int(i) for i in np_class_ID_1]\n",
    "np_class_ID_2 = [int(i) for i in np_class_ID_2]\n",
    "X_train_new = np_clinic_1\n",
    "X_test_new = np_clinic_2\n",
    "y_train_new = np.asarray(np_class_ID_1)\n",
    "y_test_new = np.asarray(np_class_ID_2)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "train, train_labels=ada.fit_sample(X_train_new, y_train_new)\n",
    "test, test_labels=ada.fit_sample(X_test_new, y_test_new)\n",
    "\n",
    "X_train_new = train\n",
    "#X_test = test\n",
    "y_train_new = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features_X = np.asarray(pd.read_csv(\"database/good_X_train.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/good_y_train.csv\").iloc[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45809 976 3788 414\n",
      "45809 976 3788 414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45809 976 3788 414\n",
      "41228 878 3409 372\n",
      "41228 878 3409 372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "old_beats_train = y_train\n",
    "n_beat = X_train[y_train==0]\n",
    "s_beat = X_train[y_train==1]\n",
    "v_beat = X_train[y_train==2]\n",
    "f_beat = X_train[y_train==3]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = y_train[y_train==0]\n",
    "s_beat_y = y_train[y_train==1]\n",
    "v_beat_y = y_train[y_train==2]\n",
    "f_beat_y = y_train[y_train==3]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "outdetect = IsolationForest(max_features = input_size)\n",
    "\n",
    "outliers_n = outdetect.fit_predict(n_beat)\n",
    "outliers_s = outdetect.fit_predict(s_beat)\n",
    "outliers_v = outdetect.fit_predict(v_beat)\n",
    "outliers_f = outdetect.fit_predict(f_beat)\n",
    "\n",
    "print(len(outliers_n), len(outliers_s), len(outliers_v),len(outliers_f) )\n",
    "\n",
    "n_beat = n_beat[outliers_n==1]\n",
    "s_beat = s_beat[outliers_s==1]\n",
    "v_beat = v_beat[outliers_v==1]\n",
    "f_beat = f_beat[outliers_f==1]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = n_beat_y[outliers_n==1]\n",
    "s_beat_y = s_beat_y[outliers_s==1]\n",
    "v_beat_y = v_beat_y[outliers_v==1]\n",
    "f_beat_y = f_beat_y[outliers_f==1]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "n_beat_y = n_beat_y.reshape(n_beat_y.shape[0],1)\n",
    "s_beat_y = s_beat_y.reshape(s_beat_y.shape[0],1)\n",
    "v_beat_y = v_beat_y.reshape(v_beat_y.shape[0],1)\n",
    "f_beat_y = f_beat_y.reshape(f_beat_y.shape[0],1)\n",
    "\n",
    "\n",
    "X_train = np.vstack((n_beat,s_beat, v_beat, f_beat))\n",
    "y_train = np.vstack((n_beat_y,s_beat_y, v_beat_y, f_beat_y))\n",
    "\n",
    "\n",
    "#outlers_test = outdetect.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "old_beats_train = y_test\n",
    "n_beat = X_test[y_test==0]\n",
    "s_beat = X_test[y_test==1]\n",
    "v_beat = X_test[y_test==2]\n",
    "f_beat = X_test[y_test==3]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = y_test[y_test==0]\n",
    "s_beat_y = y_test[y_test==1]\n",
    "v_beat_y = y_test[y_test==2]\n",
    "f_beat_y = y_test[y_test==3]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "outdetect = IsolationForest(max_features = input_size)\n",
    "\n",
    "outliers_n = outdetect.fit_predict(n_beat)\n",
    "outliers_s = outdetect.fit_predict(s_beat)\n",
    "outliers_v = outdetect.fit_predict(v_beat)\n",
    "outliers_f = outdetect.fit_predict(f_beat)\n",
    "\n",
    "print(len(outliers_n), len(outliers_s), len(outliers_v),len(outliers_f) )\n",
    "\n",
    "n_beat = n_beat[outliers_n==1]\n",
    "s_beat = s_beat[outliers_s==1]\n",
    "v_beat = v_beat[outliers_v==1]\n",
    "f_beat = f_beat[outliers_f==1]\n",
    "\n",
    "print(len(n_beat), len(s_beat), len(v_beat),len(f_beat) )\n",
    "\n",
    "n_beat_y = n_beat_y[outliers_n==1]\n",
    "s_beat_y = s_beat_y[outliers_s==1]\n",
    "v_beat_y = v_beat_y[outliers_v==1]\n",
    "f_beat_y = f_beat_y[outliers_f==1]\n",
    "\n",
    "print(len(n_beat_y), len(s_beat_y), len(v_beat_y),len(f_beat_y) )\n",
    "\n",
    "n_beat_y = n_beat_y.reshape(n_beat_y.shape[0],1)\n",
    "s_beat_y = s_beat_y.reshape(s_beat_y.shape[0],1)\n",
    "v_beat_y = v_beat_y.reshape(v_beat_y.shape[0],1)\n",
    "f_beat_y = f_beat_y.reshape(f_beat_y.shape[0],1)\n",
    "\n",
    "\n",
    "X_test = np.vstack((n_beat,s_beat, v_beat, f_beat))\n",
    "y_test = np.vstack((n_beat_y,s_beat_y, v_beat_y, f_beat_y))\n",
    "\n",
    "\n",
    "#outlers_test = outdetect.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gooddata_X_train= X_train\n",
    "gooddata_y_train = y_train \n",
    "#gooddata_X_test = X_test\n",
    "#gooddata_y_test = y_test\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(gooddata_X_train,columns=features_clinic)\n",
    "df2 = pd.DataFrame(gooddata_y_train)\n",
    "df.to_csv('database/gooddata_X_train_no_outliers.csv')\n",
    "df2.to_csv('database/gooddata_y_train_no_outliers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=gooddata_X_train\n",
    "y_train=gooddata_y_train \n",
    "X_test=gooddata_X_test \n",
    "y_test=gooddata_y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37945   984  3305  1787]\n",
      " [ 1399   471   105    75]\n",
      " [  233   221  1831   935]\n",
      " [   47     2     5   334]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91     44021\n",
      "           1       0.28      0.23      0.25      2050\n",
      "           2       0.35      0.57      0.43      3220\n",
      "           3       0.11      0.86      0.19       388\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     49679\n",
      "   macro avg       0.42      0.63      0.45     49679\n",
      "weighted avg       0.88      0.82      0.84     49679\n",
      "\n",
      "[0.35630862598200463, 1.4281087678274935, 0.356667908969439]\n"
     ]
    }
   ],
   "source": [
    "predictions_svm_poly = svm_model_poly(X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## small C from 1 to 3 seems to be good for f class recalled\n",
    "## degree 3 is better all around\n",
    "## Tol at 0.01 seems best\n",
    "\n",
    "def svm_model_poly(X_train, y_train, X_test, y_test, C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                        coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3]):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test,lb=labels))\n",
    "    \n",
    "    return svm_model_linear,y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def Linear_D(X_train, y_train, X_test, y_test, labels = [0,1,2,3]):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred, labels = labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return clf, metric.get_metrics(y_pred,y_test,lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "## numner of estimators leaves at 1000 \n",
    "\n",
    "def randomForest(X_train, y_train, X_test, y_test,n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, max_features=50, max_leaf_nodes=None, class_weight='balanced', labels = [0,1,2,3]):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,min_weight_fraction_leaf=min_weight_fraction_leaf, class_weight=class_weight, min_impurity_decrease=min_impurity_decrease,max_features=max_features)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return rf, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### xgbo is good for normal classes too\n",
    "## do this one next \n",
    "## XGB is very goos for V class\n",
    "## XGboost tree classifcation is good with 5 or more tree depth\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def xgboost(X_train,y_train, X_test, y_test, max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, process_type='default', predictor= 'cpu_predictor', labels = [0,1,2,3]): \n",
    "    model = XGBClassifier(max_depth = max_depth, eta = eta, gamma = gamma,min_child_weight=min_child_weight,subsample=subsample, max_delta_step= max_delta_step,colsample_bytree=colsample_bytree,colsample_bylevel=colsample_bylevel,colsample_bynode=colsample_bynode,alpha=alpha,reg_lambda=reg_lambda,grow_policy=grow_policy, tree_method=tree_method,refresh_leaf=refresh_leaf,process_type=process_type, predictor= predictor,objective = 'reg:logistic')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return model, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "## L1 one is good , l2 would be used\n",
    "## very good at f class recall\n",
    "## solver = ['newton-cg', 'lbfgs', 'liblinear'are all good, ensemble with all would be good\n",
    "\n",
    "## false dual is better\n",
    "def logisticRegress(X_train, y_train, X_test, y_test, penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, labels = [0,1,2,3]):\n",
    "    lr = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        # dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "    lr.fit(X_train, y_train.ravel())  \n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return lr, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def ada(X_train, y_train, X_test, y_test,labels = [0,1,2,3]):\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                            coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    ada = AdaBoostClassifier(base_estimator=svm_model_poly, n_estimators=50 )\n",
    "    ada.fit(X_train, y_train)  \n",
    "    y_pred = ada.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return ada, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "\n",
    "\n",
    "def svm_model_linear(X_train, y_train, X_test, y_test, C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3] ):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return svm_model_linear, metric.get_metrics(y_pred,y_test, lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr, HOS, myMorph training features\n",
    "train_rr=np.loadtxt('rr-train.txt', dtype=float)[:,1:] # extract class labels\n",
    "train_hos=np.loadtxt('hos_train.txt', dtype=float)\n",
    "train_mmorph=np.loadtxt('morph_train.txt', dtype=float)\n",
    "\n",
    "# rr, HOS, myMorph testing features\n",
    "test_rr=np.loadtxt('rr-test.txt', dtype=float)[:,1:] # extract class labels\n",
    "test_rr=np.loadtxt('rr-test.txt', dtype=float)[:,1:] # extract class labels\n",
    "test_hos=np.loadtxt('hos_test.txt', dtype=float)\n",
    "test_mmorph=np.loadtxt('morph_test.txt', dtype=float)\n",
    "\n",
    "# training and testing class labels.\n",
    "train_labels=np.loadtxt('labels_train.txt', dtype=str)\n",
    "test_labels=np.loadtxt('labels_test.txt', dtype=str)\n",
    "\n",
    "# aggregate all features.\n",
    "train=np.hstack((train_rr, train_hos, train_mmorph))\n",
    "test=np.hstack((test_rr, test_hos, test_mmorph))\n",
    "\n",
    "#train=np.hstack((train_rr, train_mmorph))\n",
    "#test=np.hstack((test_rr, test_mmorph))\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "train, train_labels=ada.fit_sample(train, train_labels)\n",
    "#test, test_labels=ada.fit_sample(test_rr, test_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ada=RandomUnderSampler(sampling_strategy='all', replacement=True)\n",
    "#new_train, new_train_labels=ada.fit_sample(df[rank], y_train)\n",
    "#new_x_test, new_y_test=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "\n",
    "#input_size=X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n",
      "[[34233  2674  2705  4409]\n",
      " [ 1050   475   518     7]\n",
      " [   54    84  3035    47]\n",
      " [   23     2    56   307]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.86     44021\n",
      "           1       0.15      0.23      0.18      2050\n",
      "           2       0.48      0.94      0.64      3220\n",
      "           3       0.06      0.79      0.12       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.41      0.69      0.45     49679\n",
      "weighted avg       0.90      0.77      0.81     49679\n",
      "\n",
      "[0.34544235163919657, 1.8017632897898452, 0.39794158704332894]\n"
     ]
    }
   ],
   "source": [
    "score = 0.01\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "xgboost(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y, X_test_df[df[df['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "logisticRegress(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y, X_test_df[df[df['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "Linear_D(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y, X_test_df[df[df['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "svm_model_linear(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y, X_test_df[df[df['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "\n",
    "#randomForest(train, train_labels, test, test_labels, max_depth=5, labels=['N','S','V','F'])\n",
    "#rf = randomForest(X_train_new, y_train_new ,X_test_new , y_test_new, labels=[0,1,2,3])[0]\n",
    "#Linear_D(np_non_var_1_old, y_train, np_non_var_2_old, y_test, labels=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02742642, 0.00614345, 0.00278523, 0.0031744 , 0.00362213,\n",
       "       0.00636304, 0.01103273, 0.01427674, 0.015113  , 0.02031046,\n",
       "       0.00772171, 0.01405601, 0.00715583, 0.0081402 , 0.00460254,\n",
       "       0.00837216, 0.00187325, 0.00161843, 0.00128076, 0.00114845,\n",
       "       0.0011973 , 0.00136571, 0.00128343, 0.00084806, 0.00085736,\n",
       "       0.00115603, 0.00844444, 0.000889  , 0.00317239, 0.00490351,\n",
       "       0.0066315 , 0.00264355, 0.00445453, 0.00368135, 0.00233483,\n",
       "       0.00162115, 0.00157329, 0.00145167, 0.00194029, 0.00248445,\n",
       "       0.00524868, 0.00134086, 0.00389971, 0.00788223, 0.00247649,\n",
       "       0.00290898, 0.00193526, 0.00156815, 0.00131302, 0.00145449,\n",
       "       0.00130368, 0.00136009, 0.00201549, 0.001914  , 0.00624248,\n",
       "       0.00276328, 0.00075117, 0.00090276, 0.00105916, 0.00092024,\n",
       "       0.00095551, 0.00109185, 0.00101378, 0.00104912, 0.00096675,\n",
       "       0.00089062, 0.00112281, 0.00117557, 0.0011318 , 0.0002591 ,\n",
       "       0.00129505, 0.00244649, 0.00463905, 0.00616878, 0.00642525,\n",
       "       0.00483532, 0.00363492, 0.00390421, 0.00385207, 0.00347163,\n",
       "       0.00570651, 0.00802265, 0.00321767, 0.0029651 , 0.00096593,\n",
       "       0.00084265, 0.00467133, 0.00395241, 0.00442577, 0.00283188,\n",
       "       0.00296843, 0.00258882, 0.00239707, 0.00267568, 0.00314963,\n",
       "       0.00378223, 0.00103746, 0.00090344, 0.03196753, 0.0279748 ,\n",
       "       0.00355432, 0.00662291, 0.01830295, 0.00277051, 0.00248239,\n",
       "       0.00633551, 0.00134722, 0.00163733, 0.00607481, 0.00078866,\n",
       "       0.00121877, 0.00200909, 0.00198772, 0.001147  , 0.00286276,\n",
       "       0.00511878, 0.00127351, 0.0021    , 0.00386479, 0.00269924,\n",
       "       0.00213108, 0.00061185, 0.0060412 , 0.02212995, 0.00345927,\n",
       "       0.00074148, 0.00076391, 0.00169239, 0.00046744, 0.00091956,\n",
       "       0.0045688 , 0.00125842, 0.00593046, 0.00881329, 0.01362837,\n",
       "       0.01803406, 0.01668076, 0.00517187, 0.00178271, 0.00196346,\n",
       "       0.00225733, 0.00370223, 0.00113495, 0.00154612, 0.0005226 ,\n",
       "       0.00163919, 0.00091402, 0.00090617, 0.00095193, 0.00078946,\n",
       "       0.00107228, 0.00074424, 0.00078403, 0.00091253, 0.00086344,\n",
       "       0.00113343, 0.00156357, 0.00053831, 0.0009931 , 0.00125063,\n",
       "       0.0010335 , 0.00257352, 0.00270913, 0.00199555, 0.00134177,\n",
       "       0.00123034, 0.00189133, 0.00274659, 0.00379149, 0.00497134,\n",
       "       0.00151746, 0.00062691, 0.00405126, 0.00361118, 0.00089146,\n",
       "       0.00094849, 0.00100442, 0.00092365, 0.00098271, 0.00121221,\n",
       "       0.00126537, 0.00104504, 0.0008311 , 0.00077912, 0.00396794,\n",
       "       0.00030318, 0.00107106, 0.00228053, 0.00059424, 0.00050553,\n",
       "       0.00054816, 0.00055351, 0.00064333, 0.00083488, 0.00059661,\n",
       "       0.00065498, 0.00059692, 0.00068749, 0.00232376, 0.00022297,\n",
       "       0.00055617, 0.00093467, 0.00229503, 0.00240276, 0.00212472,\n",
       "       0.0026342 , 0.00284804, 0.00217263, 0.00299427, 0.00224673,\n",
       "       0.00266036, 0.00261133, 0.00118808, 0.00085571, 0.00073373,\n",
       "       0.00083674, 0.00084302, 0.00076852, 0.00083273, 0.00072219,\n",
       "       0.00069142, 0.00085638, 0.00105654, 0.00064712, 0.00085875,\n",
       "       0.00088911, 0.00116123, 0.00061591, 0.03920104, 0.02204138,\n",
       "       0.00418584, 0.00813773, 0.0185215 , 0.00093011, 0.00113442,\n",
       "       0.00350465, 0.01516109, 0.00166107, 0.00414547, 0.01800332,\n",
       "       0.00127772, 0.00303952, 0.00093129, 0.00092179, 0.00207186,\n",
       "       0.00155247, 0.00131827, 0.00258373, 0.00113324, 0.00126346,\n",
       "       0.00139237, 0.01847617, 0.00148166, 0.00067022, 0.00064393,\n",
       "       0.0006273 , 0.00462267, 0.00089786, 0.00258653, 0.00176912,\n",
       "       0.03320959, 0.02950769])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB1.columns.get_loc('rr_int_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1656, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "norm_dtw = [\"dtw1_v1\",\"dtw2_v1\"]\n",
    "classID = [\"classID\"]\n",
    "non_clinic = list(DB1_non_cli.iloc[:,0:140].columns.values)\n",
    "\n",
    "feature_norm_mill = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_mill.append(str(feat_labels[i]+\"_n_MLII\"))\n",
    "\n",
    "feature_norm_V1 = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_V1.append(str(feat_labels[i]+\"_n_V1\"))\n",
    "    \n",
    "\n",
    "c_ID = np.asarray(classID)\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "non_cli = np.asarray(non_clinic)\n",
    "norm_mlii = np.asarray(feature_norm_mill)\n",
    "norm_v1 = np.asarray(feature_norm_V1)\n",
    "norm_dtw = np.asarray(norm_dtw)\n",
    "\n",
    "\n",
    "features_clinic = np.hstack((f_M,f_V, f_d))\n",
    "\n",
    "#row=[]\n",
    "#for i in range(0,len(np_clinic_new_1)):\n",
    "    #row.append(i)\n",
    "    \n",
    "features = []\n",
    "for feature in zip(features_clinic, rf.feature_importances_):\n",
    "    \n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['features', 'rfscore']\n",
    "import pandas as pd\n",
    "#df = pd.DataFrame(features,columns=columns)\n",
    "df.to_csv('database/features_from_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rr_int_pre_V', 'dtw1', 'rr_int_pre', 'dtw2', 'rr_int_post',\n",
       "       'R_duration', 'neg_RQ', 'rr_int_post_V', 'R_amp7', 'rr_int_all_V',\n",
       "       'RP_V', 'rr_int_all', 'R_amp3_V', 'PR_int_V', 'R_amp4_V',\n",
       "       'PQ_int_V', 'R_amp6', 'R_amp5', 'R_amp9', 'R_amp2_V', 'R_amp4',\n",
       "       'R_amp1_V', 'Q_prominence', 'Q_height', 'R_areas', 'rr_int_50_V',\n",
       "       'T_amp9', 'P_height', 'R_amp8', 'R_prominence', 'S_amp0',\n",
       "       'rr_int_50', 'T_amp2', 'R_amp3', 'QRS_int_50', 'P_prominence',\n",
       "       'T_amp1', 'R_height', 'PQ_int_50', 'TR', 'R_amp0_V', 'T_amp8',\n",
       "       'S_prominence', 'R_amp5_V', 'RT_int', 'S_amp9_V', 'S_height',\n",
       "       'T_amp3', 'T_neg_amp0', 'T_amp0', 'neg_RT_V', 'Q_duration',\n",
       "       'R_duration_V', 'S_amp2', 'T_neg_amp2', 'rr_int_10_V',\n",
       "       'PQ_int_50_V', 'P_duration_V', 'P_prominence_V', 'T_neg_amp1',\n",
       "       'T_amp5', 'P_duration', 'PT_int', 'T_amp6', 'S_amp8_V',\n",
       "       'T_neg_amp9', 'R_amp9_V', 'S_amp3', 'T_amp4', 'R_amp2',\n",
       "       'P_height_V', 'rr_int_10', 'QRS_int_50_V', 'T_amp7', 'neg_PR',\n",
       "       'T_prominence', 'R_amp1', 'S_duration', 'T_neg_amp8',\n",
       "       'PR_int_50_V', 'T_amp6_V', 'T_neg_amp4', 'T_areas', 'P_amp1',\n",
       "       'ST_int_50', 'T_amp4_V', 'T_neg_amp3', 'R_amp0', 'QRS_int',\n",
       "       'P_areas', 'S_amp7_V', 'S_amp2_V', 'PT_int_10', 'T_neg_amp7',\n",
       "       'T_amp8_V', 'S_amp1', 'T_amp3_V', 'T_amp9_V', 'T_neg_amp5',\n",
       "       'P_neg_T_V', 'RT_int_50_V', 'S_amp1_V', 'S_amp9', 'QRS_int_10',\n",
       "       'P_amp0', 'T_height', 'T_amp1_V', 'T_neg_amp6', 'S_amp4',\n",
       "       'P_neg_prominence_V', 'T_amp0_V', 'P_neg_height_V', 'R_amp8_V',\n",
       "       'T_amp7_V', 'T_amp5_V', 'PT_int_50', 'T_amp2_V', 'RT_int_50',\n",
       "       'ST_int_50_V', 'P_amp8', 'PR_int_50', 'S_amp3_V', 'ST_int',\n",
       "       'R_amp7_V', 'S_amp8', 'P_amp2', 'P_amp9', 'S_amp6_V', 'Q_amp0',\n",
       "       'R_amp6_V', 'neg_P_neg_T_V', 'neg_PT', 'PQ_int_10_V', 'Q_height_V',\n",
       "       'PQ_int_10', 'S_amp5', 'Q_amp1', 'S_amp6', 'P_amp3',\n",
       "       'Q_prominence_V', 'RT_int_V', 'R_areas_V', 'S_prominence_V',\n",
       "       'TR_V', 'P_amp5', 'S_amp7', 'PT_int_50_V', 'Q_amp5', 'P_amp7',\n",
       "       'PQ_int', 'S_amp4_V', 'S_areas', 'RT_int_10_V', 'P_amp4', 'P_amp6',\n",
       "       'T_duration', 'Q_amp6', 'Q_amp2', 'PR_int_10_V', 'RT_int_10',\n",
       "       'P_amp6_V', 'PT_int_10_V', 'R_height_V', 'S_height_V', 'S_amp5_V',\n",
       "       'PR_int_10', 'P_amp5_V', 'Q_amp4', 'T_prominence_V', 'P_neg_amp9',\n",
       "       'T_neg_prominence_V', 'Q_amp9', 'Q_amp3', 'ST_int_10',\n",
       "       'R_prominence_V', 'QRS_int_10_V', 'Q_amp9_V', 'PT_int_V',\n",
       "       'P_neg_prominence', 'P_neg_amp8', 'P_neg_amp3', 'Q_amp4_V',\n",
       "       'P_neg_duration_V', 'P_neg_amp0', 'T_neg_amp6_V', 'P_neg_amp5',\n",
       "       'P_amp7_V', 'T_neg_prominence', 'S_amp0_V', 'P_neg_amp4',\n",
       "       'P_amp2_V'], dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "df[df['rfscore'] >= 0.001]['features'].values\n",
    "\n",
    "#rank['features'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_test_df = pd.DataFrame(X_test,columns=features_clinic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1558750839562633"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_test_df[rank['features'].values]['QRS_int_50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = array(['rr_int_pre_V', 'dtw1', 'rr_int_pre', 'dtw2', 'rr_int_post',\n",
    "       'R_duration', 'neg_RQ', 'rr_int_post_V', 'R_amp7', 'rr_int_all_V',\n",
    "       'RP_V', 'rr_int_all', 'R_amp3_V', 'PR_int_V', 'R_amp4_V',\n",
    "       'PQ_int_V', 'R_amp6', 'R_amp5', 'R_amp9', 'R_amp2_V', 'R_amp4',\n",
    "       'R_amp1_V', 'Q_prominence', 'Q_height', 'R_areas', 'rr_int_50_V',\n",
    "       'T_amp9', 'P_height', 'R_amp8', 'R_prominence', 'S_amp0',\n",
    "       'rr_int_50', 'T_amp2', 'R_amp3', 'QRS_int_50', 'P_prominence',\n",
    "       'T_amp1', 'R_height', 'PQ_int_50', 'TR', 'R_amp0_V', 'T_amp8',\n",
    "       'S_prominence', 'R_amp5_V', 'RT_int', 'S_amp9_V', 'S_height',\n",
    "       'T_amp3', 'T_neg_amp0', 'T_amp0', 'neg_RT_V', 'Q_duration',\n",
    "       'R_duration_V', 'S_amp2', 'T_neg_amp2', 'rr_int_10_V',\n",
    "       'PQ_int_50_V', 'P_duration_V'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0040356294719912"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_rr[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
