{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])\n",
    "\n",
    "rank = pd.read_csv(\"database/features_ranking.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n",
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n",
      "done norm\n",
      "row 140 done\n",
      "done norm\n",
      "row 141 done\n",
      "done norm\n",
      "row 142 done\n",
      "done norm\n",
      "row 143 done\n",
      "done norm\n",
      "row 144 done\n",
      "done norm\n",
      "row 145 done\n",
      "done norm\n",
      "row 146 done\n",
      "done norm\n",
      "row 147 done\n",
      "done norm\n",
      "row 148 done\n",
      "done norm\n",
      "row 149 done\n",
      "done norm\n",
      "row 150 done\n",
      "done norm\n",
      "row 151 done\n",
      "done norm\n",
      "row 152 done\n",
      "done norm\n",
      "row 153 done\n",
      "done norm\n",
      "row 154 done\n",
      "done norm\n",
      "row 155 done\n",
      "done norm\n",
      "row 156 done\n",
      "done norm\n",
      "row 157 done\n",
      "done norm\n",
      "row 158 done\n",
      "done norm\n",
      "row 159 done\n",
      "done norm\n",
      "row 160 done\n",
      "done norm\n",
      "row 161 done\n",
      "done norm\n",
      "row 162 done\n",
      "done norm\n",
      "row 163 done\n",
      "done norm\n",
      "row 164 done\n",
      "done norm\n",
      "row 165 done\n",
      "done norm\n",
      "row 166 done\n",
      "done norm\n",
      "row 167 done\n",
      "done norm\n",
      "row 168 done\n",
      "done norm\n",
      "row 169 done\n",
      "done norm\n",
      "row 170 done\n",
      "done norm\n",
      "row 171 done\n",
      "done norm\n",
      "row 172 done\n",
      "done norm\n",
      "row 173 done\n",
      "done norm\n",
      "row 174 done\n",
      "done norm\n",
      "row 175 done\n",
      "done norm\n",
      "row 176 done\n",
      "done norm\n",
      "row 177 done\n",
      "done norm\n",
      "row 178 done\n",
      "done norm\n",
      "row 179 done\n",
      "done norm\n",
      "row 180 done\n",
      "done norm\n",
      "row 181 done\n",
      "done norm\n",
      "row 182 done\n",
      "done norm\n",
      "row 183 done\n",
      "done norm\n",
      "row 184 done\n",
      "done norm\n",
      "row 185 done\n",
      "done norm\n",
      "row 186 done\n",
      "done norm\n",
      "row 187 done\n",
      "done norm\n",
      "row 188 done\n",
      "done norm\n",
      "row 189 done\n",
      "done norm\n",
      "row 190 done\n",
      "done norm\n",
      "row 191 done\n",
      "done norm\n",
      "row 192 done\n",
      "done norm\n",
      "row 193 done\n",
      "done norm\n",
      "row 194 done\n",
      "done norm\n",
      "row 195 done\n",
      "done norm\n",
      "row 196 done\n",
      "done norm\n",
      "row 197 done\n",
      "done norm\n",
      "row 198 done\n",
      "done norm\n",
      "row 199 done\n",
      "done norm\n",
      "row 200 done\n",
      "done norm\n",
      "row 201 done\n",
      "done norm\n",
      "row 202 done\n",
      "done norm\n",
      "row 203 done\n",
      "done norm\n",
      "row 204 done\n",
      "done norm\n",
      "row 205 done\n",
      "done norm\n",
      "row 206 done\n",
      "done norm\n",
      "row 207 done\n",
      "done norm\n",
      "row 208 done\n",
      "done norm\n",
      "row 209 done\n",
      "done norm\n",
      "row 210 done\n",
      "done norm\n",
      "row 211 done\n",
      "done norm\n",
      "row 212 done\n",
      "done norm\n",
      "row 213 done\n",
      "done norm\n",
      "row 214 done\n",
      "done norm\n",
      "row 215 done\n",
      "done norm\n",
      "row 216 done\n",
      "done norm\n",
      "row 217 done\n",
      "done norm\n",
      "row 218 done\n",
      "done norm\n",
      "row 219 done\n",
      "done norm\n",
      "row 220 done\n",
      "done norm\n",
      "row 221 done\n",
      "done norm\n",
      "row 222 done\n",
      "done norm\n",
      "row 223 done\n",
      "done norm\n",
      "row 224 done\n",
      "done norm\n",
      "row 225 done\n",
      "done norm\n",
      "row 226 done\n",
      "done norm\n",
      "row 227 done\n",
      "done norm\n",
      "row 228 done\n",
      "done norm\n",
      "row 229 done\n",
      "done norm\n",
      "row 230 done\n",
      "done norm\n",
      "row 231 done\n",
      "done norm\n",
      "row 232 done\n",
      "done norm\n",
      "row 233 done\n",
      "done norm\n",
      "row 234 done\n",
      "done norm\n",
      "row 235 done\n",
      "done norm\n",
      "row 236 done\n",
      "done norm\n",
      "row 237 done\n",
      "done norm\n",
      "row 238 done\n",
      "done norm\n",
      "row 239 done\n",
      "done norm\n",
      "row 240 done\n",
      "done norm\n",
      "row 241 done\n",
      "done norm\n",
      "row 242 done\n",
      "done norm\n",
      "row 243 done\n",
      "done norm\n",
      "row 244 done\n",
      "done norm\n",
      "row 245 done\n",
      "done norm\n",
      "row 246 done\n",
      "done norm\n",
      "row 247 done\n",
      "done norm\n",
      "row 248 done\n",
      "done norm\n",
      "row 249 done\n",
      "done norm\n",
      "row 250 done\n",
      "done norm\n",
      "row 251 done\n",
      "done norm\n",
      "row 252 done\n",
      "done norm\n",
      "row 253 done\n",
      "done norm\n",
      "row 254 done\n",
      "done norm\n",
      "row 255 done\n",
      "done norm\n",
      "row 256 done\n",
      "done norm\n",
      "row 257 done\n",
      "done norm\n",
      "row 258 done\n",
      "done norm\n",
      "row 259 done\n",
      "done norm\n",
      "row 260 done\n",
      "done norm\n",
      "row 261 done\n",
      "done norm\n",
      "row 0 done\n",
      "done norm\n",
      "row 1 done\n",
      "done norm\n",
      "row 2 done\n",
      "done norm\n",
      "row 3 done\n",
      "done norm\n",
      "row 4 done\n",
      "done norm\n",
      "row 5 done\n",
      "done norm\n",
      "row 6 done\n",
      "done norm\n",
      "row 7 done\n",
      "done norm\n",
      "row 8 done\n",
      "done norm\n",
      "row 9 done\n",
      "done norm\n",
      "row 10 done\n",
      "done norm\n",
      "row 11 done\n",
      "done norm\n",
      "row 12 done\n",
      "done norm\n",
      "row 13 done\n",
      "done norm\n",
      "row 14 done\n",
      "done norm\n",
      "row 15 done\n",
      "done norm\n",
      "row 16 done\n",
      "done norm\n",
      "row 17 done\n",
      "done norm\n",
      "row 18 done\n",
      "done norm\n",
      "row 19 done\n",
      "done norm\n",
      "row 20 done\n",
      "done norm\n",
      "row 21 done\n",
      "done norm\n",
      "row 22 done\n",
      "done norm\n",
      "row 23 done\n",
      "done norm\n",
      "row 24 done\n",
      "done norm\n",
      "row 25 done\n",
      "done norm\n",
      "row 26 done\n",
      "done norm\n",
      "row 27 done\n",
      "done norm\n",
      "row 28 done\n",
      "done norm\n",
      "row 29 done\n",
      "done norm\n",
      "row 30 done\n",
      "done norm\n",
      "row 31 done\n",
      "done norm\n",
      "row 32 done\n",
      "done norm\n",
      "row 33 done\n",
      "done norm\n",
      "row 34 done\n",
      "done norm\n",
      "row 35 done\n",
      "done norm\n",
      "row 36 done\n",
      "done norm\n",
      "row 37 done\n",
      "done norm\n",
      "row 38 done\n",
      "done norm\n",
      "row 39 done\n",
      "done norm\n",
      "row 40 done\n",
      "done norm\n",
      "row 41 done\n",
      "done norm\n",
      "row 42 done\n",
      "done norm\n",
      "row 43 done\n",
      "done norm\n",
      "row 44 done\n",
      "done norm\n",
      "row 45 done\n",
      "done norm\n",
      "row 46 done\n",
      "done norm\n",
      "row 47 done\n",
      "done norm\n",
      "row 48 done\n",
      "done norm\n",
      "row 49 done\n",
      "done norm\n",
      "row 50 done\n",
      "done norm\n",
      "row 51 done\n",
      "done norm\n",
      "row 52 done\n",
      "done norm\n",
      "row 53 done\n",
      "done norm\n",
      "row 54 done\n",
      "done norm\n",
      "row 55 done\n",
      "done norm\n",
      "row 56 done\n",
      "done norm\n",
      "row 57 done\n",
      "done norm\n",
      "row 58 done\n",
      "done norm\n",
      "row 59 done\n",
      "done norm\n",
      "row 60 done\n",
      "done norm\n",
      "row 61 done\n",
      "done norm\n",
      "row 62 done\n",
      "done norm\n",
      "row 63 done\n",
      "done norm\n",
      "row 64 done\n",
      "done norm\n",
      "row 65 done\n",
      "done norm\n",
      "row 66 done\n",
      "done norm\n",
      "row 67 done\n",
      "done norm\n",
      "row 68 done\n",
      "done norm\n",
      "row 69 done\n",
      "done norm\n",
      "row 70 done\n",
      "done norm\n",
      "row 71 done\n",
      "done norm\n",
      "row 72 done\n",
      "done norm\n",
      "row 73 done\n",
      "done norm\n",
      "row 74 done\n",
      "done norm\n",
      "row 75 done\n",
      "done norm\n",
      "row 76 done\n",
      "done norm\n",
      "row 77 done\n",
      "done norm\n",
      "row 78 done\n",
      "done norm\n",
      "row 79 done\n",
      "done norm\n",
      "row 80 done\n",
      "done norm\n",
      "row 81 done\n",
      "done norm\n",
      "row 82 done\n",
      "done norm\n",
      "row 83 done\n",
      "done norm\n",
      "row 84 done\n",
      "done norm\n",
      "row 85 done\n",
      "done norm\n",
      "row 86 done\n",
      "done norm\n",
      "row 87 done\n",
      "done norm\n",
      "row 88 done\n",
      "done norm\n",
      "row 89 done\n",
      "done norm\n",
      "row 90 done\n",
      "done norm\n",
      "row 91 done\n",
      "done norm\n",
      "row 92 done\n",
      "done norm\n",
      "row 93 done\n",
      "done norm\n",
      "row 94 done\n",
      "done norm\n",
      "row 95 done\n",
      "done norm\n",
      "row 96 done\n",
      "done norm\n",
      "row 97 done\n",
      "done norm\n",
      "row 98 done\n",
      "done norm\n",
      "row 99 done\n",
      "done norm\n",
      "row 100 done\n",
      "done norm\n",
      "row 101 done\n",
      "done norm\n",
      "row 102 done\n",
      "done norm\n",
      "row 103 done\n",
      "done norm\n",
      "row 104 done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done norm\n",
      "row 105 done\n",
      "done norm\n",
      "row 106 done\n",
      "done norm\n",
      "row 107 done\n",
      "done norm\n",
      "row 108 done\n",
      "done norm\n",
      "row 109 done\n",
      "done norm\n",
      "row 110 done\n",
      "done norm\n",
      "row 111 done\n",
      "done norm\n",
      "row 112 done\n",
      "done norm\n",
      "row 113 done\n",
      "done norm\n",
      "row 114 done\n",
      "done norm\n",
      "row 115 done\n",
      "done norm\n",
      "row 116 done\n",
      "done norm\n",
      "row 117 done\n",
      "done norm\n",
      "row 118 done\n",
      "done norm\n",
      "row 119 done\n",
      "done norm\n",
      "row 120 done\n",
      "done norm\n",
      "row 121 done\n",
      "done norm\n",
      "row 122 done\n",
      "done norm\n",
      "row 123 done\n",
      "done norm\n",
      "row 124 done\n",
      "done norm\n",
      "row 125 done\n",
      "done norm\n",
      "row 126 done\n",
      "done norm\n",
      "row 127 done\n",
      "done norm\n",
      "row 128 done\n",
      "done norm\n",
      "row 129 done\n",
      "done norm\n",
      "row 130 done\n",
      "done norm\n",
      "row 131 done\n",
      "done norm\n",
      "row 132 done\n",
      "done norm\n",
      "row 133 done\n",
      "done norm\n",
      "row 134 done\n",
      "done norm\n",
      "row 135 done\n",
      "done norm\n",
      "row 136 done\n",
      "done norm\n",
      "row 137 done\n",
      "done norm\n",
      "row 138 done\n",
      "done norm\n",
      "row 139 done\n",
      "done norm\n",
      "row 140 done\n",
      "done norm\n",
      "row 141 done\n",
      "done norm\n",
      "row 142 done\n",
      "done norm\n",
      "row 143 done\n",
      "done norm\n",
      "row 144 done\n",
      "done norm\n",
      "row 145 done\n",
      "done norm\n",
      "row 146 done\n",
      "done norm\n",
      "row 147 done\n",
      "done norm\n",
      "row 148 done\n",
      "done norm\n",
      "row 149 done\n",
      "done norm\n",
      "row 150 done\n",
      "done norm\n",
      "row 151 done\n",
      "done norm\n",
      "row 152 done\n",
      "done norm\n",
      "row 153 done\n",
      "done norm\n",
      "row 154 done\n",
      "done norm\n",
      "row 155 done\n",
      "done norm\n",
      "row 156 done\n",
      "done norm\n",
      "row 157 done\n",
      "done norm\n",
      "row 158 done\n",
      "done norm\n",
      "row 159 done\n",
      "done norm\n",
      "row 160 done\n",
      "done norm\n",
      "row 161 done\n",
      "done norm\n",
      "row 162 done\n",
      "done norm\n",
      "row 163 done\n",
      "done norm\n",
      "row 164 done\n",
      "done norm\n",
      "row 165 done\n",
      "done norm\n",
      "row 166 done\n",
      "done norm\n",
      "row 167 done\n",
      "done norm\n",
      "row 168 done\n",
      "done norm\n",
      "row 169 done\n",
      "done norm\n",
      "row 170 done\n",
      "done norm\n",
      "row 171 done\n",
      "done norm\n",
      "row 172 done\n",
      "done norm\n",
      "row 173 done\n",
      "done norm\n",
      "row 174 done\n",
      "done norm\n",
      "row 175 done\n",
      "done norm\n",
      "row 176 done\n",
      "done norm\n",
      "row 177 done\n",
      "done norm\n",
      "row 178 done\n",
      "done norm\n",
      "row 179 done\n",
      "done norm\n",
      "row 180 done\n",
      "done norm\n",
      "row 181 done\n",
      "done norm\n",
      "row 182 done\n",
      "done norm\n",
      "row 183 done\n",
      "done norm\n",
      "row 184 done\n",
      "done norm\n",
      "row 185 done\n",
      "done norm\n",
      "row 186 done\n",
      "done norm\n",
      "row 187 done\n",
      "done norm\n",
      "row 188 done\n",
      "done norm\n",
      "row 189 done\n",
      "done norm\n",
      "row 190 done\n",
      "done norm\n",
      "row 191 done\n",
      "done norm\n",
      "row 192 done\n",
      "done norm\n",
      "row 193 done\n",
      "done norm\n",
      "row 194 done\n",
      "done norm\n",
      "row 195 done\n",
      "done norm\n",
      "row 196 done\n",
      "done norm\n",
      "row 197 done\n",
      "done norm\n",
      "row 198 done\n",
      "done norm\n",
      "row 199 done\n",
      "done norm\n",
      "row 200 done\n",
      "done norm\n",
      "row 201 done\n",
      "done norm\n",
      "row 202 done\n",
      "done norm\n",
      "row 203 done\n",
      "done norm\n",
      "row 204 done\n",
      "done norm\n",
      "row 205 done\n",
      "done norm\n",
      "row 206 done\n",
      "done norm\n",
      "row 207 done\n",
      "done norm\n",
      "row 208 done\n",
      "done norm\n",
      "row 209 done\n",
      "done norm\n",
      "row 210 done\n",
      "done norm\n",
      "row 211 done\n",
      "done norm\n",
      "row 212 done\n",
      "done norm\n",
      "row 213 done\n",
      "done norm\n",
      "row 214 done\n",
      "done norm\n",
      "row 215 done\n",
      "done norm\n",
      "row 216 done\n",
      "done norm\n",
      "row 217 done\n",
      "done norm\n",
      "row 218 done\n",
      "done norm\n",
      "row 219 done\n",
      "done norm\n",
      "row 220 done\n",
      "done norm\n",
      "row 221 done\n",
      "done norm\n",
      "row 222 done\n",
      "done norm\n",
      "row 223 done\n",
      "done norm\n",
      "row 224 done\n",
      "done norm\n",
      "row 225 done\n",
      "done norm\n",
      "row 226 done\n",
      "done norm\n",
      "row 227 done\n",
      "done norm\n",
      "row 228 done\n",
      "done norm\n",
      "row 229 done\n",
      "done norm\n",
      "row 230 done\n",
      "done norm\n",
      "row 231 done\n",
      "done norm\n",
      "row 232 done\n",
      "done norm\n",
      "row 233 done\n",
      "done norm\n",
      "row 234 done\n",
      "done norm\n",
      "row 235 done\n",
      "done norm\n",
      "row 236 done\n",
      "done norm\n",
      "row 237 done\n",
      "done norm\n",
      "row 238 done\n",
      "done norm\n",
      "row 239 done\n",
      "done norm\n",
      "row 240 done\n",
      "done norm\n",
      "row 241 done\n",
      "done norm\n",
      "row 242 done\n",
      "done norm\n",
      "row 243 done\n",
      "done norm\n",
      "row 244 done\n",
      "done norm\n",
      "row 245 done\n",
      "done norm\n",
      "row 246 done\n",
      "done norm\n",
      "row 247 done\n",
      "done norm\n",
      "row 248 done\n",
      "done norm\n",
      "row 249 done\n",
      "done norm\n",
      "row 250 done\n",
      "done norm\n",
      "row 251 done\n",
      "done norm\n",
      "row 252 done\n",
      "done norm\n",
      "row 253 done\n",
      "done norm\n",
      "row 254 done\n",
      "done norm\n",
      "row 255 done\n",
      "done norm\n",
      "row 256 done\n",
      "done norm\n",
      "row 257 done\n",
      "done norm\n",
      "row 258 done\n",
      "done norm\n",
      "row 259 done\n",
      "done norm\n",
      "row 260 done\n",
      "done norm\n",
      "row 261 done\n"
     ]
    }
   ],
   "source": [
    "norm_var_1_clinic = post_features.normalised_values_multiples(good_features_X)\n",
    "norm_var_2_clinic = post_features.normalised_values_multiples(X_test)\n",
    "#norm_var_1_non = post_features.normalised_values_multiples(np_non_var_1)\n",
    "#norm_var_2_non = post_features.normalised_values_multiples(np_non_var_2)\n",
    "\n",
    "\n",
    "\n",
    "X_train_norm = np.hstack((good_features_X,norm_var_1_clinic))\n",
    "X_test_norm = np.hstack((X_test,norm_var_2_clinic))\n",
    "rank_norm = pd.read_csv(\"database/features_ranking_with_norm.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "norm_dtw = [\"dtw1_v1\",\"dtw2_v1\"]\n",
    "classID = [\"classID\"]\n",
    "non_clinic = list(DB1_non_cli.iloc[:,0:140].columns.values)\n",
    "\n",
    "feature_norm_mill = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_mill.append(str(feat_labels[i]+\"_n_MLII\"))\n",
    "\n",
    "feature_norm_V1 = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_V1.append(str(feat_labels[i]+\"_n_V1\"))\n",
    "    \n",
    "\n",
    "c_ID = np.asarray(classID)\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "non_cli = np.asarray(non_clinic)\n",
    "norm_mlii = np.asarray(feature_norm_mill)\n",
    "norm_v1 = np.asarray(feature_norm_V1)\n",
    "norm_dtw = np.asarray(norm_dtw)\n",
    "\n",
    "\n",
    "features_clinic = np.hstack((f_M,f_V, f_d))\n",
    "features_clinic_norm = np.hstack((f_M,f_V, f_d,norm_mlii,norm_v1,norm_dtw ))\n",
    "#row=[]\n",
    "#for i in range(0,len(np_clinic_new_1)):\n",
    "    #row.append(i)\n",
    "    \n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "#X_train_balanced_norm = pd.DataFrame(X_train_norm,columns=features_clinic_norm)\n",
    "#X_test_norm = pd.DataFrame(X_test_norm,columns=features_clinic_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "XGboost\n",
      "[[34424  2201  2976  4420]\n",
      " [ 1189   469   383     9]\n",
      " [   49    75  3050    46]\n",
      " [   26     2    67   293]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86     44021\n",
      "           1       0.17      0.23      0.20      2050\n",
      "           2       0.47      0.95      0.63      3220\n",
      "           3       0.06      0.76      0.11       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.42      0.68      0.45     49679\n",
      "weighted avg       0.89      0.77      0.82     49679\n",
      "\n",
      "[0.34556103427813883, 1.8176868984700036, 0.3999913794478199]\n",
      "Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30605  5051  1006  7359]\n",
      " [  262  1441   296    51]\n",
      " [   46   250  2644   280]\n",
      " [   14     0    14   360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.70      0.82     44021\n",
      "           1       0.21      0.70      0.33      2050\n",
      "           2       0.67      0.82      0.74      3220\n",
      "           3       0.04      0.93      0.09       388\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     49679\n",
      "   macro avg       0.48      0.79      0.49     49679\n",
      "weighted avg       0.93      0.71      0.79     49679\n",
      "\n",
      "[0.32512357477278625, 2.4054564061636237, 0.4632438381568461]\n",
      "Linear_D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26769 10324   211  6717]\n",
      " [  317  1435   250    48]\n",
      " [   56   265  2534   365]\n",
      " [   17     0    11   360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.75     44021\n",
      "           1       0.12      0.70      0.20      2050\n",
      "           2       0.84      0.79      0.81      3220\n",
      "           3       0.05      0.93      0.09       388\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     49679\n",
      "   macro avg       0.50      0.76      0.47     49679\n",
      "weighted avg       0.93      0.63      0.73     49679\n",
      "\n",
      "[0.2526846194659283, 2.4492818710405277, 0.4325025436130301]\n",
      "SVM_LN\n",
      "[[34172  2882   617  6350]\n",
      " [  277  1495   240    38]\n",
      " [   70   335  2665   150]\n",
      " [   15     1    15   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87     44021\n",
      "           1       0.32      0.73      0.44      2050\n",
      "           2       0.75      0.83      0.79      3220\n",
      "           3       0.05      0.92      0.10       388\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     49679\n",
      "   macro avg       0.53      0.81      0.55     49679\n",
      "weighted avg       0.94      0.78      0.84     49679\n",
      "\n",
      "[0.40915665094099624, 2.627579154605376, 0.5330257197961701]\n",
      "voting_ensemble\n",
      "ada\n",
      "voting_ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39772   356   118  3775]\n",
      " [ 1708    49   254    39]\n",
      " [  379    63  2487   291]\n",
      " [   23     0     9   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     44021\n",
      "           1       0.10      0.02      0.04      2050\n",
      "           2       0.87      0.77      0.82      3220\n",
      "           3       0.08      0.92      0.15       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.50      0.65      0.48     49679\n",
      "weighted avg       0.90      0.86      0.88     49679\n",
      "\n",
      "[0.4309214399351081, 1.768118353887931, 0.43647551420354547]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " 0.43647551420354547)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0.01\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values], good_features_y, X_test[rank[rank['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values], good_features_y, X_test[rank[rank['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "#voting_ensemble(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "\n",
    "#Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "score = 0.01\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "clf.fit(X_train[feature_good], y_train)\n",
    "y_pred = clf.predict(X_test[feature_good])\n",
    "print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38399   783   351  4488]\n",
      " [ 1493   183   329    45]\n",
      " [   91    77  2839   213]\n",
      " [   20     0    12   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91     44021\n",
      "           1       0.18      0.09      0.12      2050\n",
      "           2       0.80      0.88      0.84      3220\n",
      "           3       0.07      0.92      0.13       388\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     49679\n",
      "   macro avg       0.50      0.69      0.50     49679\n",
      "weighted avg       0.91      0.84      0.87     49679\n",
      "\n",
      "[0.4323372331587701, 1.9504222520303145, 0.45997139808317433]\n",
      "[[39772   356   118  3775]\n",
      " [ 1708    49   254    39]\n",
      " [  379    63  2487   291]\n",
      " [   23     0     9   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     44021\n",
      "           1       0.10      0.02      0.04      2050\n",
      "           2       0.87      0.77      0.82      3220\n",
      "           3       0.08      0.92      0.15       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.50      0.65      0.48     49679\n",
      "weighted avg       0.90      0.86      0.88     49679\n",
      "\n",
      "[0.4309214399351081, 1.768118353887931, 0.43647551420354547]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001),\n",
       " 0.43647551420354547)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0.01\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "#xgboost(np.asarray(X_train[rank[rank['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "logisticRegress(X_train[rank[rank['rfscore'] >= score]['features'].values], y_train, X_test[rank[rank['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "Linear_D(X_train[rank[rank['rfscore'] >= score]['features'].values], y_train, X_test[rank[rank['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#svm_model_linear(np.asarray(X_train[rank[rank['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "#voting_ensemble(np.asarray(X_train[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "#ada(np.asarray(X_train[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5940 10612  5189 22280]\n",
      " [   28  1620    42   360]\n",
      " [   89  1968   348   815]\n",
      " [   10    12    18   348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.13      0.24     44021\n",
      "           1       0.11      0.79      0.20      2050\n",
      "           2       0.06      0.11      0.08      3220\n",
      "           3       0.01      0.90      0.03       388\n",
      "\n",
      "   micro avg       0.17      0.17      0.17     49679\n",
      "   macro avg       0.29      0.48      0.14     49679\n",
      "weighted avg       0.88      0.17      0.22     49679\n",
      "\n",
      "[0.040419908159769806, 1.0744827814072828, 0.15452030175579526]\n",
      "[[26710  2582  7599  7130]\n",
      " [  652   429   940    29]\n",
      " [    6     4  3202     8]\n",
      " [   12     2   373     1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.61      0.75     44021\n",
      "           1       0.14      0.21      0.17      2050\n",
      "           2       0.26      0.99      0.42      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     49679\n",
      "   macro avg       0.35      0.45      0.33     49679\n",
      "weighted avg       0.89      0.61      0.70     49679\n",
      "\n",
      "[0.20917495180946646, 1.6101947350042893, 0.3058618177802694]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26285 10827   659  6250]\n",
      " [  217  1509   311    13]\n",
      " [   16   245  2839   120]\n",
      " [   11     1    19   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75     44021\n",
      "           1       0.12      0.74      0.21      2050\n",
      "           2       0.74      0.88      0.81      3220\n",
      "           3       0.05      0.92      0.10       388\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     49679\n",
      "   macro avg       0.48      0.78      0.46     49679\n",
      "weighted avg       0.93      0.62      0.72     49679\n",
      "\n",
      "[0.2628040160327314, 2.4793483609328204, 0.44132055313296825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26508  8288   256  8969]\n",
      " [  222  1453   352    23]\n",
      " [   11   100  2965   144]\n",
      " [   12     0    16   360]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.60      0.75     44021\n",
      "           1       0.15      0.71      0.24      2050\n",
      "           2       0.83      0.92      0.87      3220\n",
      "           3       0.04      0.93      0.07       388\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     49679\n",
      "   macro avg       0.50      0.79      0.48     49679\n",
      "weighted avg       0.94      0.63      0.73     49679\n",
      "\n",
      "[0.2718542460334204, 2.603370951774251, 0.4613484919884916]\n",
      "[[24981  9489  1327  8224]\n",
      " [  197  1550   290    13]\n",
      " [   25   809  2326    60]\n",
      " [   10     2    74   302]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.57      0.72     44021\n",
      "           1       0.13      0.76      0.22      2050\n",
      "           2       0.58      0.72      0.64      3220\n",
      "           3       0.04      0.78      0.07       388\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     49679\n",
      "   macro avg       0.43      0.71      0.41     49679\n",
      "weighted avg       0.92      0.59      0.69     49679\n",
      "\n",
      "[0.22627419854837794, 2.1882985810799807, 0.38667442190918655]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
       "   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "   tol=0.1, verbose=False), 0.38667442190918655)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0.009\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "logisticRegress(X_train_balanced_norm, good_features_y,X_test_norm , y_test, labels=[0,1,2,3])\n",
    "\n",
    "xgboost(np.asarray(X_train_balanced_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "logisticRegress(X_train_balanced_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], good_features_y, X_test_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "Linear_D(X_train_balanced_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], good_features_y, X_test_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "svm_model_linear(X_train_balanced_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], good_features_y, X_test_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n",
      "[0.4007485524244452, 2.4813919785105343, 0.5105482735260394]\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34986   635  3908  4492]\n",
      " [  502  1336   122    90]\n",
      " [  198    51  2525   446]\n",
      " [   26     0     7   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88     44021\n",
      "           1       0.66      0.65      0.66      2050\n",
      "           2       0.38      0.78      0.52      3220\n",
      "           3       0.07      0.91      0.12       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     49679\n",
      "   macro avg       0.52      0.79      0.54     49679\n",
      "weighted avg       0.92      0.79      0.84     49679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34303  2467   450  6166]\n",
      " [  269   364    52    29]\n",
      " [   58   187  2778   146]\n",
      " [   16     0    15   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.88     43386\n",
      "           1       0.12      0.51      0.20       714\n",
      "           2       0.84      0.88      0.86      3169\n",
      "           3       0.05      0.92      0.10       388\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     47657\n",
      "   macro avg       0.50      0.77      0.51     47657\n",
      "weighted avg       0.96      0.79      0.86     47657\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38936   249     7  3744]\n",
      " [  621    17     6    18]\n",
      " [   95    26   140   130]\n",
      " [   17     0     1   355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94     42936\n",
      "           1       0.06      0.03      0.04       662\n",
      "           2       0.91      0.36      0.51       391\n",
      "           3       0.08      0.95      0.15       373\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     44362\n",
      "   macro avg       0.51      0.56      0.41     44362\n",
      "weighted avg       0.96      0.89      0.92     44362\n",
      "\n",
      "[0.17087378697643424, 1.3510461114659127, 0.2543176574214562]\n",
      "[[1174    0  140 2686]\n",
      " [   9    0   15   17]\n",
      " [  11    0  158  127]\n",
      " [   4    0    1  351]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.29      0.45      4000\n",
      "           1       0.00      0.00      0.00        41\n",
      "           2       0.50      0.53      0.52       296\n",
      "           3       0.11      0.99      0.20       356\n",
      "\n",
      "   micro avg       0.36      0.36      0.36      4693\n",
      "   macro avg       0.40      0.45      0.29      4693\n",
      "weighted avg       0.88      0.36      0.43      4693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 668  123  255 1780]\n",
      " [   3   14    7    8]\n",
      " [   2   41  113  129]\n",
      " [   0    0    1  351]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.24      0.38      2826\n",
      "           1       0.08      0.44      0.13        32\n",
      "           2       0.30      0.40      0.34       285\n",
      "           3       0.15      1.00      0.27       352\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3495\n",
      "   macro avg       0.38      0.52      0.28      3495\n",
      "weighted avg       0.84      0.33      0.36      3495\n",
      "\n",
      "[0.12680155733899456, 1.213174828357051, 0.21504763221412865]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])[1]\n",
    "\n",
    "X_test_n_1 =X_test[y_pred != 1]\n",
    "y_test_n_1=y_test[y_pred != 1]\n",
    "\n",
    "y_pred_n = y_pred[y_pred == 1]\n",
    "y_true_1 = y_test[y_pred == 1]\n",
    "\n",
    "score = 0.01\n",
    "\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "y_pred_voting_ensemble =voting_ensemble(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test_n_1[rank[rank['rfscore'] >= score]['features'].values]), np.asarray(y_test_n_1),  labels=[0,1,2,3])[1]\n",
    "\n",
    "X_test_n_2 =X_test_n_1[y_pred_voting_ensemble != 2]\n",
    "y_test_n_2=y_test_n_1[y_pred_voting_ensemble != 2]\n",
    "\n",
    "y_pred_voting_ensemble_n = y_pred_voting_ensemble[y_pred_voting_ensemble == 2]\n",
    "y_true_2 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "y_pred_linear_D =Linear_D(np.asarray(X_train[feature_good]), y_train,np.asarray( X_test_n_2[feature_good]), y_test_n_2,  labels=[0,1,2,3])[1]\n",
    "\n",
    "X_test_n_3 =X_test_n_2[y_pred_linear_D != 2]\n",
    "y_test_n_3=y_test_n_2[y_pred_linear_D != 2]\n",
    "\n",
    "y_pred_linear_D_2 = y_pred_linear_D[y_pred_linear_D == 2]\n",
    "y_true_3 = y_test_n_2[y_pred_linear_D == 2]\n",
    "\n",
    "#y_true_1 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "X_test_n_3 =X_test_n_2[y_pred_linear_D != 0]\n",
    "y_test_n_3=y_test_n_2[y_pred_linear_D != 0]\n",
    "\n",
    "y_pred_linear_D_0 = y_pred_linear_D[y_pred_linear_D == 0]\n",
    "y_true_4 = y_test_n_2[y_pred_linear_D == 0]\n",
    "\n",
    "y_svm = svm_model_poly(X_train_balanced, good_features_y, X_test_n_3, y_test_n_3, labels=[0,1,2,3])[1]\n",
    "\n",
    "\n",
    "X_test_n_4 =X_test_n_3[y_svm != 0]\n",
    "y_test_n_4=y_test_n_3[y_svm != 0]\n",
    "\n",
    "y_svm_n = y_svm[y_svm == 0]\n",
    "y_true_5 = y_test_n_3[y_svm == 0]\n",
    "\n",
    "y_v_e = Linear_D(X_train_balanced, good_features_y, X_test_n_4, y_test_n_4, labels=[0,1,2,3])[1]\n",
    "\n",
    "y_final_pred = np.hstack((y_pred_n,y_pred_voting_ensemble_n, y_pred_linear_D_2,y_pred_linear_D_0,y_svm_n,y_v_e))\n",
    "y_final_true = np.hstack((y_true_1,y_true_2,y_true_3,y_true_4,y_true_5,y_test_n_4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40778   758   712  1780]\n",
      " [  633  1350    65     8]\n",
      " [  108    92  3031   129]\n",
      " [   21     0    17   351]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95     44028\n",
      "           1       0.61      0.66      0.63      2056\n",
      "           2       0.79      0.90      0.84      3360\n",
      "           3       0.15      0.90      0.26       389\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49833\n",
      "   macro avg       0.64      0.85      0.67     49833\n",
      "weighted avg       0.95      0.91      0.93     49833\n",
      "\n",
      "[0.6613550772862884, 2.96475278361551, 0.701271636595083]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_final_true,y_final_pred, labels=[0,1,2,3]))  \n",
    "print(classification_report(y_final_true,y_final_pred, labels=[0,1,2,3]))\n",
    "print(metric.get_metrics(y_final_true,y_final_pred, lb=[0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## small C from 1 to 3 seems to be good for f class recalled\n",
    "## degree 3 is better all around\n",
    "## Tol at 0.01 seems best\n",
    "\n",
    "def svm_model_poly(X_train, y_train, X_test, y_test, C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                        coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3]):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    #print(metric.get_metrics(y_pred,y_test,lb=labels))\n",
    "    \n",
    "    return svm_model_linear,y_pred#, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def Linear_D(X_train, y_train, X_test, y_test, labels = [0,1,2,3]):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred, labels = labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return clf,y_pred, metric.get_metrics(y_pred,y_test,lb=labels)[2]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "## numner of estimators leaves at 1000 \n",
    "\n",
    "def randomForest(X_train, y_train, X_test, y_test,n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, max_features=50, max_leaf_nodes=None, class_weight='balanced', labels = [0,1,2,3]):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,min_weight_fraction_leaf=min_weight_fraction_leaf, class_weight=class_weight, min_impurity_decrease=min_impurity_decrease,max_features=max_features)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return rf, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "### xgbo is good for normal classes too\n",
    "## do this one next \n",
    "## XGB is very goos for V class\n",
    "## XGboost tree classifcation is good with 5 or more tree depth\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def xgboost(X_train,y_train, X_test, y_test, max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, process_type='default', predictor= 'cpu_predictor', labels = [0,1,2,3]): \n",
    "    model = XGBClassifier(max_depth = max_depth, eta = eta, gamma = gamma,min_child_weight=min_child_weight,subsample=subsample, max_delta_step= max_delta_step,colsample_bytree=colsample_bytree,colsample_bylevel=colsample_bylevel,colsample_bynode=colsample_bynode,alpha=alpha,reg_lambda=reg_lambda,grow_policy=grow_policy, tree_method=tree_method,refresh_leaf=refresh_leaf,process_type=process_type, predictor= predictor,objective = 'reg:logistic')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return model, y_pred,metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## L1 one is good , l2 would be used\n",
    "## very good at f class recall\n",
    "## solver = ['newton-cg', 'lbfgs', 'liblinear'are all good, ensemble with all would be good\n",
    "\n",
    "## false dual is better\n",
    "def logisticRegress(X_train, y_train, X_test, y_test, penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, labels = [0,1,2,3]):\n",
    "    lr = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        # dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "    lr.fit(X_train, y_train.ravel())  \n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return lr,y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def ada(X_train, y_train, X_test, y_test,labels = [0,1,2,3]):\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10,  kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    ada = AdaBoostClassifier(base_estimator=svm_model_poly, n_estimators=50 )\n",
    "    ada.fit(X_train, y_train)  \n",
    "    y_pred = ada.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return ada, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "\n",
    "\n",
    "def svm_model_linear(X_train, y_train, X_test, y_test, C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3] ):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape=decision_function_shape, random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return svm_model_linear, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "def voting_ensemble(X_train, y_train, X_test,y_test, labels=[0,1,2,3]):\n",
    "    svm_model_linear = svm.SVC(C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                            coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                            coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, \n",
    "                                min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, \n",
    "                                max_features=50, max_leaf_nodes=None, class_weight='balanced')\n",
    "\n",
    "\n",
    "    lr = LogisticRegression( penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, \n",
    "                            class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn',\n",
    "                            verbose=0, warm_start=False, n_jobs=None)\n",
    "\n",
    "\n",
    "    xgb = XGBClassifier(max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, \n",
    "                        subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, \n",
    "                        reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, \n",
    "                        process_type='default', predictor= 'cpu_predictor',objective = 'reg:logistic')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[ ('xgb',xgb), ('lr',lr), ('svm_ln',svm_model_linear)], voting='hard')#, weights=[2,2,2,2])\n",
    "\n",
    "\n",
    "\n",
    "    #clf1 = clf1.fit(X, y)\n",
    "    #clf2 = clf2.fit(X, y)\n",
    "    #clf3 = clf3.fit(X, y)\n",
    "    eclf = eclf.fit(X_train, y_train)\n",
    "\n",
    "    predict = eclf.predict(X_test)\n",
    "    #scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "    #scores.mean()                             \n",
    "\n",
    "    print(confusion_matrix(y_test,predict, labels=labels))  \n",
    "    print(classification_report(y_test,predict, labels=labels))\n",
    "    #print(metric.get_metrics(predict.ravel(),y_test.ravel(),lb=labels))\n",
    "    \n",
    "    return eclf, predict#, metric.get_metrics(predict.ravel(),y_test.ravel(), lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(X_train, y_train, X_test, y_test, X_train_2,y_train_2, feature_rank ,score=0.01):   \n",
    "    score = 0.01\n",
    "\n",
    "    feature_good = feature_rank[feature_rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "    y_pred = svm_model_poly(X_train_2, y_train_2,X_test , y_test, labels=[0,1,2,3])[1]\n",
    "\n",
    "    X_test_n_1 =X_test[y_pred != 1]\n",
    "    y_test_n_1=y_test[y_pred != 1]\n",
    "\n",
    "    y_pred_n = y_pred[y_pred == 1]\n",
    "    y_true_1 = y_test[y_pred == 1]\n",
    "\n",
    "   \n",
    "    y_pred_voting_ensemble =voting_ensemble(np.asarray(X_train_2[rank[rank['rfscore'] >= score]['features'].values]), y_train_2,np.asarray( X_test_n_1[rank[rank['rfscore'] >= score]['features'].values]), np.asarray(y_test_n_1),  labels=[0,1,2,3])[1]\n",
    "\n",
    "    X_test_n_2 =X_test_n_1[y_pred_voting_ensemble != 2]\n",
    "    y_test_n_2=y_test_n_1[y_pred_voting_ensemble != 2]\n",
    "\n",
    "    y_pred_voting_ensemble_n = y_pred_voting_ensemble[y_pred_voting_ensemble == 2]\n",
    "    y_true_2 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "    y_pred_linear_D =Linear_D(np.asarray(X_train[feature_good]), y_test,np.asarray( X_test_n_2[feature_good]), y_test_n_2,  labels=[0,1,2,3])[1]\n",
    "\n",
    "    X_test_n_3 =X_test_n_2[y_pred_linear_D != 2]\n",
    "    y_test_n_3=y_test_n_2[y_pred_linear_D != 2]\n",
    "\n",
    "    y_pred_linear_D_2 = y_pred_linear_D[y_pred_linear_D == 2]\n",
    "    y_true_3 = y_test_n_2[y_pred_linear_D == 2]\n",
    "\n",
    "    #y_true_1 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "    X_test_n_3 =X_test_n_2[y_pred_linear_D != 0]\n",
    "    y_test_n_3=y_test_n_2[y_pred_linear_D != 0]\n",
    "\n",
    "    y_pred_linear_D_0 = y_pred_linear_D[y_pred_linear_D == 0]\n",
    "    y_true_4 = y_test_n_2[y_pred_linear_D == 0]\n",
    "\n",
    "    y_svm = svm_model_poly(X_train_2, y_train_2, X_test_n_3, y_test_n_3, labels=[0,1,2,3])[1]\n",
    "\n",
    "\n",
    "    X_test_n_4 =X_test_n_3[y_svm != 0]\n",
    "    y_test_n_4=y_test_n_3[y_svm != 0]\n",
    "\n",
    "    y_svm_n = y_svm[y_svm == 0]\n",
    "    y_true_5 = y_test_n_3[y_svm == 0]\n",
    "\n",
    "    y_v_e = svm_model_linear(X_train_2, y_train_2, X_test_n_4, y_test_n_4, labels=[0,1,2,3])[1]\n",
    "\n",
    "    y_final_pred = np.hstack((y_pred_n,y_pred_voting_ensemble_n, y_pred_linear_D_2,y_pred_linear_D_0,y_svm_n,y_v_e))\n",
    "    y_final_true = np.hstack((y_true_1,y_true_2,y_true_3,y_true_4,y_true_5,y_test_n_4))\n",
    "    \n",
    "    return y_final_pred, y_final_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
