{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])\n",
    "\n",
    "rank = pd.read_csv(\"database/features_ranking.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0.0075\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_var_1_clinic = post_features.normalised_values_multiples(good_features_X)\n",
    "norm_var_2_clinic = post_features.normalised_values_multiples(X_test)\n",
    "#norm_var_1_non = post_features.normalised_values_multiples(np_non_var_1)\n",
    "#norm_var_2_non = post_features.normalised_values_multiples(np_non_var_2)\n",
    "\n",
    "\n",
    "\n",
    "X_train_norm = np.hstack((good_features_X,norm_var_1_clinic))\n",
    "X_test_norm = np.hstack((X_test,norm_var_2_clinic))\n",
    "rank_norm = pd.read_csv(\"database/features_ranking_with_norm.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nimch681\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, test_labels=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "smote=SMOTE(ratio='auto')\n",
    "\n",
    "\n",
    "train, train_labels=smote.fit_sample(X_train, y_train)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = [\"R_duration\", \"R_height\", \"R_amp0\", \"R_amp1\",\"R_amp2\",\"R_amp3\", \"R_amp4\", \"R_amp5\", \"R_amp6\", \"R_amp7\", \"R_amp8\", \"R_amp9\", \"R_prominence\",\"R_areas\",\"Q_duration\", \"Q_height\", \"Q_amp0\", \"Q_amp1\",\"Q_amp2\",\"Q_amp3\", \"Q_amp4\", \"Q_amp5\", \"Q_amp6\", \"Q_amp7\", \"Q_amp8\", \"Q_amp9\", \"Q_prominence\",\"Q_areas\", \"S_duration\", \"S_height\", \"S_amp0\", \"S_amp1\",\"S_amp2\",\"S_amp3\", \"S_amp4\", \"S_amp5\", \"S_amp6\", \"S_amp7\", \"S_amp8\", \"S_amp9\", \"S_prominence\",\"S_areas\", \"P_duration\", \"P_height\", \"P_amp0\", \"P_amp1\",\"P_amp2\",\"P_amp3\", \"P_amp4\", \"P_amp5\", \"P_amp6\", \"P_amp7\", \"P_amp8\", \"P_amp9\", \"P_prominence\",\"P_areas\", \"P_neg_duration\", \"P_neg_height\", \"P_neg_amp0\", \"P_neg_amp1\",\"P_neg_amp2\",\"P_neg_amp3\", \"P_neg_amp4\", \"P_neg_amp5\", \"P_neg_amp6\", \"P_neg_amp7\", \"P_neg_amp8\", \"P_neg_amp9\", \"P_neg_prominence\",\"P_neg_areas\", \"T_duration\", \"T_height\", \"T_amp0\", \"T_amp1\",\"T_amp2\",\"T_amp3\", \"T_amp4\", \"T_amp5\", \"T_amp6\", \"T_amp7\", \"T_amp8\", \"T_amp9\", \"T_prominence\",\"T_areas\",\"T_neg_durations\",\"T_neg_height\", \"T_neg_amp0\", \"T_neg_amp1\", \"T_neg_amp2\", \"T_neg_amp3\", \"T_neg_amp4\", \"T_neg_amp5\", \"T_neg_amp6\", \"T_neg_amp7\", \"T_neg_amp8\", \"T_neg_amp9\",\"T_neg_prominence\",\"T_neg_areas\", \"rr_int_pre\", \"rr_int_post\", \"rr_int_10\", \"rr_int_50\", \"rr_int_all\", \"QRS_int\", \"QRS_int_10\", \"QRS_int_50\", \"PQ_int\", \"PQ_int_10\", \"PQ_int_50\", \"PR_int\", \"PR_int_10\", \"PR_int_50\", \"ST_int\", \"ST_int_10\", \"ST_int_50\", \"RT_int\", \"RT_int_10\", \"RT_int_50\", \"PT_int\", \"PT_int_10\", \"PT_int_50\", \"RP\",\"TR\",\"neg_RQ\", \"neg_PR\", \"neg_ST\", \"neg_RT\", \"neg_PT\", \"P_neg_T\", \"neg_P_neg_T\"]\n",
    "feat_labels_V = [\"R_duration_V\", \"R_height_V\", \"R_amp0_V\", \"R_amp1_V\",\"R_amp2_V\",\"R_amp3_V\", \"R_amp4_V\", \"R_amp5_V\", \"R_amp6_V\", \"R_amp7_V\", \"R_amp8_V\", \"R_amp9_V\", \"R_prominence_V\",\"R_areas_V\",\"Q_duration_V\", \"Q_height_V\", \"Q_amp0_V\", \"Q_amp1_V\",\"Q_amp2_V\",\"Q_amp3_V\", \"Q_amp4_V\", \"Q_amp5_V\", \"Q_amp6_V\", \"Q_amp7_V\", \"Q_amp8_V\", \"Q_amp9_V\", \"Q_prominence_V\",\"Q_areas_V\", \"S_duration_V\", \"S_height_V\", \"S_amp0_V\", \"S_amp1_V\",\"S_amp2_V\",\"S_amp3_V\", \"S_amp4_V\", \"S_amp5_V\", \"S_amp6_V\", \"S_amp7_V\", \"S_amp8_V\", \"S_amp9_V\", \"S_prominence_V\",\"S_areas_V\", \"P_duration_V\", \"P_height_V\", \"P_amp0_V\", \"P_amp1_V\",\"P_amp2_V\",\"P_amp3_V\", \"P_amp4_V\", \"P_amp5_V\", \"P_amp6_V\", \"P_amp7_V\", \"P_amp8_V\", \"P_amp9_V\", \"P_prominence_V\",\"P_areas_V\", \"P_neg_duration_V\", \"P_neg_height_V\", \"P_neg_amp0_V\", \"P_neg_amp1_V\",\"P_neg_amp2_V\",\"P_neg_amp3_V\", \"P_neg_amp4_V\", \"P_neg_amp5_V\", \"P_neg_amp6_V\", \"P_neg_amp7_V\", \"P_neg_amp8_V\", \"P_neg_amp9_V\", \"P_neg_prominence_V\",\"P_neg_areas_V\", \"T_duration_V\", \"T_height_V\", \"T_amp0_V\", \"T_amp1_V\",\"T_amp2_V\",\"T_amp3_V\", \"T_amp4_V\", \"T_amp5_V\", \"T_amp6_V\", \"T_amp7_V\", \"T_amp8_V\", \"T_amp9_V\", \"T_prominence_V\",\"T_areas_V\",\"T_neg_durations_V\",\"T_neg_height_V\", \"T_neg_amp0_V\", \"T_neg_amp1_V\", \"T_neg_amp2_V\", \"T_neg_amp3_V\", \"T_neg_amp4_V\", \"T_neg_amp5_V\", \"T_neg_amp6_V\", \"T_neg_amp7_V\", \"T_neg_amp8_V\", \"T_neg_amp9_V\",\"T_neg_prominence_V\",\"T_neg_areas_V\", \"rr_int_pre_V\", \"rr_int_post_V\", \"rr_int_10_V\", \"rr_int_50_V\", \"rr_int_all_V\", \"QRS_int_V\", \"QRS_int_10_V\", \"QRS_int_50_V\", \"PQ_int_V\", \"PQ_int_10_V\", \"PQ_int_50_V\", \"PR_int_V\", \"PR_int_10_V\", \"PR_int_50_V\", \"ST_int_V\", \"ST_int_10_V\", \"ST_int_50_V\", \"RT_int_V\", \"RT_int_10_V\", \"RT_int_50_V\", \"PT_int_V\", \"PT_int_10_V\", \"PT_int_50_V\", \"RP_V\",\"TR_V\",\"neg_RQ_V\", \"neg_PR_V\", \"neg_ST_V\", \"neg_RT_V\", \"neg_PT_V\", \"P_neg_T_V\", \"neg_P_neg_T_V\"]\n",
    "feat_labels_dtw = [\"dtw1\",\"dtw2\"]\n",
    "norm_dtw = [\"dtw1_v1\",\"dtw2_v1\"]\n",
    "classID = [\"classID\"]\n",
    "non_clinic = list(DB1_non_cli.iloc[:,0:140].columns.values)\n",
    "\n",
    "feature_norm_mill = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_mill.append(str(feat_labels[i]+\"_n_MLII\"))\n",
    "\n",
    "feature_norm_V1 = []\n",
    "for i in range(len(feat_labels)):\n",
    "    feature_norm_V1.append(str(feat_labels[i]+\"_n_V1\"))\n",
    "    \n",
    "\n",
    "c_ID = np.asarray(classID)\n",
    "f_M = np.asarray(feat_labels)\n",
    "f_V = np.asarray(feat_labels_V)\n",
    "f_d = np.asarray(feat_labels_dtw)\n",
    "non_cli = np.asarray(non_clinic)\n",
    "norm_mlii = np.asarray(feature_norm_mill)\n",
    "norm_v1 = np.asarray(feature_norm_V1)\n",
    "norm_dtw = np.asarray(norm_dtw)\n",
    "\n",
    "\n",
    "features_clinic = np.hstack((f_M,f_V, f_d))\n",
    "features_clinic_norm = np.hstack((f_M,f_V, f_d,norm_mlii,norm_v1,norm_dtw ))\n",
    "#row=[]\n",
    "#for i in range(0,len(np_clinic_new_1)):\n",
    "    #row.append(i)\n",
    "    \n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "#X_train_balanced_norm = pd.DataFrame(X_train_norm,columns=features_clinic_norm)\n",
    "#X_test_norm = pd.DataFrame(X_test_norm,columns=features_clinic_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b02a491ff8c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SVM\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msvm_model_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_features_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgood_features_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"XGboost\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_balanced\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rfscore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgood_features_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rfscore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-9292ec80a628>\u001b[0m in \u001b[0;36msvm_model_poly\u001b[1;34m(X_train, y_train, X_test, y_test, C, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, verbose, max_iter, decision_function_shape, random_state, labels)\u001b[0m\n\u001b[0;32m     15\u001b[0m                         max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0msvm_model_linear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_model_linear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "print(\"SVM\")\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "print(\"XGboost\")\n",
    "xgboost(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "print(\"Logistic\")\n",
    "logisticRegress(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values], good_features_y, X_test[rank[rank['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "print(\"Linear_D\")\n",
    "Linear_D(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values], good_features_y, X_test[rank[rank['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "print(\"SVM_LN\")\n",
    "svm_model_linear(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "print(\"voting_ensemble\")\n",
    "voting_ensemble(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "print(\"ada\")\n",
    "\n",
    "#ada(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"voting_ensemble\")\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n",
    "\n",
    "feature_good = rank[rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "\n",
    "#svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = 0.01\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "#xgboost(np.asarray(X_train[rank[rank['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "logisticRegress(X_train[rank[rank['rfscore'] >= score]['features'].values], y_train, X_test[rank[rank['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "Linear_D(X_train[rank[rank['rfscore'] >= score]['features'].values], y_train, X_test[rank[rank['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#svm_model_linear(np.asarray(X_train[rank[rank['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "#voting_ensemble(np.asarray(X_train[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "#ada(np.asarray(X_train[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test[rank[rank['rfscore'] >= score]['features'].values]), y_test,  labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "#voting_ensemble(good_features_X, good_features_y,np.asarray(X_test) , y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = 0.009\n",
    "#Linear_D(train, train_labels, test, test_labels, labels=['N','S','V','F'])\n",
    "svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])\n",
    "logisticRegress(X_train_balanced_norm, good_features_y,X_test_norm , y_test, labels=[0,1,2,3])\n",
    "\n",
    "xgboost(np.asarray(X_train_balanced_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values]), good_features_y, np.asarray(X_test_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values]), y_test, labels=[0,1,2,3])\n",
    "#svm_model_poly(X_train_df[df[df['rfscore'] >= score]['features'].values], good_features_y,X_test_df[df[df['rfscore'] >= score]['features'].values] , y_test, labels=[0,1,2,3])[0]\n",
    "logisticRegress(X_train_balanced_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], good_features_y, X_test_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], y_test, labels=[0,1,2,3])\n",
    "Linear_D(X_train_balanced_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], good_features_y, X_test_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n",
    "#ada(X_train_df[rank['features'].values], good_features_y, X_test_df[rank['features'].values], y_test,  labels=[0,1,2,3])\n",
    "svm_model_linear(X_train_balanced_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], good_features_y, X_test_norm[rank_norm[rank_norm['rfscore'] >= score]['features'].values], y_test,  labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_model_linear(X_train_balanced[feature_good], good_features_y, X_test[feature_good], y_test, labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "svm_model_linear(X_train_balanced, good_features_y, X_test, y_test, labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nimch681\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31766  5906   340  6009]\n",
      " [  511  1352   114    73]\n",
      " [   54   206  2591   369]\n",
      " [   19     0     4   365]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.72      0.83     44021\n",
      "           1       0.18      0.66      0.28      2050\n",
      "           2       0.85      0.80      0.83      3220\n",
      "           3       0.05      0.94      0.10       388\n",
      "\n",
      "    accuracy                           0.73     49679\n",
      "   macro avg       0.52      0.78      0.51     49679\n",
      "weighted avg       0.93      0.73      0.80     49679\n",
      "\n",
      "[0.33486584151001647, 2.4950935156072864, 0.47931961020591907]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nimch681\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31589  3716  3748  4968]\n",
      " [ 1193   651    83   123]\n",
      " [  115   277  2608   220]\n",
      " [   23     3     8   354]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.82     44021\n",
      "           1       0.14      0.32      0.19      2050\n",
      "           2       0.40      0.81      0.54      3220\n",
      "           3       0.06      0.91      0.12       388\n",
      "\n",
      "    accuracy                           0.71     49679\n",
      "   macro avg       0.39      0.69      0.42     49679\n",
      "weighted avg       0.88      0.71      0.77     49679\n",
      "\n",
      "[0.2708426691398277, 1.6721184831048712, 0.34443614495802277]\n"
     ]
    }
   ],
   "source": [
    "y_pred = Linear_D(X_train[feature_good], y_train,X_test[feature_good] , y_test, labels = [0,1,2,3])[1]\n",
    "#y_pred = Linear_D(X_train_balanced[feature_good], good_features_y,X_test[feature_good] , y_test, labels = [0,1,2,3])[1]\n",
    "y_pred = Linear_D(X_train, y_train,X_test , y_test, labels = [0,1,2,3])[1]\n",
    "svm_model_linear(X_train, y_train,X_test , y_test, labels = [0,1,2,3])\n",
    "svm_model_linear(X_train[feature_good], y_train,X_test[feature_good] , y_test, labels = [0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = svm_model_linear(X_train[feature_good], y_train,X_test[feature_good] , y_test, labels  0,1,2,3])[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model_poly(X_train[feature_good], y_train,X_test[feature_good] , y_test, labels=[0,1,2,3])[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = svm_model_poly(good_features_X, good_features_y,X_test , y_test, labels=[0,1,2,3])[1]\n",
    "\n",
    "X_test_n_1 =X_test[y_pred != 1]\n",
    "y_test_n_1=y_test[y_pred != 1]\n",
    "\n",
    "y_pred_n = y_pred[y_pred == 1]\n",
    "y_true_1 = y_test[y_pred == 1]\n",
    "\n",
    "\n",
    "y_pred_voting_ensemble =voting_ensemble(np.asarray(X_train_balanced[rank[rank['rfscore'] >= score]['features'].values]), good_features_y,np.asarray( X_test_n_1[rank[rank['rfscore'] >= score]['features'].values]), np.asarray(y_test_n_1),  labels=[0,1,2,3])[1]\n",
    "\n",
    "X_test_n_2 =X_test_n_1[y_pred_voting_ensemble != 2]\n",
    "y_test_n_2=y_test_n_1[y_pred_voting_ensemble != 2]\n",
    "\n",
    "y_pred_voting_ensemble_n = y_pred_voting_ensemble[y_pred_voting_ensemble == 2]\n",
    "y_true_2 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "y_pred_linear_D =Linear_D(np.asarray(X_train[feature_good]), y_train,np.asarray( X_test_n_2[feature_good]), y_test_n_2,  labels=[0,1,2,3])[1]\n",
    "\n",
    "X_test_n_3 =X_test_n_2[y_pred_linear_D != 2]\n",
    "y_test_n_3=y_test_n_2[y_pred_linear_D != 2]\n",
    "\n",
    "y_pred_linear_D_2 = y_pred_linear_D[y_pred_linear_D == 2]\n",
    "y_true_3 = y_test_n_2[y_pred_linear_D == 2]\n",
    "\n",
    "#y_true_1 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "X_test_n_3 =X_test_n_2[y_pred_linear_D != 0]\n",
    "y_test_n_3=y_test_n_2[y_pred_linear_D != 0]\n",
    "\n",
    "y_pred_linear_D_0 = y_pred_linear_D[y_pred_linear_D == 0]\n",
    "y_true_4 = y_test_n_2[y_pred_linear_D == 0]\n",
    "\n",
    "y_svm = svm_model_poly(X_train_balanced, good_features_y, X_test_n_3, y_test_n_3, labels=[0,1,2,3])[1]\n",
    "\n",
    "\n",
    "X_test_n_4 =X_test_n_3[y_svm != 0]\n",
    "y_test_n_4=y_test_n_3[y_svm != 0]\n",
    "\n",
    "y_svm_n = y_svm[y_svm == 0]\n",
    "y_true_5 = y_test_n_3[y_svm == 0]\n",
    "\n",
    "y_v_e = Linear_D(X_train, y_train, X_test_n_4, y_test_n_4, labels=[0,1,2,3])[1]\n",
    "\n",
    "y_final_pred = np.hstack((y_pred_n,y_pred_voting_ensemble_n, y_pred_linear_D_2,y_pred_linear_D_0,y_svm_n,y_v_e))\n",
    "y_final_true = np.hstack((y_true_1,y_true_2,y_true_3,y_true_4,y_true_5,y_test_n_4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_final_true,y_final_pred, labels=[0,1,2,3]))  \n",
    "print(classification_report(y_final_true,y_final_pred, labels=[0,1,2,3]))\n",
    "print(metric.get_metrics(y_final_true,y_final_pred, lb=[0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "## small C from 1 to 3 seems to be good for f class recalled\n",
    "## degree 3 is better all around\n",
    "## Tol at 0.01 seems best\n",
    "\n",
    "def svm_model_poly(X_train, y_train, X_test, y_test, C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                        coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3]):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape='ovo', random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(metric.get_metrics(y_pred,y_test,lb=labels))\n",
    "    \n",
    "    return svm_model_linear,y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def Linear_D(X_train, y_train, X_test, y_test, labels = [0,1,2,3]):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X_train, y_train)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred, labels = labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return clf,y_pred, metric.get_metrics(y_pred,y_test,lb=labels)[2]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "## numner of estimators leaves at 1000 \n",
    "\n",
    "def randomForest(X_train, y_train, X_test, y_test,n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, max_features=50, max_leaf_nodes=None, class_weight='balanced', labels = [0,1,2,3]):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,min_weight_fraction_leaf=min_weight_fraction_leaf, class_weight=class_weight, min_impurity_decrease=min_impurity_decrease,max_features=max_features)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)    \n",
    "    print(confusion_matrix(y_test,y_pred, labels=labels))  \n",
    "    print(classification_report(y_test,y_pred, labels=labels))\n",
    "    print(metric.get_metrics(y_pred,y_test, lb=labels))\n",
    "    \n",
    "    return rf, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "### xgbo is good for normal classes too\n",
    "## do this one next \n",
    "## XGB is very goos for V class\n",
    "## XGboost tree classifcation is good with 5 or more tree depth\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def xgboost(X_train,y_train, X_test, y_test, max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, process_type='default', predictor= 'cpu_predictor', labels = [0,1,2,3]): \n",
    "    model = XGBClassifier(max_depth = max_depth, eta = eta, gamma = gamma,min_child_weight=min_child_weight,subsample=subsample, max_delta_step= max_delta_step,colsample_bytree=colsample_bytree,colsample_bylevel=colsample_bylevel,colsample_bynode=colsample_bynode,alpha=alpha,reg_lambda=reg_lambda,grow_policy=grow_policy, tree_method=tree_method,refresh_leaf=refresh_leaf,process_type=process_type, predictor= predictor,objective = 'reg:logistic')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return model, y_pred,metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## L1 one is good , l2 would be used\n",
    "## very good at f class recall\n",
    "## solver = ['newton-cg', 'lbfgs', 'liblinear'are all good, ensemble with all would be good\n",
    "\n",
    "## false dual is better\n",
    "def logisticRegress(X_train, y_train, X_test, y_test, penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, labels = [0,1,2,3]):\n",
    "    lr = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)\n",
    "        # dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "    lr.fit(X_train, y_train.ravel())  \n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return lr,y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "def ada(X_train, y_train, X_test, y_test,labels = [0,1,2,3]):\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10,  kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    ada = AdaBoostClassifier(base_estimator=svm_model_poly, n_estimators=50 )\n",
    "    ada.fit(X_train, y_train)  \n",
    "    y_pred = ada.predict(X_test)\n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return ada, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "## Number of feature at 50 or 100 is good too\n",
    "\n",
    "\n",
    "def svm_model_linear(X_train, y_train, X_test, y_test, C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                        coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                        cache_size=200, verbose=False, \n",
    "                        max_iter=-1, decision_function_shape='ovo', random_state=None, labels = [0,1,2,3] ):\n",
    "    \n",
    "    svm_model_linear = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, \n",
    "                        coef0=coef0, shrinking=shrinking, probability=probability, tol=tol, \n",
    "                        cache_size=cache_size, verbose=verbose, \n",
    "                        max_iter=max_iter, decision_function_shape=decision_function_shape, random_state=random_state) \n",
    "    \n",
    "    svm_model_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_model_linear.predict(X_test)    \n",
    "    print(confusion_matrix(y_test.ravel(),y_pred.ravel(), labels=labels))  \n",
    "    print(classification_report(y_test.ravel(),y_pred.ravel(), labels=labels))\n",
    "    print(metric.get_metrics(y_pred.ravel(),y_test.ravel(), lb=labels))\n",
    "    \n",
    "    return svm_model_linear, y_pred, metric.get_metrics(y_pred,y_test, lb=labels)[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "def voting_ensemble(X_train, y_train, X_test,y_test, labels=[0,1,2,3]):\n",
    "    svm_model_linear = svm.SVC(C=10, kernel='linear', degree=3, gamma='auto', \n",
    "                            coef0=0.0, shrinking=True, probability=True, tol=0.1, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "    svm_model_poly = svm.SVC(C=10, kernel='poly', degree=3, gamma='auto', \n",
    "                            coef0=0.001, shrinking=True, probability=True, tol=0.01, \n",
    "                            cache_size=200, verbose=False, \n",
    "                            max_iter=-1, decision_function_shape='ovo', random_state=None)\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=16, min_samples_split=2, \n",
    "                                min_samples_leaf=3, min_weight_fraction_leaf=0.0001,min_impurity_decrease=0.0, \n",
    "                                max_features=50, max_leaf_nodes=None, class_weight='balanced')\n",
    "\n",
    "\n",
    "    lr = LogisticRegression( penalty='l2', dual=False, tol=0.0001, C=100, fit_intercept=True, intercept_scaling=10, \n",
    "                            class_weight='balanced', random_state=None, solver='warn', max_iter=100, multi_class='warn',\n",
    "                            verbose=0, warm_start=False, n_jobs=None)\n",
    "\n",
    "\n",
    "    xgb = XGBClassifier(max_depth = 8, eta = 1, gamma = 0.001, min_child_weight=0.1,max_delta_step=10, \n",
    "                        subsample=0.6,colsample_bytree = 0.7, colsample_bylevel= 1, colsample_bynode=1, alpha=0, \n",
    "                        reg_lambda= 1, tree_method='exact', grow_policy='depthwise', refresh_leaf=True, \n",
    "                        process_type='default', predictor= 'cpu_predictor',objective = 'reg:logistic')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    eclf = VotingClassifier(estimators=[ ('xgb',xgb), ('lr',lr), ('svm_ln',svm_model_linear)], voting='hard')#, weights=[2,2,2,2])\n",
    "\n",
    "\n",
    "\n",
    "    #clf1 = clf1.fit(X, y)\n",
    "    #clf2 = clf2.fit(X, y)\n",
    "    #clf3 = clf3.fit(X, y)\n",
    "    eclf = eclf.fit(X_train, y_train)\n",
    "\n",
    "    predict = eclf.predict(X_test)\n",
    "    #scores = cross_val_score(X_train, y_train.data, iris.target, cv=10)\n",
    "    #scores.mean()                             \n",
    "\n",
    "    print(confusion_matrix(y_test,predict, labels=labels))  \n",
    "    print(classification_report(y_test,predict, labels=labels))\n",
    "    print(metric.get_metrics(predict.ravel(),y_test.ravel(),lb=labels))\n",
    "    \n",
    "    return eclf, predict, metric.get_metrics(predict.ravel(),y_test.ravel(), lb=labels)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(X_train, y_train, X_test, y_test, X_train_2,y_train_2, feature_rank ,score=0.01):   \n",
    "    score = 0.01\n",
    "\n",
    "    feature_good = feature_rank[feature_rank['rfscore'] >= score]['features'].values\n",
    "\n",
    "    y_pred = svm_model_poly(X_train_2, y_train_2,X_test , y_test, labels=[0,1,2,3])[1]\n",
    "\n",
    "    X_test_n_1 =X_test[y_pred != 1]\n",
    "    y_test_n_1=y_test[y_pred != 1]\n",
    "\n",
    "    y_pred_n = y_pred[y_pred == 1]\n",
    "    y_true_1 = y_test[y_pred == 1]\n",
    "\n",
    "   \n",
    "    y_pred_voting_ensemble =voting_ensemble(np.asarray(X_train_2[rank[rank['rfscore'] >= score]['features'].values]), y_train_2,np.asarray( X_test_n_1[rank[rank['rfscore'] >= score]['features'].values]), np.asarray(y_test_n_1),  labels=[0,1,2,3])[1]\n",
    "\n",
    "    X_test_n_2 =X_test_n_1[y_pred_voting_ensemble != 2]\n",
    "    y_test_n_2=y_test_n_1[y_pred_voting_ensemble != 2]\n",
    "\n",
    "    y_pred_voting_ensemble_n = y_pred_voting_ensemble[y_pred_voting_ensemble == 2]\n",
    "    y_true_2 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "    y_pred_linear_D =Linear_D(np.asarray(X_train[feature_good]), y_test,np.asarray( X_test_n_2[feature_good]), y_test_n_2,  labels=[0,1,2,3])[1]\n",
    "\n",
    "    X_test_n_3 =X_test_n_2[y_pred_linear_D != 2]\n",
    "    y_test_n_3=y_test_n_2[y_pred_linear_D != 2]\n",
    "\n",
    "    y_pred_linear_D_2 = y_pred_linear_D[y_pred_linear_D == 2]\n",
    "    y_true_3 = y_test_n_2[y_pred_linear_D == 2]\n",
    "\n",
    "    #y_true_1 = y_test_n_1[y_pred_voting_ensemble == 2]\n",
    "\n",
    "\n",
    "    X_test_n_3 =X_test_n_2[y_pred_linear_D != 0]\n",
    "    y_test_n_3=y_test_n_2[y_pred_linear_D != 0]\n",
    "\n",
    "    y_pred_linear_D_0 = y_pred_linear_D[y_pred_linear_D == 0]\n",
    "    y_true_4 = y_test_n_2[y_pred_linear_D == 0]\n",
    "\n",
    "    y_svm = svm_model_poly(X_train_2, y_train_2, X_test_n_3, y_test_n_3, labels=[0,1,2,3])[1]\n",
    "\n",
    "\n",
    "    X_test_n_4 =X_test_n_3[y_svm != 0]\n",
    "    y_test_n_4=y_test_n_3[y_svm != 0]\n",
    "\n",
    "    y_svm_n = y_svm[y_svm == 0]\n",
    "    y_true_5 = y_test_n_3[y_svm == 0]\n",
    "\n",
    "    y_v_e = svm_model_linear(X_train_2, y_train_2, X_test_n_4, y_test_n_4, labels=[0,1,2,3])[1]\n",
    "\n",
    "    y_final_pred = np.hstack((y_pred_n,y_pred_voting_ensemble_n, y_pred_linear_D_2,y_pred_linear_D_0,y_svm_n,y_v_e))\n",
    "    y_final_true = np.hstack((y_true_1,y_true_2,y_true_3,y_true_4,y_true_5,y_test_n_4))\n",
    "    \n",
    "    return y_final_pred, y_final_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
