{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes.python import features_columns as col\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "from codes.python import TunedClassifier as classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n",
    "\n",
    "\n",
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])\n",
    "\n",
    "\n",
    "\n",
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "import pickle\n",
    "f = open('database/nvs_log.text', 'rb')\n",
    "n_s = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/nvv_log.text', 'rb')\n",
    "n_v = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/nvf_log.text', 'rb')\n",
    "n_f = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/svv_log.text', 'rb')\n",
    "s_v = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/svf_log.text', 'rb')\n",
    "s_f = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/vvf_log.text', 'rb')\n",
    "v_f = pickle.load(f)[1]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log\n",
      "all\n",
      "n_s\n",
      "[[41636   977   889   519]\n",
      " [ 1117   431   461    41]\n",
      " [  249   150  2524   297]\n",
      " [  172     0     6   210]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96     44021\n",
      "           1       0.28      0.21      0.24      2050\n",
      "           2       0.65      0.78      0.71      3220\n",
      "           3       0.20      0.54      0.29       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.52      0.62      0.55     49679\n",
      "weighted avg       0.91      0.90      0.90     49679\n",
      "\n",
      "[0.560462419836804, 1.9212470117691272, 0.5203870863895429]\n",
      "n_v\n",
      "[[42258   511   840   412]\n",
      " [  963    96   924    67]\n",
      " [  645   130  2287   158]\n",
      " [  380     0     7     1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     44021\n",
      "           1       0.13      0.05      0.07      2050\n",
      "           2       0.56      0.71      0.63      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.41      0.43      0.41     49679\n",
      "weighted avg       0.89      0.90      0.89     49679\n",
      "\n",
      "[0.50490525571265, 1.4509136346964069, 0.43381683219337586]\n",
      "n_f\n",
      "[[39880   850   960  2331]\n",
      " [ 1837   160    14    39]\n",
      " [  214    55  2799   152]\n",
      " [  139     1    45   203]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93     44021\n",
      "           1       0.15      0.08      0.10      2050\n",
      "           2       0.73      0.87      0.80      3220\n",
      "           3       0.07      0.52      0.13       388\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     49679\n",
      "   macro avg       0.48      0.59      0.49     49679\n",
      "weighted avg       0.89      0.87      0.88     49679\n",
      "\n",
      "[0.4509277772797045, 1.8305035859003587, 0.45427683687739706]\n",
      "s_v\n",
      "[[39880   850   960  2331]\n",
      " [ 1837   160    14    39]\n",
      " [  214    55  2799   152]\n",
      " [  139     1    45   203]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93     44021\n",
      "           1       0.15      0.08      0.10      2050\n",
      "           2       0.73      0.87      0.80      3220\n",
      "           3       0.07      0.52      0.13       388\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     49679\n",
      "   macro avg       0.48      0.59      0.49     49679\n",
      "weighted avg       0.89      0.87      0.88     49679\n",
      "\n",
      "[0.4509277772797045, 1.8305035859003587, 0.45427683687739706]\n",
      "s_f\n",
      "[[39611  1317  1392  1701]\n",
      " [ 1821   163    14    52]\n",
      " [  790    74  2155   201]\n",
      " [   57     1     9   321]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92     44021\n",
      "           1       0.10      0.08      0.09      2050\n",
      "           2       0.60      0.67      0.63      3220\n",
      "           3       0.14      0.83      0.24       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.45      0.62      0.47     49679\n",
      "weighted avg       0.87      0.85      0.86     49679\n",
      "\n",
      "[0.3758127231078974, 1.4572314612150792, 0.3700602942058336]\n",
      "v_f\n",
      "[[39847   660   540  2974]\n",
      " [  727   131  1122    70]\n",
      " [  337   222  2408   253]\n",
      " [   33     2     0   353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94     44021\n",
      "           1       0.13      0.06      0.09      2050\n",
      "           2       0.59      0.75      0.66      3220\n",
      "           3       0.10      0.91      0.17       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.45      0.66      0.46     49679\n",
      "weighted avg       0.91      0.86      0.88     49679\n",
      "\n",
      "[0.468767835912084, 1.5324387570359708, 0.4259387625855383]\n",
      "Lin\n",
      "all\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42400   462  1040   119]\n",
      " [ 1611    60   379     0]\n",
      " [ 1043   176  1832   169]\n",
      " [  202     0    60   126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     44021\n",
      "           1       0.09      0.03      0.04      2050\n",
      "           2       0.55      0.57      0.56      3220\n",
      "           3       0.30      0.32      0.31       388\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     49679\n",
      "   macro avg       0.47      0.47      0.47     49679\n",
      "weighted avg       0.87      0.89      0.88     49679\n",
      "\n",
      "[0.43615731933906016, 1.2374794354069492, 0.37276358909539875]\n",
      "n_v\n",
      "[[43360   229   352    80]\n",
      " [ 1519    35   463    33]\n",
      " [ 1448    38  1669    65]\n",
      " [  381     0     7     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96     44021\n",
      "           1       0.12      0.02      0.03      2050\n",
      "           2       0.67      0.52      0.58      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49679\n",
      "   macro avg       0.43      0.38      0.39     49679\n",
      "weighted avg       0.87      0.91      0.89     49679\n",
      "\n",
      "[0.4313224958380087, 1.3213022351893482, 0.38082402731767284]\n",
      "n_f\n",
      "[[39761    21   123  4116]\n",
      " [ 2004     1     5    40]\n",
      " [  343    80  2420   377]\n",
      " [   23     0     6   359]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92     44021\n",
      "           1       0.01      0.00      0.00      2050\n",
      "           2       0.95      0.75      0.84      3220\n",
      "           3       0.07      0.93      0.14       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.49      0.65      0.47     49679\n",
      "weighted avg       0.90      0.86      0.87     49679\n",
      "\n",
      "[0.4119485058497267, 1.709377802605375, 0.41964647825053525]\n",
      "s_v\n",
      "[[39761    21   123  4116]\n",
      " [ 2004     1     5    40]\n",
      " [  343    80  2420   377]\n",
      " [   23     0     6   359]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92     44021\n",
      "           1       0.01      0.00      0.00      2050\n",
      "           2       0.95      0.75      0.84      3220\n",
      "           3       0.07      0.93      0.14       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.49      0.65      0.47     49679\n",
      "weighted avg       0.90      0.86      0.87     49679\n",
      "\n",
      "[0.4119485058497267, 1.709377802605375, 0.41964647825053525]\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42973    61   415   572]\n",
      " [ 2025     7    10     8]\n",
      " [  983    26  2037   174]\n",
      " [   38     0     6   344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95     44021\n",
      "           1       0.07      0.00      0.01      2050\n",
      "           2       0.83      0.63      0.72      3220\n",
      "           3       0.31      0.89      0.46       388\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49679\n",
      "   macro avg       0.54      0.62      0.54     49679\n",
      "weighted avg       0.89      0.91      0.90     49679\n",
      "\n",
      "[0.5053122031734347, 1.535856082652062, 0.4446381119182251]\n",
      "v_f\n",
      "[[43123   139   231   528]\n",
      " [ 1755    40   240    15]\n",
      " [  616   119  2271   214]\n",
      " [   49     0     0   339]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96     44021\n",
      "           1       0.13      0.02      0.03      2050\n",
      "           2       0.83      0.71      0.76      3220\n",
      "           3       0.31      0.87      0.46       388\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     49679\n",
      "   macro avg       0.55      0.64      0.55     49679\n",
      "weighted avg       0.90      0.92      0.91     49679\n",
      "\n",
      "[0.5719153410322441, 1.6872474572629783, 0.4968636026739943]\n",
      "Random\n",
      "all\n",
      "n_s\n",
      "[[38412   671  4834   104]\n",
      " [ 1343    81   626     0]\n",
      " [  405    31  2782     2]\n",
      " [  275     2   111     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91     44021\n",
      "           1       0.10      0.04      0.06      2050\n",
      "           2       0.33      0.86      0.48      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     49679\n",
      "   macro avg       0.35      0.44      0.36     49679\n",
      "weighted avg       0.87      0.83      0.84     49679\n",
      "\n",
      "[0.366909338044258, 1.3397260563547675, 0.35092042606647494]\n",
      "n_v\n",
      "[[41614   964  1374    69]\n",
      " [ 1368   104   577     1]\n",
      " [  288   147  2782     3]\n",
      " [  361     1    23     3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     44021\n",
      "           1       0.09      0.05      0.06      2050\n",
      "           2       0.58      0.86      0.70      3220\n",
      "           3       0.04      0.01      0.01       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.42      0.47      0.43     49679\n",
      "weighted avg       0.89      0.90      0.89     49679\n",
      "\n",
      "[0.514362201051736, 1.5851785105979928, 0.4553284143506171]\n",
      "n_f\n",
      "[[35806  1154  7059     2]\n",
      " [ 1445   179   426     0]\n",
      " [  434     7  2779     0]\n",
      " [   17     0   371     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.88     44021\n",
      "           1       0.13      0.09      0.11      2050\n",
      "           2       0.26      0.86      0.40      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     49679\n",
      "   macro avg       0.34      0.44      0.35     49679\n",
      "weighted avg       0.86      0.78      0.81     49679\n",
      "\n",
      "[0.29699907117545593, 1.3452496461554433, 0.31665574135715835]\n",
      "s_v\n",
      "[[35665  1074  7278     4]\n",
      " [ 1480   173   397     0]\n",
      " [  406     6  2808     0]\n",
      " [   17     0   371     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87     44021\n",
      "           1       0.14      0.08      0.10      2050\n",
      "           2       0.26      0.87      0.40      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     49679\n",
      "   macro avg       0.34      0.44      0.34     49679\n",
      "weighted avg       0.86      0.78      0.80     49679\n",
      "\n",
      "[0.29431315340541453, 1.3532150362804636, 0.3163084562377652]\n",
      "s_f\n",
      "[[37697  1115  3981  1228]\n",
      " [ 1734   223    83    10]\n",
      " [  379    28  2797    16]\n",
      " [   22     1   365     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     44021\n",
      "           1       0.16      0.11      0.13      2050\n",
      "           2       0.39      0.87      0.54      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.82      0.82      0.82     49679\n",
      "   macro avg       0.37      0.46      0.39     49679\n",
      "weighted avg       0.87      0.82      0.84     49679\n",
      "\n",
      "[0.352875090046802, 1.5276194252126802, 0.367389973174986]\n",
      "v_f\n",
      "[[39128  1760   558  2575]\n",
      " [ 1450    96   501     3]\n",
      " [  175    23  2960    62]\n",
      " [   28    70     8   282]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92     44021\n",
      "           1       0.05      0.05      0.05      2050\n",
      "           2       0.74      0.92      0.82      3220\n",
      "           3       0.10      0.73      0.17       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.46      0.65      0.49     49679\n",
      "weighted avg       0.90      0.85      0.87     49679\n",
      "\n",
      "[0.4526622752458285, 1.7503784456016689, 0.44512844332312285]\n",
      "ensamble ln\n",
      "all\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42867   454   608    92]\n",
      " [ 1613    62   375     0]\n",
      " [ 1064   190  1818   148]\n",
      " [  224     0    47   117]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95     44021\n",
      "           1       0.09      0.03      0.04      2050\n",
      "           2       0.64      0.56      0.60      3220\n",
      "           3       0.33      0.30      0.31       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.50      0.47      0.48     49679\n",
      "weighted avg       0.88      0.90      0.89     49679\n",
      "\n",
      "[0.4594172230813849, 1.3210015692440162, 0.39483380769619447]\n",
      "n_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43459   240   302    20]\n",
      " [ 1536    43   462     9]\n",
      " [ 1472    56  1656    36]\n",
      " [  385     0     3     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96     44021\n",
      "           1       0.13      0.02      0.04      2050\n",
      "           2       0.68      0.51      0.59      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49679\n",
      "   macro avg       0.43      0.38      0.39     49679\n",
      "weighted avg       0.87      0.91      0.89     49679\n",
      "\n",
      "[0.43427276252199265, 1.3455552501214054, 0.385330787526172]\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41945    43   195  1838]\n",
      " [ 2006     2     9    33]\n",
      " [  368    84  2662   106]\n",
      " [  144     1    42   201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95     44021\n",
      "           1       0.02      0.00      0.00      2050\n",
      "           2       0.92      0.83      0.87      3220\n",
      "           3       0.09      0.52      0.16       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.49      0.57      0.49     49679\n",
      "weighted avg       0.90      0.90      0.90     49679\n",
      "\n",
      "[0.516440592574184, 1.7584740768413119, 0.47802955589225593]\n",
      "s_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41945    43   195  1838]\n",
      " [ 2006     2     9    33]\n",
      " [  368    84  2662   106]\n",
      " [  144     1    42   201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95     44021\n",
      "           1       0.02      0.00      0.00      2050\n",
      "           2       0.92      0.83      0.87      3220\n",
      "           3       0.09      0.52      0.16       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.49      0.57      0.49     49679\n",
      "weighted avg       0.90      0.90      0.90     49679\n",
      "\n",
      "[0.516440592574184, 1.7584740768413119, 0.47802955589225593]\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43109    60   529   323]\n",
      " [ 2028     7    10     5]\n",
      " [ 1007    45  2005   163]\n",
      " [   68     0     6   314]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96     44021\n",
      "           1       0.06      0.00      0.01      2050\n",
      "           2       0.79      0.62      0.69      3220\n",
      "           3       0.39      0.81      0.53       388\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49679\n",
      "   macro avg       0.54      0.60      0.55     49679\n",
      "weighted avg       0.88      0.91      0.90     49679\n",
      "\n",
      "[0.503854905620098, 1.4748599514036793, 0.43628494673550894]\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43184   148   208   481]\n",
      " [ 1757    44   238    11]\n",
      " [  643   182  2193   202]\n",
      " [   49     0     0   339]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96     44021\n",
      "           1       0.12      0.02      0.04      2050\n",
      "           2       0.83      0.68      0.75      3220\n",
      "           3       0.33      0.87      0.48       388\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     49679\n",
      "   macro avg       0.56      0.64      0.56     49679\n",
      "weighted avg       0.90      0.92      0.91     49679\n",
      "\n",
      "[0.5668965080645184, 1.651162963696073, 0.4898436244942683]\n",
      "ensamble ln\n",
      "all\n",
      "n_s\n",
      "[[32976  2679  1931  6435]\n",
      " [  210  1465   354    21]\n",
      " [  116   428  2462   214]\n",
      " [   83     2     4   299]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.75      0.85     44021\n",
      "           1       0.32      0.71      0.44      2050\n",
      "           2       0.52      0.76      0.62      3220\n",
      "           3       0.04      0.77      0.08       388\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     49679\n",
      "   macro avg       0.47      0.75      0.50     49679\n",
      "weighted avg       0.92      0.75      0.81     49679\n",
      "\n",
      "[0.3616316226500116, 2.3177257006305454, 0.47053152390382397]\n",
      "n_v\n",
      "[[37873  2595  1006  2547]\n",
      " [  587   488   903    72]\n",
      " [  242   877  1948   153]\n",
      " [  295    65     5    23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91     44021\n",
      "           1       0.12      0.24      0.16      2050\n",
      "           2       0.50      0.60      0.55      3220\n",
      "           3       0.01      0.06      0.01       388\n",
      "\n",
      "   micro avg       0.81      0.81      0.81     49679\n",
      "   macro avg       0.40      0.44      0.41     49679\n",
      "weighted avg       0.90      0.81      0.85     49679\n",
      "\n",
      "[0.3635077074380554, 1.4686618249310341, 0.36533658183540696]\n",
      "n_f\n",
      "[[27108  5805  5920  5188]\n",
      " [ 1499   482    25    44]\n",
      " [  246   356  2426   192]\n",
      " [   15     4    15   354]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.62      0.74     44021\n",
      "           1       0.07      0.24      0.11      2050\n",
      "           2       0.29      0.75      0.42      3220\n",
      "           3       0.06      0.91      0.11       388\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     49679\n",
      "   macro avg       0.34      0.63      0.35     49679\n",
      "weighted avg       0.85      0.61      0.69     49679\n",
      "\n",
      "[0.16899954627417563, 1.3503436929440533, 0.2532927347550945]\n",
      "s_v\n",
      "[[27108  5805  5920  5188]\n",
      " [ 1499   482    25    44]\n",
      " [  246   356  2426   192]\n",
      " [   15     4    15   354]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.62      0.74     44021\n",
      "           1       0.07      0.24      0.11      2050\n",
      "           2       0.29      0.75      0.42      3220\n",
      "           3       0.06      0.91      0.11       388\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     49679\n",
      "   macro avg       0.34      0.63      0.35     49679\n",
      "weighted avg       0.85      0.61      0.69     49679\n",
      "\n",
      "[0.16899954627417563, 1.3503436929440533, 0.2532927347550945]\n",
      "s_f\n",
      "[[26967  7560  4111  5383]\n",
      " [ 1504   479    21    46]\n",
      " [  779   718  1536   187]\n",
      " [   16     9     7   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.61      0.74     44021\n",
      "           1       0.05      0.23      0.09      2050\n",
      "           2       0.27      0.48      0.35      3220\n",
      "           3       0.06      0.92      0.11       388\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     49679\n",
      "   macro avg       0.33      0.56      0.32     49679\n",
      "weighted avg       0.84      0.59      0.68     49679\n",
      "\n",
      "[0.1144512209462227, 1.035980901703792, 0.18672322318608536]\n",
      "v_f\n",
      "[[30745  6045  1259  5972]\n",
      " [  817   759   407    67]\n",
      " [  148   557  2214   301]\n",
      " [   18     7     1   362]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.70      0.81     44021\n",
      "           1       0.10      0.37      0.16      2050\n",
      "           2       0.57      0.69      0.62      3220\n",
      "           3       0.05      0.93      0.10       388\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     49679\n",
      "   macro avg       0.42      0.67      0.42     49679\n",
      "weighted avg       0.90      0.69      0.77     49679\n",
      "\n",
      "[0.2556523186107291, 1.73130609946325, 0.3442394217382708]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
       "   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "   tol=0.1, verbose=False),\n",
       " SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
       "   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "   tol=0.1, verbose=False),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " [0.2556523186107291, 1.73130609946325, 0.3442394217382708])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "#classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "#classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random\")\n",
    "print(\"all\")\n",
    "#classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ensamble ln\")\n",
    "print(\"all\")\n",
    "#classifier.voting_ensemble_lin(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble_lin(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble_lin(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble_lin(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble_lin(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble_lin(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble_lin(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "ada=RandomUnderSampler(ratio='not minority')\n",
    "train, train_labels=ada.fit_sample(X_train, y_train)\n",
    "test, \n",
    "=ada.fit_sample(X_test, y_test)\n",
    "\n",
    "X_train = train\n",
    "#X_test = test\n",
    "y_train = train_labels\n",
    "#y_test = test_labels\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "\n",
    "\n",
    "print(\"ensamble ln\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs all results for various classifier\n",
      "n_s\n",
      "[[43360   229   352    80]\n",
      " [ 1519    35   463    33]\n",
      " [ 1448    38  1669    65]\n",
      " [  381     0     7     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96     44021\n",
      "           1       0.12      0.02      0.03      2050\n",
      "           2       0.67      0.52      0.58      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     49679\n",
      "   macro avg       0.43      0.38      0.39     49679\n",
      "weighted avg       0.87      0.91      0.89     49679\n",
      "\n",
      "[0.4313224958380087, 1.3213022351893482, 0.38082402731767284]\n"
     ]
    }
   ],
   "source": [
    "##N class majority\n",
    "## V_f ensemble all \n",
    "#V_f lin\n",
    "##1 n_v Low percision but high recall  (43360)\n",
    "\n",
    "### \n",
    "\n",
    "\n",
    "print(\"n vs all results for various classifier\")\n",
    "\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "y_pred_rand_n_s = classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])[2]\n",
    "ab_normal_X_test_1 =X_test[y_pred_rand_n_s != 0]\n",
    "ab_normal_y_test_1 =y_test[y_pred_rand_n_s != 0]\n",
    "\n",
    "\n",
    "normal_x_test = X_test[y_pred_rand_n_s == 0]\n",
    "normal_y_pred = y_pred_rand_n_s[y_pred_rand_n_s == 0]\n",
    "normal_y_test = y_test[y_pred_rand_n_s == 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41363    31   162  1804]\n",
      " [ 1509     2     4     4]\n",
      " [  202    42  1142    62]\n",
      " [  141     1    41   198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96     43360\n",
      "           1       0.03      0.00      0.00      1519\n",
      "           2       0.85      0.79      0.82      1448\n",
      "           3       0.10      0.52      0.16       381\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     46708\n",
      "   macro avg       0.48      0.57      0.48     46708\n",
      "weighted avg       0.92      0.91      0.91     46708\n",
      "\n",
      "[0.3869327429872356, 1.6628594805412609, 0.4013238065612754]\n"
     ]
    }
   ],
   "source": [
    "normal_x_test = pd.DataFrame(normal_x_test,columns=features_clinic)\n",
    "##s_v seperate v class best\n",
    "y_pred_ensemble_lin = classifier.voting_ensemble_lin(X_train[s_v], y_train, normal_x_test[s_v], normal_y_test, jk=True,labels=[0,1,2,3])[1]\n",
    "\n",
    "\n",
    "normal_x_test_s_v =normal_x_test[y_pred_ensemble_lin != 2]\n",
    "normal_y_test_s_v =normal_y_test[y_pred_ensemble_lin != 2]\n",
    "\n",
    "\n",
    "normal_x_v_normal = normal_x_test[y_pred_ensemble_lin == 2]\n",
    "y_pred_svm_v_normal = y_pred_ensemble_lin[y_pred_ensemble_lin == 2]\n",
    "y_test_svm_v_normal = normal_y_test[y_pred_ensemble_lin == 2]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ensemble_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41363    31   162  1804]\n",
      " [ 1509     2     4     4]\n",
      " [  202    42  1142    62]\n",
      " [  141     1    41   198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96     43360\n",
      "           1       0.03      0.00      0.00      1519\n",
      "           2       0.85      0.79      0.82      1448\n",
      "           3       0.10      0.52      0.16       381\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     46708\n",
      "   macro avg       0.48      0.57      0.48     46708\n",
      "weighted avg       0.92      0.91      0.91     46708\n",
      "\n",
      "[0.3869327429872356, 1.6628594805412609, 0.4013238065612754]\n"
     ]
    }
   ],
   "source": [
    "normal_x_test = pd.DataFrame(normal_x_test,columns=features_clinic)\n",
    "##s_v seperate v class best\n",
    "y_pred_ensemble_lin = classifier.voting_ensemble_lin(X_train[s_v], y_train, normal_x_test[s_v], normal_y_test, jk=True,labels=[0,1,2,3])[1]\n",
    "\n",
    "\n",
    "normal_x_test_s_v =normal_x_test[y_pred_ensemble_lin != 2]\n",
    "normal_y_test_s_v =normal_y_test[y_pred_ensemble_lin != 2]\n",
    "\n",
    "#y_pred_temp = y_pred_ensemble_lin[y_pred_ensemble_lin != 2]\n",
    "\n",
    "#normal_x_test_s_v =normal_x_test_s_v[y_pred_temp != 3]\n",
    "#normal_y_test_s_v =normal_y_test_s_v[y_pred_temp != 3]\n",
    "\n",
    "\n",
    "y_pred_svm_f_normal = y_pred_ensemble_lin[y_pred_ensemble_lin == 3]\n",
    "y_test_svm_f_normal = normal_y_test[y_pred_ensemble_lin == 3] \n",
    "\n",
    "y_pred_svm_v_normal = y_pred_ensemble_lin[y_pred_ensemble_lin == 2]\n",
    "y_test_svm_v_normal = normal_y_test[y_pred_ensemble_lin == 2]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42608    14   139   437]\n",
      " [ 1502    10     2     1]\n",
      " [  192     7    63    44]\n",
      " [   39     0     0   301]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     43198\n",
      "           1       0.32      0.01      0.01      1515\n",
      "           2       0.31      0.21      0.25       306\n",
      "           3       0.38      0.89      0.54       340\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     45359\n",
      "   macro avg       0.49      0.52      0.44     45359\n",
      "weighted avg       0.93      0.95      0.93     45359\n",
      "\n",
      "[0.2386826086238746, 0.8438871875802381, 0.22482720275946705]\n"
     ]
    }
   ],
   "source": [
    "normal_x_test_s_v = pd.DataFrame(normal_x_test_s_v,columns=features_clinic)\n",
    "##s_v seperate v class best\n",
    "y_pred_ensemble_lin = classifier.voting_ensemble_lin(X_train[v_f], y_train, normal_x_test_s_v[v_f], normal_y_test_s_v, jk=True,labels=[0,1,2,3])[1]\n",
    "\n",
    "\n",
    "normal_x_test_v_f =normal_x_test_s_v[y_pred_ensemble_lin != 3]\n",
    "normal_y_test_v_f =normal_y_test_s_v[y_pred_ensemble_lin != 3]\n",
    "\n",
    "\n",
    "normal_x_test_f =normal_x_test_s_v[y_pred_ensemble_lin == 3]\n",
    "y_pred_svm_f_normal = y_pred_ensemble_lin[y_pred_ensemble_lin == 3]\n",
    "y_test_svm_f_normal = normal_x_test_s_v[y_pred_ensemble_lin == 3]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39933  2828     0     0]\n",
      " [  264  1250     0     0]\n",
      " [   57   205     0     0]\n",
      " [   35     4     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96     42761\n",
      "           1       0.29      0.83      0.43      1514\n",
      "           2       0.00      0.00      0.00       262\n",
      "           3       0.00      0.00      0.00        39\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     44576\n",
      "   macro avg       0.32      0.44      0.35     44576\n",
      "weighted avg       0.96      0.92      0.94     44576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-5693aa29650d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0my_pred_ensemble_lin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogisticRegress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormal_x_test_v_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormal_y_test_v_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\TunedClassifier.py\u001b[0m in \u001b[0;36mlogisticRegress\u001b[1;34m(X_train, y_train, X_test, y_test, jk, penalty, dual, tol, C, fit_intercept, intercept_scaling, class_weight, random_state, solver, max_iter, multi_class, verbose, warm_start, n_jobs, labels)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mjk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_metrics\u001b[1;34m(yhat, labels, lb)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_kappa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mjkappa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkappa\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.125\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjkappa\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\feature_selection_ECG\\codes\\python\\metric.py\u001b[0m in \u001b[0;36mget_j_index\u001b[1;34m(confusion_matrix)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_j_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mSes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mSev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mPs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mPv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes.python import features_columns as col\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "from codes.python import TunedClassifier as classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "#ls.extend(['V', 'E'])                         # VEB\n",
    "#ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "#ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "#ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n",
    "\n",
    "\n",
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])\n",
    "\n",
    "\n",
    "\n",
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )\n",
    "\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "#X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "#y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "\n",
    "rank_n_s = pd.read_csv(\"database/features_n_vs_s_randomforest.py\").sort_values(['rfscore', 'features'], ascending=[0,1])\n",
    "\n",
    "score_n_s = 0.01\n",
    "\n",
    "\n",
    "n_s = rank_n_s[rank_n_s['rfscore'] >= score_n_s]['features'].values\n",
    "\n",
    "\n",
    "y_pred_ensemble_lin = classifier.logisticRegress(X_train[n_s], y_train, normal_x_test_v_f[n_s], normal_y_test_v_f, jk=True,labels=[0,1,2,3])[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44576,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_y_test_v_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_linear\n",
      "all\n",
      "n_s\n",
      "[[45696   763     0     0]\n",
      " [  979  2241     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     46459\n",
      "           1       0.75      0.70      0.72      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     49679\n",
      "   macro avg       0.43      0.42      0.43     49679\n",
      "weighted avg       0.96      0.96      0.96     49679\n",
      "\n",
      "n_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45666   793     0     0]\n",
      " [ 1759  1461     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     46459\n",
      "           1       0.65      0.45      0.53      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     49679\n",
      "   macro avg       0.40      0.36      0.38     49679\n",
      "weighted avg       0.94      0.95      0.94     49679\n",
      "\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45711   748     0     0]\n",
      " [  480  2740     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     46459\n",
      "           1       0.79      0.85      0.82      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     49679\n",
      "   macro avg       0.44      0.46      0.45     49679\n",
      "weighted avg       0.98      0.98      0.98     49679\n",
      "\n",
      "s_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45711   748     0     0]\n",
      " [  480  2740     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     46459\n",
      "           1       0.79      0.85      0.82      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     49679\n",
      "   macro avg       0.44      0.46      0.45     49679\n",
      "weighted avg       0.98      0.98      0.98     49679\n",
      "\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46301   158     0     0]\n",
      " [ 1357  1863     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     46459\n",
      "           1       0.92      0.58      0.71      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     49679\n",
      "   macro avg       0.47      0.39      0.42     49679\n",
      "weighted avg       0.97      0.97      0.97     49679\n",
      "\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45972   487     0     0]\n",
      " [  963  2257     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     46459\n",
      "           1       0.82      0.70      0.76      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     49679\n",
      "   macro avg       0.45      0.42      0.44     49679\n",
      "weighted avg       0.97      0.97      0.97     49679\n",
      "\n",
      "svm_poly\n",
      "all\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45654   805     0     0]\n",
      " [ 1345  1875     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     46459\n",
      "           1       0.70      0.58      0.64      3220\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     49679\n",
      "   macro avg       0.84      0.78      0.81     49679\n",
      "weighted avg       0.95      0.96      0.95     49679\n",
      "\n",
      "n_v\n",
      "[[45967   492     0     0]\n",
      " [ 2570   650     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     46459\n",
      "           1       0.57      0.20      0.30      3220\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     49679\n",
      "   macro avg       0.76      0.60      0.63     49679\n",
      "weighted avg       0.92      0.94      0.92     49679\n",
      "\n",
      "n_f\n"
     ]
    }
   ],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "\n",
    "y_train[y_train != 2] = 0\n",
    "y_train[y_train == 2] = 1\n",
    "\n",
    "y_test[y_test != 2] = 0\n",
    "y_test[y_test == 2] = 1\n",
    "\n",
    "\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\") \n",
    "classifier.voting_ensemble(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "#classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "#classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random\")\n",
    "print(\"all\")\n",
    "#classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_balanced\n",
    "\n",
    "y_train = good_features_y\n",
    "\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "#classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "#classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random\")\n",
    "print(\"all\")\n",
    "#classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
