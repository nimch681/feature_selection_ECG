{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.python import load_DF_beats as DF\n",
    "import numpy as np\n",
    "from codes.python import metric\n",
    "from codes.python import features_columns as col\n",
    "from codes. python import post_process_features_ex as post_features\n",
    "import pandas as pd\n",
    "from codes.python import TunedClassifier as classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "ls = []\n",
    "ls.extend(['N', 'L', 'R'])                    # N\n",
    "ls.extend(['A', 'a', 'J', 'S', 'e', 'j'])     # SVEB \n",
    "ls.extend(['V', 'E'])                         # VEB\n",
    "ls.extend(['F'])\n",
    "#ls.extend([ 'P', '/', 'f', 'u'])\n",
    "patient_l_1 = [101]\n",
    "#patient_l_2 = [100]\n",
    "patient_ls_1 = [101,106,108,109,112,114,115,116,118,119,122,124,201,203,205,207,208,209,215,220,223,230]\n",
    "patient_ls_2 = [100,103,105,111,113,117,121,123,200,202,210,212,213,214,219,221,222,228,231,232,233,234]\n",
    "\n",
    "\n",
    "ls2 = []\n",
    "ls2.extend([\"['N']\",\"['L']\", \"['R']\"])                    # N\n",
    "ls2.extend([\"['A']\", \"['a']\", \"['J']\", \"['S']\",  \"['e']\", \"['j']\"])     # SVEB \n",
    "ls2.extend([\"['V']\", \"['E']\"])                         # VEB\n",
    "ls2.extend([\"['F']\"])\n",
    "#ls.extend([ \"['P']\",\"[ '/']\",\" ['f']\", \"['u']\"])\n",
    "\n",
    "\n",
    "good_features_X = np.asarray(pd.read_csv(\"database/gooddata_X_train_no_outliers.csv\").iloc[:,1:263])\n",
    "good_features_y = np.asarray(pd.read_csv(\"database/gooddata_y_train_no_outliers.csv\").iloc[:,1])\n",
    "\n",
    "\n",
    "\n",
    "np_clinic_1_old, np_clinic_2_old,np_non_var_1_old, np_non_var_2_old, np_class_ID_1_old, np_class_ID_2_old, patients_ls_1, patients_ls_2, DB1, DB2, DB1_V1, DB2_V1, DB1_non_cli, DB2_non_cli, DB1_dwt, DB2_dwt, DB1_dwt_V1, DB2_dwt_V1 = DF.get_all_dataframe(patient_l_1=patient_ls_1,patient_l_2=patient_ls_2 , ls=ls, ls2=ls2 )\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "    \n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "import pickle\n",
    "f = open('database/nvs_lin.text', 'rb')\n",
    "n_s = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/nvv_lin.text', 'rb')\n",
    "n_v = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/nvf_lin.text', 'rb')\n",
    "n_f = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/svv_lin.text', 'rb')\n",
    "s_v = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/svf_lin.text', 'rb')\n",
    "s_f = pickle.load(f)[1]\n",
    "f.close()\n",
    "\n",
    "f = open('database/vvf_lin.text', 'rb')\n",
    "v_f = pickle.load(f)[1]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log\n",
      "all\n",
      "n_s\n",
      "[[41034   955  1021  1011]\n",
      " [  512  1245   237    56]\n",
      " [  537   404  2226    53]\n",
      " [  361     0     6    21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95     44021\n",
      "           1       0.48      0.61      0.54      2050\n",
      "           2       0.64      0.69      0.66      3220\n",
      "           3       0.02      0.05      0.03       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.53      0.57      0.54     49679\n",
      "weighted avg       0.92      0.90      0.91     49679\n",
      "\n",
      "[0.560565089690647, 2.4145543696453604, 0.5821018410509935]\n",
      "n_v\n",
      "[[39564  2044  2300   113]\n",
      " [ 1794   196    46    14]\n",
      " [  296   218  2413   293]\n",
      " [  191    61     6   130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     44021\n",
      "           1       0.08      0.10      0.09      2050\n",
      "           2       0.51      0.75      0.60      3220\n",
      "           3       0.24      0.34      0.28       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.44      0.52      0.47     49679\n",
      "weighted avg       0.88      0.85      0.86     49679\n",
      "\n",
      "[0.3945505823809135, 1.4291981317673614, 0.37592505766137696]\n",
      "n_f\n",
      "[[36733   576  3011  3701]\n",
      " [ 1612    53   321    64]\n",
      " [ 1654    49  1052   465]\n",
      " [   27     0     5   356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.87     44021\n",
      "           1       0.08      0.03      0.04      2050\n",
      "           2       0.24      0.33      0.28      3220\n",
      "           3       0.08      0.92      0.14       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.33      0.53      0.33     49679\n",
      "weighted avg       0.83      0.77      0.80     49679\n",
      "\n",
      "[0.17155579513674793, 0.6704229589431512, 0.16958076743626788]\n",
      "s_v\n",
      "[[36941   491  4717  1872]\n",
      " [ 1991     6    35    18]\n",
      " [  327   178  2572   143]\n",
      " [   62     1    70   255]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89     44021\n",
      "           1       0.01      0.00      0.00      2050\n",
      "           2       0.35      0.80      0.48      3220\n",
      "           3       0.11      0.66      0.19       388\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     49679\n",
      "   macro avg       0.35      0.57      0.39     49679\n",
      "weighted avg       0.86      0.80      0.82     49679\n",
      "\n",
      "[0.30788957299298064, 1.1584099406785184, 0.2987460290813051]\n",
      "s_f\n",
      "[[40738   518  2253   512]\n",
      " [ 1935    51    47    17]\n",
      " [ 2058   269   729   164]\n",
      " [  171     2   116    99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     44021\n",
      "           1       0.06      0.02      0.04      2050\n",
      "           2       0.23      0.23      0.23      3220\n",
      "           3       0.12      0.26      0.17       388\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     49679\n",
      "   macro avg       0.33      0.36      0.34     49679\n",
      "weighted avg       0.82      0.84      0.83     49679\n",
      "\n",
      "[0.16423454335988483, 0.5437863524074615, 0.1500905657308751]\n",
      "v_f\n",
      "[[36791   768  1841  4621]\n",
      " [ 1746   126   125    53]\n",
      " [  443   146  2398   233]\n",
      " [   28     0     3   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89     44021\n",
      "           1       0.12      0.06      0.08      2050\n",
      "           2       0.55      0.74      0.63      3220\n",
      "           3       0.07      0.92      0.13       388\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     49679\n",
      "   macro avg       0.42      0.64      0.43     49679\n",
      "weighted avg       0.88      0.80      0.83     49679\n",
      "\n",
      "[0.32140230645442996, 1.4764561455917222, 0.34525817142618026]\n",
      "Lin\n",
      "all\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42710   778   416   117]\n",
      " [ 1076   857   117     0]\n",
      " [ 1866   359   939    56]\n",
      " [  360     0    16    12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     44021\n",
      "           1       0.43      0.42      0.42      2050\n",
      "           2       0.63      0.29      0.40      3220\n",
      "           3       0.06      0.03      0.04       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.51      0.43      0.45     49679\n",
      "weighted avg       0.88      0.90      0.88     49679\n",
      "\n",
      "[0.4086296431852523, 1.7705014425211902, 0.42562750190777493]\n",
      "n_v\n",
      "[[43521   258   212    30]\n",
      " [ 2019    14    16     1]\n",
      " [  764    57  2205   194]\n",
      " [  349    12     5    22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96     44021\n",
      "           1       0.04      0.01      0.01      2050\n",
      "           2       0.90      0.68      0.78      3220\n",
      "           3       0.09      0.06      0.07       388\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     49679\n",
      "   macro avg       0.49      0.43      0.46     49679\n",
      "weighted avg       0.89      0.92      0.90     49679\n",
      "\n",
      "[0.5202896030929742, 1.637097456004836, 0.4647819835470916]\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41005    19   458  2539]\n",
      " [ 1919     3   101    27]\n",
      " [ 1982    10  1027   201]\n",
      " [   34     0     7   347]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     44021\n",
      "           1       0.09      0.00      0.00      2050\n",
      "           2       0.64      0.32      0.43      3220\n",
      "           3       0.11      0.89      0.20       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.44      0.54      0.39     49679\n",
      "weighted avg       0.86      0.85      0.85     49679\n",
      "\n",
      "[0.24992622275099316, 1.058853057013656, 0.2573197435022036]\n",
      "s_v\n",
      "[[39588    35  1862  2536]\n",
      " [ 2003     0     7    40]\n",
      " [  839    80  2016   285]\n",
      " [   22     0    44   322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.51      0.63      0.56      3220\n",
      "           3       0.10      0.83      0.18       388\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     49679\n",
      "   macro avg       0.39      0.59      0.41     49679\n",
      "weighted avg       0.86      0.84      0.85     49679\n",
      "\n",
      "[0.34171875656803863, 1.1391946175041774, 0.3132587054720415]\n",
      "s_f\n",
      "[[41925    17  1949   130]\n",
      " [ 2016     0    30     4]\n",
      " [ 2718    12   437    53]\n",
      " [  280     0   106     2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.17      0.14      0.15      3220\n",
      "           3       0.01      0.01      0.01       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.27      0.27      0.27     49679\n",
      "weighted avg       0.80      0.85      0.83     49679\n",
      "\n",
      "[0.07636546102569701, 0.30898946414410333, 0.07680641353086143]\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40968    52   840  2161]\n",
      " [ 1991    10    41     8]\n",
      " [  727    15  2301   177]\n",
      " [   33     0     4   351]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93     44021\n",
      "           1       0.13      0.00      0.01      2050\n",
      "           2       0.72      0.71      0.72      3220\n",
      "           3       0.13      0.90      0.23       388\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     49679\n",
      "   macro avg       0.48      0.64      0.47     49679\n",
      "weighted avg       0.88      0.88      0.88     49679\n",
      "\n",
      "[0.43511968335932966, 1.5715666741647651, 0.4140056759502605]\n",
      "Random\n",
      "all\n",
      "n_s\n",
      "[[40557   681  1879   904]\n",
      " [ 1389   125   518    18]\n",
      " [  341    22  2853     4]\n",
      " [  129     0   259     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94     44021\n",
      "           1       0.15      0.06      0.09      2050\n",
      "           2       0.52      0.89      0.65      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     49679\n",
      "   macro avg       0.41      0.47      0.42     49679\n",
      "weighted avg       0.89      0.88      0.88     49679\n",
      "\n",
      "[0.47466241427034567, 1.6158464710520182, 0.43931201601667513]\n",
      "n_v\n",
      "[[40210  1655  2083    73]\n",
      " [ 1811   133   104     2]\n",
      " [  312   126  2743    39]\n",
      " [  233     2   152     1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93     44021\n",
      "           1       0.07      0.06      0.07      2050\n",
      "           2       0.54      0.85      0.66      3220\n",
      "           3       0.01      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     49679\n",
      "   macro avg       0.39      0.46      0.42     49679\n",
      "weighted avg       0.87      0.87      0.87     49679\n",
      "\n",
      "[0.42933989804061384, 1.525904982326751, 0.4054080718111508]\n",
      "n_f\n",
      "[[35664   652  7702     3]\n",
      " [ 1617    16   417     0]\n",
      " [  739    23  2458     0]\n",
      " [   95     1   292     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.81      0.87     44021\n",
      "           1       0.02      0.01      0.01      2050\n",
      "           2       0.23      0.76      0.35      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     49679\n",
      "   macro avg       0.30      0.40      0.31     49679\n",
      "weighted avg       0.84      0.77      0.79     49679\n",
      "\n",
      "[0.23931728296513452, 1.0204280622826016, 0.24721214926789248]\n",
      "s_v\n",
      "[[35106   588  7674   653]\n",
      " [ 1852    39   156     3]\n",
      " [  671     8  2535     6]\n",
      " [  174     1   212     1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86     44021\n",
      "           1       0.06      0.02      0.03      2050\n",
      "           2       0.24      0.79      0.37      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     49679\n",
      "   macro avg       0.31      0.40      0.31     49679\n",
      "weighted avg       0.84      0.76      0.79     49679\n",
      "\n",
      "[0.22415582852865723, 1.1072832099172492, 0.25048831550398476]\n",
      "s_f\n",
      "[[40077  1030  2383   531]\n",
      " [ 1890   101    57     2]\n",
      " [  752    24  2440     4]\n",
      " [  249     1   137     1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92     44021\n",
      "           1       0.09      0.05      0.06      2050\n",
      "           2       0.49      0.76      0.59      3220\n",
      "           3       0.00      0.00      0.00       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.38      0.43      0.39     49679\n",
      "weighted avg       0.86      0.86      0.86     49679\n",
      "\n",
      "[0.37119184215624534, 1.3807489322173794, 0.3581895376052951]\n",
      "v_f\n",
      "[[35764   576  3922  3759]\n",
      " [ 1518    96   422    14]\n",
      " [  408    15  2715    82]\n",
      " [   20     0    48   320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.88     44021\n",
      "           1       0.14      0.05      0.07      2050\n",
      "           2       0.38      0.84      0.53      3220\n",
      "           3       0.08      0.82      0.14       388\n",
      "\n",
      "   micro avg       0.78      0.78      0.78     49679\n",
      "   macro avg       0.39      0.63      0.40     49679\n",
      "weighted avg       0.87      0.78      0.81     49679\n",
      "\n",
      "[0.3149637208459872, 1.4117526904219904, 0.33395094672574244]\n",
      "ensamble ln\n",
      "all\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42843   776   396     6]\n",
      " [ 1084   850   116     0]\n",
      " [ 1954   389   859    18]\n",
      " [  378     0     8     2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     44021\n",
      "           1       0.42      0.41      0.42      2050\n",
      "           2       0.62      0.27      0.37      3220\n",
      "           3       0.08      0.01      0.01       388\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     49679\n",
      "   macro avg       0.51      0.41      0.44     49679\n",
      "weighted avg       0.88      0.90      0.88     49679\n",
      "\n",
      "[0.3981745973349851, 1.7261557168747879, 0.41485676327684107]\n",
      "n_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43563   272   174    12]\n",
      " [ 2020    14    16     0]\n",
      " [  831    66  2141   182]\n",
      " [  354    12     4    18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96     44021\n",
      "           1       0.04      0.01      0.01      2050\n",
      "           2       0.92      0.66      0.77      3220\n",
      "           3       0.08      0.05      0.06       388\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     49679\n",
      "   macro avg       0.49      0.43      0.45     49679\n",
      "weighted avg       0.89      0.92      0.90     49679\n",
      "\n",
      "[0.5113663977480521, 1.627114127275056, 0.4590724647834081]\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41030    61   411  2519]\n",
      " [ 1919     3   103    25]\n",
      " [ 2009    12  1001   198]\n",
      " [   34     0     7   347]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     44021\n",
      "           1       0.04      0.00      0.00      2050\n",
      "           2       0.66      0.31      0.42      3220\n",
      "           3       0.11      0.89      0.20       388\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     49679\n",
      "   macro avg       0.43      0.53      0.39     49679\n",
      "weighted avg       0.85      0.85      0.85     49679\n",
      "\n",
      "[0.24648540674011496, 1.00949391767573, 0.24942944307952375]\n",
      "s_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40334    51  2070  1566]\n",
      " [ 2006     1    27    16]\n",
      " [  845   117  2137   121]\n",
      " [   65     1    67   255]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92     44021\n",
      "           1       0.01      0.00      0.00      2050\n",
      "           2       0.50      0.66      0.57      3220\n",
      "           3       0.13      0.66      0.22       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.39      0.56      0.43     49679\n",
      "weighted avg       0.86      0.86      0.86     49679\n",
      "\n",
      "[0.3710710129110553, 1.1668959491634308, 0.3313975001009565]\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42489    29  1431    72]\n",
      " [ 2021     0    26     3]\n",
      " [ 2759    26   410    25]\n",
      " [  282     0   104     2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     44021\n",
      "           1       0.00      0.00      0.00      2050\n",
      "           2       0.21      0.13      0.16      3220\n",
      "           3       0.02      0.01      0.01       388\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     49679\n",
      "   macro avg       0.28      0.27      0.27     49679\n",
      "weighted avg       0.81      0.86      0.83     49679\n",
      "\n",
      "[0.08563822160253709, 0.3353454279600795, 0.08473728929627848]\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41059    56   794  2112]\n",
      " [ 1991    11    41     7]\n",
      " [  775    33  2236   176]\n",
      " [   35     0     4   349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93     44021\n",
      "           1       0.11      0.01      0.01      2050\n",
      "           2       0.73      0.69      0.71      3220\n",
      "           3       0.13      0.90      0.23       388\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     49679\n",
      "   macro avg       0.48      0.63      0.47     49679\n",
      "weighted avg       0.88      0.88      0.88     49679\n",
      "\n",
      "[0.4311712029574429, 1.5369302630914508, 0.40770188436515276]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(VotingClassifier(estimators=[('ln', LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "               solver='svd', store_covariance=False, tol=0.0001)), ('lr', LogisticRegression(C=100, class_weight='balanced', dual=False,\n",
       "           fit_intercept=True, intercept_scaling=10, max_iter=100,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "           solver='warn', tol=0.0001, verbose=0, warm_start=False))],\n",
       "          flatten_transform=None, n_jobs=None, voting='hard', weights=None),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " [0.4311712029574429, 1.5369302630914508, 0.40770188436515276])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "#X_train_under = pd.DataFrame(under_X_train,columns=features_clinic)\n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "y_train[y_train != 2] = 0\n",
    "y_train[y_train == 2] = 1\n",
    "\n",
    "y_test[y_test != 2] = 0\n",
    "y_test[y_test == 2] = 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "#classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "#classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random\")\n",
    "print(\"all\")\n",
    "#classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ensamble ln\")\n",
    "print(\"all\")\n",
    "#classifier.voting_ensemble_lin(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble_lin(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble_lin(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble_lin(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble_lin(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble_lin(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble_lin(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_linear\n",
      "all\n",
      "n_s\n",
      "[[45976   483     0     0]\n",
      " [ 1444  1776     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     46459\n",
      "           1       0.79      0.55      0.65      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     49679\n",
      "   macro avg       0.44      0.39      0.41     49679\n",
      "weighted avg       0.96      0.96      0.96     49679\n",
      "\n",
      "n_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46199   260     0     0]\n",
      " [ 1493  1727     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     46459\n",
      "           1       0.87      0.54      0.66      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     49679\n",
      "   macro avg       0.46      0.38      0.41     49679\n",
      "weighted avg       0.96      0.96      0.96     49679\n",
      "\n",
      "n_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46459     0     0     0]\n",
      " [ 3220     0     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97     46459\n",
      "           1       0.00      0.00      0.00      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     49679\n",
      "   macro avg       0.23      0.25      0.24     49679\n",
      "weighted avg       0.87      0.94      0.90     49679\n",
      "\n",
      "s_v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44694  1765     0     0]\n",
      " [  772  2448     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     46459\n",
      "           1       0.58      0.76      0.66      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     49679\n",
      "   macro avg       0.39      0.43      0.41     49679\n",
      "weighted avg       0.96      0.95      0.95     49679\n",
      "\n",
      "s_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45491   968     0     0]\n",
      " [ 2842   378     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96     46459\n",
      "           1       0.28      0.12      0.17      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     49679\n",
      "   macro avg       0.31      0.27      0.28     49679\n",
      "weighted avg       0.90      0.92      0.91     49679\n",
      "\n",
      "v_f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46100   359     0     0]\n",
      " [ 1203  2017     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     46459\n",
      "           1       0.85      0.63      0.72      3220\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     49679\n",
      "   macro avg       0.46      0.40      0.43     49679\n",
      "weighted avg       0.97      0.97      0.97     49679\n",
      "\n",
      "svm_poly\n",
      "all\n",
      "n_s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\chont\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46074   385     0     0]\n",
      " [ 1317  1903     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     46459\n",
      "           1       0.83      0.59      0.69      3220\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     49679\n",
      "   macro avg       0.90      0.79      0.84     49679\n",
      "weighted avg       0.96      0.97      0.96     49679\n",
      "\n",
      "n_v\n",
      "[[46449    10     0     0]\n",
      " [ 3219     1     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97     46459\n",
      "           1       0.09      0.00      0.00      3220\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     49679\n",
      "   macro avg       0.51      0.50      0.48     49679\n",
      "weighted avg       0.88      0.94      0.90     49679\n",
      "\n",
      "n_f\n",
      "[[42645  3814     0     0]\n",
      " [ 1791  1429     0     0]\n",
      " [    0     0     0     0]\n",
      " [    0     0     0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94     46459\n",
      "           1       0.27      0.44      0.34      3220\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     49679\n",
      "   macro avg       0.62      0.68      0.64     49679\n",
      "weighted avg       0.92      0.89      0.90     49679\n",
      "\n",
      "s_v\n"
     ]
    }
   ],
   "source": [
    "np_class_ID_1_old = [int(i) for i in np_class_ID_1_old]\n",
    "np_class_ID_2_old = [int(i) for i in np_class_ID_2_old]\n",
    "X_train = np_clinic_1_old\n",
    "X_test = np_clinic_2_old\n",
    "y_train = np.asarray(np_class_ID_1_old)\n",
    "y_test = np.asarray(np_class_ID_2_old)\n",
    "input_size=X_train.shape[1]\n",
    "features_clinic,c_ID,f_M, f_V, f_d , norm_mlii, norm_v1 , norm_dtw = col.get_columns()\n",
    "\n",
    "#X_train_under = pd.DataFrame(under_X_train,columns=features_clinic)\n",
    "X_train_balanced = pd.DataFrame(good_features_X,columns=features_clinic)\n",
    "X_train = pd.DataFrame(X_train,columns=features_clinic)\n",
    "X_test = pd.DataFrame(X_test,columns=features_clinic)\n",
    "\n",
    "y_train[y_train != 2] = 0\n",
    "y_train[y_train == 2] = 1\n",
    "\n",
    "y_test[y_test != 2] = 0\n",
    "y_test[y_test == 2] = 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble(X_train[n_s], y_train, X_test[n_s], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble(X_train[n_v], y_train, X_test[n_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble(X_train[n_f], y_train, X_test[n_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble(X_train[s_v], y_train, X_test[s_v], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble(X_train[s_f], y_train, X_test[s_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble(X_train[v_f], y_train, X_test[v_f], y_test, jk=False,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"Log\")\n",
    "print(\"all\")\n",
    "#classifier.logisticRegress(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.logisticRegress(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.logisticRegress(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.logisticRegress(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.logisticRegress(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.logisticRegress(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.logisticRegress(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Lin\")\n",
    "print(\"all\")\n",
    "#classifier.Linear_D(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.Linear_D(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.Linear_D(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.Linear_D(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.Linear_D(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.Linear_D(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.Linear_D(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Random\")\n",
    "print(\"all\")\n",
    "#classifier.randomForest(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "## Use this first\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_train = X_train_balanced\n",
    "\n",
    "y_train = good_features_y\n",
    "\n",
    "print(\"svm_linear\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_linear(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_linear(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_linear(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_linear(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_linear(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_linear(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"svm_poly\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.svm_model_poly(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.svm_model_poly(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.svm_model_poly(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.svm_model_poly(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.svm_model_poly(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.svm_model_poly(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"randomforest\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.randomForest(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.randomForest(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.randomForest(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.randomForest(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.randomForest(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.randomForest(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "print(\"xgboost\")\n",
    "print(\"all\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.xgboost(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.xgboost(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.xgboost(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.xgboost(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.xgboost(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.xgboost(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.ada(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.ada(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.ada(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.ada(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.ada(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.ada(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n",
    "\n",
    "print(\"ada\")\n",
    "#classifier.svm_model_linear(X_train[feature_good], y_train, X_test[feature_good], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_s\")\n",
    "classifier.voting_ensemble(X_train[n_s], y_train, X_test[n_s], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_v\")\n",
    "classifier.voting_ensemble(X_train[n_v], y_train, X_test[n_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"n_f\")\n",
    "classifier.voting_ensemble(X_train[n_f], y_train, X_test[n_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_v\")\n",
    "classifier.voting_ensemble(X_train[s_v], y_train, X_test[s_v], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"s_f\")\n",
    "classifier.voting_ensemble(X_train[s_f], y_train, X_test[s_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "print(\"v_f\")\n",
    "classifier.voting_ensemble(X_train[v_f], y_train, X_test[v_f], y_test, jk=True,labels=[0,1,2,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
